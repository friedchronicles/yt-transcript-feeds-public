{"video_id": "25FsKN0f8gQ", "title": "[NeurIPS Best Paper] 1000 Layer Networks for Self-Supervised RL \u2014 Kevin Wang et al, Princeton", "link": "https://www.youtube.com/watch?v=25FsKN0f8gQ", "published": "2025-12-31T16:00:04+00:00", "summary": "From undergraduate research seminars at Princeton to winning *Best Paper award at NeurIPS 2025,* *Kevin Wang, Ishaan Javali, Micha\u0142 Bortkiewicz, Tomasz Trzcinski, Benjamin Eysenbach* defied conventional wisdom by scaling reinforcement learning networks to *1,000 layers deep*\u2014unlocking performance gains that the RL community thought impossible. We caught up with the team live at *NeurIPS* to dig into the story behind *RL1000:* why deep networks have worked in language and vision but failed in RL for over a decade (spoiler: it's not just about depth, it's about the objective), how they discovered that *self-supervised RL* (learning representations of states, actions, and future states via contrastive learning) scales where value-based methods collapse, the critical architectural tricks that made it work (*residual connections, layer normalization, and a shift from regression to classification*), why scaling depth is more parameter-efficient than scaling width (linear vs. quadratic growth), how *Jax and GPU-accelerated environments* let them collect hundreds of millions of transitions in hours (the data abundance that unlocked scaling in the first place), the \"critical depth\" phenomenon where performance doesn't just improve\u2014it *multiplies* once you cross 15M+ transitions and add the right architectural components, why this isn't just \"make networks bigger\" but a fundamental shift in RL objectives (their code doesn't have a line saying \"maximize rewards\"\u2014it's pure self-supervised representation learning), how *deep teacher, shallow student* distillation could unlock deployment at scale (train frontier capabilities with 1000 layers, distill down to efficient inference models), the robotics implications (goal-conditioned RL without human supervision or demonstrations, scaling architecture instead of scaling manual data collection), and their thesis that *RL is finally ready to scale like language and vision*\u2014not by throwing compute at value functions, but by borrowing the self-supervised, representation-learning paradigms that made the rest of deep learning work.\nWe discuss:\n\n* The *self-supervised RL objective:* instead of learning value functions (noisy, biased, spurious), they learn representations where states along the same trajectory are pushed together, states along different trajectories are pushed apart\u2014turning RL into a classification problem\n* Why *naive scaling failed:* doubling depth degraded performance, doubling again with residual connections and layer norm suddenly skyrocketed performance in one environment\u2014unlocking the \"critical depth\" phenomenon\n* *Scaling depth vs. width:* depth grows parameters linearly, width grows quadratically\u2014depth is more parameter-efficient and sample-efficient for the same performance\n* The *Jax + GPU-accelerated environments* unlock: collecting thousands of trajectories in parallel meant data wasn't the bottleneck, and crossing 15M+ transitions was when deep networks really paid off\n* The *blurring of RL and self-supervised learning:* their code doesn't maximize rewards directly, it's an actor-critic goal-conditioned RL algorithm, but the learning burden shifts to classification (cross-entropy loss, representation learning) instead of TD error regression\n* Why *scaling batch size unlocks at depth:* traditional RL doesn't benefit from larger batches because networks are too small to exploit the signal, but once you scale depth, batch size becomes another effective scaling dimension\n\n\u2014\nRL1000 Team (Princeton)\n\n* *1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities* (https://openreview.net/forum?id=s0JVsx3bx1): https://openreview.net/forum?id=s0JVsx3bx1\n\n00:00:00 Introduction: Best Paper Award and NeurIPS Poster Experience\n00:01:11 Team Introductions and Princeton Research Origins\n00:03:35 The Deep Learning Anomaly: Why RL Stayed Shallow\n00:04:35 Self-Supervised RL: A Different Approach to Scaling\n00:05:13 The Breakthrough Moment: Residual Connections and Critical Depth\n00:07:15 Architectural Choices: Borrowing from ResNets and Avoiding Vanishing Gradients\n00:07:50 Clarifying the Paper: Not Just Big Networks, But Different Objectives\n00:08:46 Blurring the Lines: RL Meets Self-Supervised Learning\n00:09:44 From TD Errors to Classification: Why This Objective Scales\n00:11:06 Architecture Details: Building on Braw and SymbaFowl\n00:12:05 Robotics Applications: Goal-Conditioned RL Without Human Supervision\n00:13:15 Efficiency Trade-offs: Depth vs Width and Parameter Scaling\n00:15:48 JAX and GPU-Accelerated Environments: The Data Infrastructure\n00:18:05 World Models and Next State Classification\n00:22:37 Unlocking Batch Size Scaling Through Network Capacity\n00:24:10 Compute Requirements: State-of-the-Art on a Single GPU\n00:21:02 Future Directions: Distillation, VLMs, and Hierarchical Planning\n00:27:15 Closing Thoughts: Challenging Conventional Wisdom in RL Scaling", "transcript_html": "<p><strong>[00:00]</strong></p>\n<p>[music] Light space to fire up. [music] >> So, welcome to L Space. Uh, we are basically trying to provide the best optimal sort of podcast experience of neurops for people who are not here. Uh, and congrats on your paper. How does it feel? >> Yeah, it was very exciting. Um, >> yeah, >> we had a poster yesterday uh yesterday and then today we'll have an oral talk. >> We just like mobbed. Oh yeah, there was a lot of people. It's like three hours straight of like you know like waves of</p>\n<p>people to like that were we were trying to but >> so I've never received the best paper. Do you just find out on the website like what uh >> oh I just like woke up one day and like checked my email and then >> ah they just tell they just >> they was like oh like it's like I just saw email oh you award like been awarded best paper on >> but maybe you know from the reviews as well right? Yeah, we know from the reviews that we did well. Um, but there's a difference between like doing well in the reviews and getting best paper. So, that part we didn't actually know.</p>\n<p>>> Yeah. >> Okay. So, I I I skipped a little bit. Uh, maybe we can go sort of um one by one and and sort of introduce um you know, who you are and what you did on on on the team. >> I'm Kevin. I was an undergrad from from Princeton. I just graduated and yeah, I guess I led the project like started the project and then was very happy to collaborate with Isan and Nicole and Ben also. Um, >> right. And were you in like the same research group? Like how do you how what's your social context? >> So, so yeah, so we're all from Princeton. Yeah. >> Um,</p>\n<p>>> with and thanks to Ellen for booking you guys. >> So, this project actually started from like an IW seminar. So, uh like like independent work research seminar uh that Ben was teaching >> and this was like actually like like one of my first experiences in like ML research. Um, so it was really valuable to like get that experience and then Ean was also in that seminar and working on adjacent things. And so we collaborated um a lot during that seminar and then yeah the project turned out to have some pretty cool results and then later on also like the whole working on sort of similar things also um joined on the project and became like a good</p>\n<p>collaboration. >> Yeah and um I I don't know if any of you guys want to want to chime in on like other elements of coming into like deciding on this uh problem. So it's like broadly my lab works on deep reinforcement learning but historically deep meant like two or three or four layers >> not 10,00 [laughter] >> when Kevin and Sean mentioned they wanted to try really deep networks kind of skeptical it was going to work. I've tried this before it doesn't work other people have tried this before and doesn't going to work. So I was very very skeptical starting out. I don't</p>\n<p>know if I made this at the time, but that was my prior going in because >> But do you do you view your job as like screening or like, hey guys, this is probably isn't going to work. You should try a different idea, you know, like or should you be encouraging even if it's dumb? >> It's selecting bets. >> Yeah. >> And this was a bet I was willing to make. >> What what made you willing to make a bet? It seemed relatively low cost uh in that we Mihal in particular had spent</p>\n<p><strong>[03:03]</strong></p>\n<p>the past year developing infrastructure that made it a lot easier to run some of these experiments and the precedent was deeper numbers could do a whole lot better like that's what the deep learning re revolution has been over the last decade >> yeah I know why do we stop making them deeper [laughter] >> uh and reinforcement learning was like this one anomaly where we continue to use these really shallow networks and that's particularly true in the settings that we were looking at where you're starting from scratch you're starting from often. >> Any other perspectives you guys want to chime in with? >> I guess maybe I should just go over like an overview of our project. >> Yes. Okay. Sorry. Yes.</p>\n<p>>> So, the way that I kind of view our project um is that if you look at the landscape of deep learning, you know, you have NLP like language, vision, and then RL. And like as Ben kind of alluded to, you know, like in in language in vision, we've sort of converged to these like paradigms of scaling to massive networks, right? Like hundreds of billions of parameters, trillions of parameters. And there's been you know a lot a lot gained from in deep learning from from that right and then but then it seems like in the third sort of branch of deep learning in deep RL that has not yet been the case. Um like I was very surprised like coming into some</p>\n<p>like you know Ben's class and seminar when I was looking at the networks. Oh why were you just using like a simple like two-layer MLP for like these frontier sort of you know state-of-the-art RL algorithms. Um and so I was very curious like can we design RL algorithms? can we sort of put together a recipe for RL that can allow it to scale in potentially you know analogous ways that language envision might scale and so what we did is that we know that traditional RL like let's say like value uh value based RL doesn't really scale right this is pretty clear from the literature so we tried a</p>\n<p>different approach to RL um called self-supervised RL where instead of learning like a value function we're learning representations of states actions and future states such that the representations along the same trajectory are pushed together the representations longterm strategies are pushed apart and this is just like a different approach to uh RL that's allows us to learn in a self-supervised manner so there's we can solve task reach goals without any human crafted reward signal and so we know that self-supervised learning is scalable in these different areas in if deep learning so can self-supervised RL scale</p>\n<p>in similar ways um when we first tried it it actually didn't work like we made the networks deeper the performance like totally degraded but then we also but then I separately was like there's also some other work like um in in our literature like we tried like residual connections um and then there's other a few other architectural components that we had to put into the recipe and then all of a sudden like one day like I ran this experiment and there was like this one environment in which there was like like going from like like doubling the depth didn't really do anything but like</p>\n<p>doubling the depth again with these different components suddenly like skyrocketed performance in this one environment. Getting this to work was very non-trivial in the sense that like usually when we need to think about doing hyperparameter optim optimization we try changing A see if it makes it better try changing B see whether it makes it better and if we just made the depth bigger makes it worse we just add residual connections didn't make it better and it was really this combination of factors that Kevin and</p>\n<p><strong>[06:04]</strong></p>\n<p>Ean figured out that really made this work >> and as a precursor to that we also tried scaling along different dimensions so scaling the batch size uh scaling the the width of the network so the hidden layers and Yeah, pretty much kind of similar to just scaling depth naively. Um, and then once we started introducing residual connections, layer these specific architectural choices, that's when we saw these significant jumps in performance like these critical depths at which performance multiplies by a pretty huge factor and that's where we really noticed like unlocking some significant performance gains uh as</p>\n<p>opposed to scaling just along with which did yield some performance improvements. Um, but when you look at the number of parameters that your network has as you grow width, it's roughly a quadratic as opposed to something like growing depth. So it's more in some senses more parameter efficient, also more sample efficient from the experiments that we conducted. >> Nice. Um, in some ways you're sort of replicating stuff that is seen in the wild but on on a very small model that you can study. Is that would you would you say that's >> Yeah. So I kind of add to what Kevin said earlier. We saw these huge</p>\n<p>performance improvements in language models, image generation models by making them larger, making them deeper, which seems very intuitive. Yeah. >> Uh and so that's why our work we we draw from like foundational research, right? Like uh residual networks which employ residual connections to avoid uh vanishing gradients. And that's something that we show in some of our ablations in our in our paper like further down probably in appendices. We did experiments without these residual connections. And so sort of borrowing these concepts that have existed in other fields and applying them to this setting with uh RL and showing that it</p>\n<p>works. >> Before Ben has to has to go, I'll leave the sort of last word uh to him. What additional work does this inspire that like that that you want to push on next? >> I think there's one thing I'd clarify about the paper and then I'll directly answer the question. I think the thing I might clarify about the paper is I think a lot of people reading the title are like wow big networks they're great. I'll take big networks and >> you solved it now. We can just go. Yeah, we say big network add them to PO, add them to SACE, add them to your favorite reinforcement learning algorithm. But I think that's actually not the main conclusion. I think the main conclusion is that using big networks not only</p>\n<p>requires these architectural tricks, but also as Kevin mentioned before, it requires using a different objective. This objective doesn't actually use rewards in it. And so there's another word in the title, reinforcement learning, that also might be a little bit of a misnomer because we aren't directly trying to maximize rewards. Our code doesn't have a line of code saying maximize rewards here. And so is at the end of the day this a reinforcement learning method? I don't know. It looks much more similar to the self-supervised methods in other areas of machine</p>\n<p>learning. And so I think that the method the work really stands in some sort of interesting intersection of reinforcement learning and self-supervised learning research. And we had this little figure on the bottom left of the poster which was the screenshot of a slide from Yan Lakun talking about how to build intelligent systems and whether that's going to be done by unsupervised learning or</p>\n<p><strong>[09:04]</strong></p>\n<p>supervised learning and reinforcement learning. And I think what her paper really suggests is that the boundary between these things is really blurry and maybe the keys to building intelligent systems are going to be lever insights from all of them. >> Yeah, the layer kick. >> Exactly. [laughter] >> U well thank you for your time. I know I know you have to go soon for Jon. >> Yeah, thank you so much for coming. I I think that that insight of like blurring things is interesting. I I don't know if you like you were talking about so like uh the abstraction layer of representation learning. I don't know if if that triggers anything in terms of like the mix between self-supervised and</p>\n<p>reinforcement learning. Is that is that something fundamental that you've discovered or that we that people don't understand when they when they read the paper? Yeah, I think the best way that I would explain it is that we know that standard RL is not super scalable. And so like why can this different approach or different objective RL be scalable? I think it's because we're fundamentally shifting the burden of learning from something like like Q-learning or like regressing to like TD errors which we know is quite sperious and noisy and biased to fundamentally like a</p>\n<p>classification problem. We're trying to classify whether future state is along the same trajectory or along a different trajectory. And we do this with representation learning right and we know that classification cross entry loss and representation learning is scalable in the deep learning literature right if we think about language um and like some of the objectives there so in some sense we're kind of blurring the the lines we're doing reinforcement learning it's still an actor critic reinforcement learning algorithm it's like a goal condition reinforcement algorithm but the objective the burden of like learning of that of of solving</p>\n<p>that RL task shifts to something that's more similar to what's objective that you might see in language in vision that we know have scaled so much and so I think yeah I think that's like one of the fundamental insights that we've seen is that um it seems like by approaching RL in this different approach we were able to like get so much more out of we were able to scale our networks like significantly beyond what is like standard used in RA >> can I jump in I will just give a bit of more of context about the architecture because uh yeah we use another objective the uh influency so the contrast plastic</p>\n<p>plus. However, the architecture is quite similar to the previous works uh of previous uh papers like bra or or uh simba simba and simba fu 2 uh simba one simba 2. So we we also tweaked a bit of this uh architecture. However, it's not that we like uh invented the wheel for the first time. It's the merging between the architecture and the objective that makes the scale uh really uh like go up and and performance follow the the</p>\n<p>scale. >> I think that's something that we should uh probably mine deeper. Um do you think I guess like what domains what industry like if you you've applied it on on multiple different uh types of networks or or data sets is there a particular affinity that you think like has is like sort of low hanging fruit? >> Yeah. So actually if you look at a lot</p>\n<p><strong>[12:05]</strong></p>\n<p>of our tasks they're particularly sort of like robotics tasks. Um so this is personally I'd be very curious about how a work like this could impact like the robotics field. Like my understanding of robotics is that a lot of robotics right now there's kind of multi a few different approaches. Like one approach is we want to train robots using imitation learning. So we try to collect like an insane amount of data. But we have a ton of cub human cuban civ supervision and we try to scale up this data and we're like learning with imitation learning like but on the other hand potential like perhaps there's another approach which is like for example like goal condition</p>\n<p>reinforcement learning where we can actually train robotic agents and trillion RL agents to solve meaningful tasks with absolutely no human supervision no demonstration >> it's much more scalable yeah >> so yeah so this could serve as an alternate approach and perhaps instead of like scaling data like scaling manual like human supervision which which is you know not super scalable If there are ways to sort of make goal condition reinforcement learning scalable and like we can just scale the architecture or we can scale >> because you're focused on your objectives. Yeah. >> Right. What's with certain different objectives? I think that could be very exciting and see to see how that can affect a field like robotics for</p>\n<p>example. >> Yeah. Uh double click on on just one one thing on the efficiency which you you was talking about. I would expect very deep the deeper it is this should be quadratically worse. I I'm not familiar with like the the pre-existing literature. I'm just like sort of working out intuitions. But um >> basically uh what are the trade-offs that you've found that I think you might want to warn people about >> because because you you were the guy who mentioned efficiency. So >> sure, sure. Yeah. So I was referring to like one of the figures on our poster also in our paper where we compare like</p>\n<p>the number of parameters that models have as we scale along the axis of depth and as we scale along the axis of width. >> Yeah. from our baseline architecture, the most baseline one would be like a width of 256 like the hidden layers have 256 neurons and then the depth is four four layers or hidden layers. Um and so the point I was making there is that when you scale along depth you're the number of parameters that your model has is going to grow roughly linearly. Uh whereas with width you're making your network outputs wider and then the input to the next network is also growing as</p>\n<p>well. And so the the number of parameters your network's then going to have it grows approximately quadratically. And so one of the experiments we did was sort of examining as we grow the number of parameters in our model by scaling along these two different choices which one for the same like approximate number of parameters yields a better performance. And the depth curve kind of goes like this. It jumps up pretty fast. That's like present throughout our paper. For with it grows a little bit more slowly. And so that the kind of takeaway from that is that if you are a bit more resource constrainted scaling along that might be</p>\n<p>better because there's fewer parameters with a smaller model to a smaller number tool learnable parameters >> with is expensive >> which is expensive. Exactly. And in general of course like more parameters is also going to be more expensive. So that's just like another consideration to think about when using these networks and I suppose. >> Yeah. Any other sort of rules of thumbs like that that I can extract that? This is just the most basic one that I could think of. >> Yeah. Uh I don't know if there's any others.</p>\n<p><strong>[15:05]</strong></p>\n<p>>> Yeah, I guess like to your original question of like the trade-offs um like one of the trade-offs, one of the limitations that we say is like obviously if you make the networks bigger the it it will takes longer to run, right? So if you like double the depth at some level of depth you you it might take like twice as much to like take make a forward pass through the network, right? However, this is not so like within our paper like for most environments um we are able to like saturate like get to like almost perfect performance within just you know we don't even need to get to like a thousand layers like maybe just 64 layers for example is sufficient um and</p>\n<p>in this regime like like the latency of the network is not necessarily actually even the uh not necessarily like a significant bottleneck like you can imagine there's a lot of tasks in which especially in RL that like collecting data might be the bottleneck right and making four passes through our network may not be the bottleneck. And so in our environment, we in our research, we specifically used the Jax GCRL environment, which is a Jax based GPU accelerate environment. So we can collect like thousands of like environment trajectories like in parallel at the same time. So that we're</p>\n<p>able to like make uh like oh this is built in, >> right? This is built in so that we can collect you know like like a thousand trajectories at the same time along all these environments and so um makes that make sure that like we have enough data to like saturate the learning from >> wow >> that's like work data columns >> okay and you I don't know if you want to explore expound upon that on the drug maybe >> and you know most people are familiar with Pyth less familiar with Jax >> with J I think Jax is getting the uh the traction especially in RL field because</p>\n<p>the in for online reinforcement learning getting as much data as you can is is the most important. There's got to be a pie to equivalence. But anyway, >> how are any tips for other people also exploring this kind of roll out? Yeah. >> Yeah. So, I think I can also recommend like uh for for go conditioned RL, I I'm recommending JRL, but there are also like multi- aent J implementation and others. So going back to our paper, if you look at the plots, we only see this like huge performance increase when we</p>\n<p>cross like 50 uh millions of uh transitions gap. So so I think the data is crucial like here. Yeah, I guess even to build on that like I I like drawing analogies to like successes in other areas of deep learning like for example in large language models the reason why we're able to scale to such large networks is that we found a paradigm in which we can leverage the entire internet scale of data to learn right >> and so data in RL traditionally has been hard to come by um but now with these</p>\n<p>like GPU accelerate environments we can collect hundreds of millions of time steps of data within just a few hours and so I think that this serves as like a really good test bed for us to be able to also find ways to scale up um like network capacity and get similar kind of gains. >> I think that has to go. >> Are you saying that you have a difference you would do pre-training differently in LLMs? Like what's the</p>\n<p><strong>[18:05]</strong></p>\n<p>what's the difference uh objective now? Um yeah, very simply the the paradigm that you're referencing is next word or next token, right? >> It's very robust. [laughter] >> Yeah. >> How do you change that? Oh, I'm not saying I want to leverage insights from that to apply to morale. >> I I feel like you should go the other way. >> You think you should go other way? >> Maybe. I mean, that would be a very interesting research direction, too. But actually, yeah, even on that point, like one of the things I was thinking about is that the way that our our objective works is in some it's not exactly next</p>\n<p>word prediction, but it's kind of like next state prediction, right? You imagine you're at some current state and you're at some current action and we want to predict whether or not this future state this this certain state is a future state along the same trajectory or a different trajectory and so in some sense we are actually doing some sort of like >> implicit world model >> implicit like uh you know like in >> I don't know if that's a bad word these >> or like in language you you do a cross entry loss to classify the next token right and here we're just doing a binary classification of like whether or not some next state is some</p>\n<p>>> yeah yeah it's a classification yeah >> and so I do see that there are some like sort of parallels here that perhaps we should dig into deeper and see like what is the core to of what enables deep learning to scale and then how can we like leverage that how how we can distill those like insights and then apply those across like all different fields whether it's language or reinforcement learning. >> Yeah. Uh did you did you get my my meaning about the world model stuff? >> Yeah. Yeah. actually and I I heard I think I might have heard professor Eisenbach yesterday talking about this at a poster and he's explaining to a</p>\n<p>couple people that because this is like doing representation learning and trying to learn these meaningful representations for a given state and action but for a given goal in some sense you can think of it almost like learning a model of the environment learning a model of the world but without having to do any sort of like next frame prediction or stuff like that that's a little bit more highdimensional and complex. Yeah. >> Yeah. I I will think like um the the angle that I'm trying to think about and push is instead of learn the next world they're basically like generate a number of candidates possible worlds and</p>\n<p>classify them uh to your point uh which is exactly how I do things. Let's say I'm playing poker and I'm trying to classify what hands you have. Well, there's a range of hands based on what you're you're doing. And the more information I get, the more I resolve to, oh, I know exactly what hand you have based on what you're showing, you know, or you're buffing. But that's that's a different thing. But you know what I mean? Like that I I feel like that is the ultimate sort of end goal of representation which is a world. But I don't know if that is too vague compared to the more concrete types of world</p>\n<p>models that let's say the video gen people are doing. >> And then I I guess one one other thing like I'm also exploring I you mentioned like the deep models being slower or more expensive. Yeah, that that is a trend in the inference world of making models shallower, right? And I wonder if this like short catchphrase I was thinking about like deep teacher shallow student would be a good</p>\n<p><strong>[21:05]</strong></p>\n<p>deployment paradigm. >> Yeah. >> Like you push the frontier capabilities with the with with F and then you distill >> distill it back. >> I actually this is like a good point like if you go out to our website like this is one of the future directions that we list at the very bottom. >> Okay. >> Yeah. uh we we we we would love to see if we could get similar performance like we pushed the you know like we do achieve state-of-the-art performance on u gold condition RL in Java CRL by a significant amount and so it was very exciting to see the like the the sort of frontier of the ability to train RL agents uh sort of pushed um and if we</p>\n<p>can do that in a way that also sort of is just as efficient as a standard uh you know networks that would be very cool so you know like is able >> yeah because training uh doesn't have to be the the same thing that you deployed inference, >> right? You know what I mean? Like >> Yeah. So, yeah. So, if there's ways to like distill down to a smaller model or prune the model and maybe not still retain performance, that's a very interesting research direction that we're choos what what else is your personal passions? >> Yeah. So, uh currently I'm pursuing direction of uh stitching in</p>\n<p>reinforcement learning. So we are trying to generalize uh reinforcement learning from shorter subbehaviors so that they are stitched merged uh during the test time and uh yeah I think this is one of my uh last papers that I will tackle during the PhD. >> Personally I would I'm very curious of like can we like what's the like real like can we push I'm I'm I'm curious about like advancing the frontier as much as possible. Um so if you actually look at our paper we focus on scaling</p>\n<p>depth but we notice that we see that scaling width actually also improves performance and we also find that actually by scaling depth we actually unlock the ability to scale along batch size as well. Um so this is one of Yeah. Uh so so okay so I guess >> colinear like yeah >> right so like okay I guess for context like in traditional RL like value based RL scaling batch size is not super effective but there's we also can see there's also other work in other areas of deep learning that show that scaling batch size is only most effective when there's like a large enough network capacity to take advantage of the scaled</p>\n<p>batch size and we actually find that you know perhaps so one hypothesis might be like perhaps the reason why scale batches isn't that effective in traditional RLS because like we've been using these tiny networks that haven't be able to capture And one of our experiments is that like because we are enabled successful training of deep network we actually were able to this is a great test bed for you know like testing this hypothesis and we find that indeed as we scale the network capacity we also unlock this different dimension of scaling by our site and so all that to say is that I'm very curious for someone</p>\n<p>like with enough compute to like take some of these environments scale up batch uh scale up depth to the maximum capability also scale along width also scale along batch size And let let's like basically like in the same way that in language we're scaling on so many different aes can we unlock different dimensions of scaling as well and what capabilities and how far can we push the frontier of training these RO agents from doing that >> before we pass Sean uh when you say</p>\n<p><strong>[24:05]</strong></p>\n<p>enough compute what kind of compute budget did you have how does it how I just want to see what you guys got >> good question so we we wanted to make sure that this is we we wanted to make it such that like uh you know it's quite accessible so actually the nice thing is that all of our experiments even the thousand networks can be run on one single 80 gigabyte H100 GPU. >> Um, >> so that's dollars. >> Yeah. >> Right. Right. Right. So everything can be run on one GPU. Um, but in theory if we had, you know, like a distributed training setup and like can just like blast compute through this and really wanted to push the frontier, it'd be</p>\n<p>very interesting to see how things go. >> Yep. Cool. >> Uh, and I've actively been trying to learn as much as I can about vision language action models, um, role models at Europe's going to a lot of >> machine language action models. >> Vision language. >> Vision language. Yeah. Yeah. Um and yeah, curious about applications of representative for these. Yeah, exactly. For robotics. Um actively trying to explore more in that area. So just reading a lot of literature, talking to as many as I can. >> Yeah, we just released our episode with uh General Intuition. >> Okay. Um where if you know a bit about their history, they started as a gaming</p>\n<p>clipping company and uh they basically have a vision language action model >> which um I I saw I saw a preview. It was very impressive. I'm not sure exactly how transferable it is to embodied use cases, but it doesn't have to like screen is fine, you know, like Yeah. I I don't know if you have any takes on. >> Yeah, it's an exciting research direction. Definitely. >> Yeah, I I think um the the the the concept of actions as as something that you are outputting is actually not that</p>\n<p>popular in industry, right? O only because text has completely dominated the last three years and tool calling and which is a just another form of structured text. Uh and and I I feel like the uh action research is is kind of like I don't know how I don't know what needs to happen in order to unlock the next phase in in that. I don't know if you anything interesting out here shout it out. Yeah, there's a lot of cool work on like leveraging pre-trained VLMs and</p>\n<p>>> you freeze it and then you apply and then you on top of that like some sort of experts to output actions. Um also like systems for doing like hierarchical planning maybe outputting some higher level plan that and this is like a larger network that takes a long time to a little longer to do inference and so it outputs its plans with less frequency like some sort of chunk and then from there there's like some sort of uh second system that operates a bit more fast. I think there's quite a bit of interesting research in that direction. So that's sort of what I'm looking forward to.</p>\n<p>>> Cool. Final question. Uh hardest question you were asked at the postal session or just favorite encounter anyone famous that you met. >> So I actually haven't gotten a chance to go to the conference that much. I'm actually working full-time now. So uh [laughter] yeah. Uh so so far I I actually literally just got my badge like a few moments before session. So I guess I wouldn't be the best to answer that question. >> No, no, no. Like you like people ask you</p>\n<p><strong>[27:07]</strong></p>\n<p>stuff, right? Oh. Oh, that might I might post >> because people asking you or meeting you and like you know just just give a vibe of like what people are saying and >> yeah I think people were very I think it it's sort of like a very eye opening I think that the general question is that people thought it was a very eye opening paper because like the objective is quite simple. It's quite elegant and for us to be able to like you know like I I don't want to say like overturn but like sort of challenge the conventional wisdom that like RL is not super scalable and push it to such limits like a thousand layers deep and see continue</p>\n<p>improve performance. I think the general impression that I've gotten is that, you know, this this this could be like a really cool like if we can sort of build along this direction and that like we can really scale along all these different dimensions and push the frontier of the ability for RL. I'm very curious to see how that goes. >> All right. Well, thank you so much for dropping by. Uh, congrats on the paper again. Yeah. >> And, uh, good luck in your future work. >> Thank you. Thanks for having us. Yeah. Yeah. Thanks. >> [music]</p>\n<p>[music]</p>"}