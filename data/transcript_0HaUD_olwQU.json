{"video_id": "0HaUD_olwQU", "title": "SWE-Bench Verified is Contaminated: What Comes Next \u2014 with OpenAI Frontier Evals team", "link": "https://www.youtube.com/watch?v=0HaUD_olwQU", "published": "2026-02-23T18:33:04+00:00", "summary": "Olivia Watkins (Frontier Evals team) and Mia Glaese (VP of Research at OpenAI, leading the Codex, human data, and alignment teams) discuss a new blog post (https://openai.com/index/why-we-no-longer-evaluate-swe-bench-verified/) arguing that SWE-Bench Verified\u2014long treated as a key \u201cNorth Star\u201d coding benchmark\u2014has become saturated and highly contaminated, making it less useful for measuring real coding progress. \n\nSWE-Bench Verified originated as a major OpenAI-led cleanup of the original Princeton SWE-Bench benchmark, including a large human review effort with nearly 100 software engineers and multiple independent reviews to curate ~500 higher-quality tasks. But recent findings  show that many remaining failures can reflect unfair or overly narrow tests (e.g., requiring specific naming or unspecified implementation details) rather than true model inability, and cite examples suggesting contamination such as models recalling repository-specific implementation details or task identifiers. \n\nFrom now on, OpenAI plans to stop reporting SWE-Bench Verified and instead focus on SWE-Bench Pro (from Scale), which is harder, more diverse (more repos and languages), includes longer tasks (1\u20134 hours and 4+ hours), and shows substantially less evidence of contamination under their \u201ccontamination auditor agent\u201d analysis. \n\nWe also discuss what future coding/agent benchmarks should measure beyond pass/fail tests\u2014longer-horizon tasks, open-ended design decisions, code quality/maintainability, and real-world product-building\u2014along with the tradeoffs between fast automated grading and human-intensive evaluation. \n\n00:00 Meet the Frontier Evals Team\n00:56 Why SWE Bench Stalled\n01:47 How Verified Was Built\n04:32 Contamination In The Wild\n06:16 Unfair Tests And Narrow Specs\n08:40 When Benchmarks Saturate\n10:28 Switching To SWE Bench Pro\n12:31 What Great Coding Evals Measure\n18:17 Beyond Tests Dollars And Autonomy\n21:49 Preparedness And Future Directions", "transcript_html": "<p><em>No transcript available for this video.</em></p>", "cleanup_applied": false, "cleanup_reason": "no_transcript"}