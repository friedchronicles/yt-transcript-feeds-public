{"video_id": "JTllHR8DfQE", "title": "Is AI actually increasing productivity? \u2013 Dario Amodei", "link": "https://www.youtube.com/watch?v=JTllHR8DfQE", "published": "2026-02-16T17:15:03+00:00", "summary": "Full episode: https://youtu.be/n1E9IZfvGMA\nMe on twitter: https://x.com/dwarkesh_sp\n\nDario Amodei thinks we are just a few years away from \u201ca country of geniuses in a data center\u201d. In this episode, we discuss what to make of the scaling hypothesis in the current RL regime, how AI will diffuse throughout the economy, whether Anthropic is underinvesting in compute given their timelines, how frontier labs will ever make money, whether regulation will destroy the boons of this technology, US-China competition, and much more.", "transcript_html": "<p><strong>[00:00]</strong></p>\n<p>Right? We're trying to keep this 10x revenue curve going. There's like there is zero time for There is zero time for feeling like we're productive when we're not. Like these tools make us a lot more productive. Like why why do you think we're concerned about competitors using the tools? Because we think we're ahead of the competitors and like we don't we don't want to excel. We we we we wouldn't be going through all this trouble if this was secretly reducing reducing our productivity. Like</p>\n<p>we see the end productivity every few months in the form of model launches. Like there's no kidding yourself about this. Like the models make you more productive. >> Um one that is people feeling like they're more productive is qualitatively predicted by studies like this. But two, if I just look at the end output, obviously you guys are making fast progress. But the fact, you know, the the the idea was supposed to be with with recursive self-improvement is that you make a better AI, the AI helps you build a better next AI, etc., etc. And what I see instead, if I look at the you</p>\n<p>open AI, deep mind, is that people are just shifting around the podium every few months. And maybe you think that stops because you you won or whatever, but um but why why are we not seeing the person with the best coding model have this lasting advantage if in fact there are these enormous productivity gains from the last model? >> So no, no, no. I I I mean I mean I mean I think it's all like my my model of the situation is there's there's an advantage that's gradually growing. Like I would say right now the coding models give maybe I don't know a a like 15</p>\n<p>maybe 20% total factor speed up like that's my view. Um uh and 6 months ago it was maybe 5%. And so and so it didn't matter like 5% doesn't register. It's now just getting to the point where it's like one of several factors that that kind of matters and and that's gonna that's going to keep speeding up. And so I think 6 months ago like you know there were several there were several companies that were at roughly the same point because uh you know this this wasn't uh this wasn't a notable factor</p>\n<p>but I think it's starting to speed up more and more. I you know I I would I would also say there are multiple companies that you know write models that are used for code and you know we're not perfectly good at you know preventing some of these other companies from from from using from from from kind of using our models internally. Um, so, uh, you know, I think I think everything we're kind kind of everything we're seeing is consistent with this kind of, um, this kind of snowball model where where, you know, there's no hard. Again, my my my my theme in all of this is like</p>\n<p>all of this is soft takeoff, like soft smooth exponentials, although the exponentials are relatively steep. And so and so we're seeing this snowball gather momentum where it's like 10% 20% 25% you know for 40% and as you go yeah AMD doll's law you have to get all the like things that are preventing you from</p>\n<p><strong>[03:01]</strong></p>\n<p>from closing the loop out of the way but like this is one of the biggest priorities within anthropic. If you enjoyed this clip you can watch the full episode here and subscribe for more clips. Thanks.</p>"}