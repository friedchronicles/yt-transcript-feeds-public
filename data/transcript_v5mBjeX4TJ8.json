{"video_id": "v5mBjeX4TJ8", "title": "Artificial Analysis: The Independent LLM Analysis House \u2014 with George Cameron and Micah Hill-Smith", "link": "https://www.youtube.com/watch?v=v5mBjeX4TJ8", "published": "2026-01-09T00:18:47+00:00", "summary": "don\u2019t miss George\u2019s AIE talk: https://www.youtube.com/watch?v=sRpqPgKeXNk\n\u2014-\nFrom launching a side project in a Sydney basement to becoming the *independent gold standard for AI benchmarking*\u2014trusted by developers, enterprises, and every major lab to navigate the exploding landscape of models, providers, and capabilities\u2014*George Cameron* and *Micah Hill-Smith* have spent two years building *Artificial Analysis* into the platform that answers the questions no one else will: *Which model is actually best for your use case? What are the real speed-cost trade-offs? And how open is \"open\" really?*\nWe discuss:\n\n* The *origin story:* built as a side project in 2023 while Micah was building a legal AI assistant, launched publicly in January 2024, and went viral after Swyx's retweet\n* Why they *run evals themselves:* labs prompt models differently, cherry-pick chain-of-thought examples (Google Gemini 1.0 Ultra used 32-shot prompts to beat GPT-4 on MMLU), and self-report inflated numbers\n* The *mystery shopper policy:* they register accounts not on their own domain and run intelligence + performance benchmarks incognito to prevent labs from serving different models on private endpoints\n* How they make money: *enterprise benchmarking insights subscription* (standardized reports on model deployment, serverless vs. managed vs. leasing chips) and *private custom benchmarking* for AI companies (no one pays to be on the public leaderboard)\n* The *Intelligence Index* (V3): synthesizes 10 eval datasets (MMLU, GPQA, agentic benchmarks, long-context reasoning) into a single score, with 95% confidence intervals via repeated runs\n* *Omissions Index* (hallucination rate): scores models from -100 to +100 (penalizing incorrect answers, rewarding \\\"I don't know\\\"), and Claude models lead with the lowest hallucination rates despite not always being the smartest\n* *GDP Val AA:* their version of OpenAI's GDP-bench (44 white-collar tasks with spreadsheets, PDFs, PowerPoints), run through their Stirrup agent harness (up to 100 turns, code execution, web search, file system), graded by Gemini 3 Pro as an LLM judge (tested extensively, no self-preference bias)\n* The *Openness Index:* scores models 0-18 on transparency of pre-training data, post-training data, methodology, training code, and licensing (AI2 OLMo 2 leads, followed by Nous Hermes and NVIDIA Nemotron)\n* The *smiling curve of AI costs:* GPT-4-level intelligence is 100-1000x cheaper than at launch (thanks to smaller models like Amazon Nova), but frontier reasoning models in agentic workflows cost more than ever (sparsity, long context, multi-turn agents)\n* Why *sparsity might go way lower than 5%:* GPT-4.5 is ~5% active, Gemini models might be ~3%, and Omissions Index accuracy correlates with total parameters (not active), suggesting massive sparse models are the future\n* *Token efficiency vs. turn efficiency:* GPT-5 costs more per token but solves Tau-bench in fewer turns (cheaper overall), and models are getting better at using more tokens only when needed (5.1 Codex has tighter token distributions)\n* V4 of the Intelligence Index coming soon: adding GDP Val AA, Critical Point, hallucination rate, and dropping some saturated benchmarks (human-eval-style coding is now trivial for small models)\n\n\u2014\nArtificial Analysis\n\n* Website: https://artificialanalysis.ai (https://artificialanalysis.ai (\\))\n* George Cameron on X: https://x.com/grmcameron (https://x.com/grmcameron (\\grmcameron\\\" (\\)))\n* Micah Hill-Smith on X: https://x.com/_micah_h (https://x.com/_micah_h (\\_micah_h\\\" (\\)))\n\n00:00:00 Introduction: Full Circle Moment and Artificial Analysis Origins\n00:01:08 Business Model: Independence and Revenue Streams\n00:04:00 The Origin Story: From Legal AI to Benchmarking\n00:07:00 Early Challenges: Cost, Methodology, and Independence\n00:16:13 AI Grant and Moving to San Francisco\n00:18:58 Evolution of the Intelligence Index: V1 to V3\n00:27:55 New Benchmarks: Hallucination Rate and Omissions Index\n00:33:19 Critical Point and Frontier Physics Problems\n00:35:56 GDPVAL AA: Agentic Evaluation and Stirrup Harness\n00:51:47 The Openness Index: Measuring Model Transparency\n00:57:57 The Smiling Curve: Cost of Intelligence Paradox\n01:04:00 Hardware Efficiency and Sparsity Trends\n01:07:43 Reasoning vs Non-Reasoning: Token Efficiency Matters\n01:10:47 Multimodal Benchmarking and Community Requests\n01:14:50 Looking Ahead: V4 Intelligence Index and Beyond", "transcript_html": "<p><strong>[00:06]</strong></p>\n<p>This is kind of a full circle moment [music] for us in a way. Um cuz >> the like first time artificial analysis got mentioned on a podcast was you and Alysia on Lena Space. >> Amazing. >> Which was January 24. >> I I don't even remember doing that, but [laughter] yeah, it was it was very influential to me. Um, yeah, I'm looking at AI news for Jan 17 or Jan 16, 2024. Uh, I said this gem of a models and host comparison site was just launched. And, uh, and then I put in a few screenshots and I said, um, it's an independent</p>\n<p>third party. It clearly outlines the quality versus throughput tradeoff and it breaks out by model and hosting provider. Uh, I did give you for missing fireworks and [laughter] uh, how do you have a model benchmarking thing without fireworks? But you had together, you had Perplexity, and uh I think we just started chatting there. Welcome George and Micah uh to Lyn Space. You I've been following your progress. Congrats on an amazing year. You guys have really come together to be the presumptive new gardener of AI. Okay. How do I pay you [laughter]</p>\n<p>>> and let's get right into that. How do you make money? >> Um well, very happy to talk about that. So, it's been a like big journey the last couple years. artificial analysis is going to be 2 years old in January 2026, which is pretty soon now. >> We first run like the website for free obviously and give away a ton of data to help developers and companies navigate AI and make decisions about models, providers, technologies across the AI stack for building stuff. We're very committed to doing that intend to keep</p>\n<p>doing that. We have along the way built a business that is working out pretty sustainably. We've got just over 20 people now and two main customer groups. So we want to be who enterprise look to for data and insights on AI. So we want to help them with their decisions about models and technologies for building stuff. And then on the other side we do private benchmarking for companies throughout the AI stack who build AI stuff. So no one pays to be on the website. We've</p>\n<p>been very clear about that from the very start because there's no use doing what we do unless it's independent AI benchmarking. >> Yeah. >> Um but turns out a bunch of our stuff can be pretty useful to companies building AI stuff. >> And is it like I'm a Fortune 500, I need advisor on objective analysis and I call you guys and you pull up a custom report for me. You come into my office and give me a workshop. What what what kind of engagement is that? So we have a benchmark and insight subscription which</p>\n<p>looks like standardized reports that cover key topics or key challenges enterprises face when looking to understand AI and choose between all the technologies. And so for instance one of the report is a model deployment report. How to think about choosing between serverless inference, managed deployment solutions or leasing chips and running</p>\n<p><strong>[03:08]</strong></p>\n<p>inference yourself is is is an example kind of decision that big enterprises uh face and it's hard to hard to reason through like this AI stuff is is really new to to everybody and so we try and help with our reports and insight subscription companies [snorts] navigate that. We also do custom private benchmarking and um so that's very different from the public benchmarking um that we publicize and there's no commercial model around that for for private benchmarking we'll at times create benchmarks run benchmarks to</p>\n<p>specs that enterprises want and we'll also do that sometimes for AI companies who have built things and we help them understand what they've built with private benchmarking um you know through the expertise mainly that we've developed through trying to support everybody uh publicly uh with our public benchmarks. >> Yeah. Let's talk about tech stack uh behind that. But okay, I'm going to rewind uh all the way to when you guys started this project. Uh you were all all the way in Sydney. >> Yeah. Well, Sydney, Australia for me. George was an SF, but he's Australian</p>\n<p>but moved here already. >> Yeah. And u I remember I that Zoom call with you. Um what was the impetus for starting artificial analysis in the first place? You know, he started with public benchmarks. And so let's let's start there and we'll go to the private stuff. >> Yeah. Why don't we even go back a little bit to like why we, you know, thought that it was that it was needed. >> Yeah. >> The story kind of begins like in 2022, 2023, like both George and I have been into AI stuff for quite a while. In 2023 specifically, I was trying to build a</p>\n<p>legal AI research assistant. So, it actually worked pretty well for for for its era, I would say. but was finding that the more you go into building something using LLMs, the more each bit of what you're doing ends up being a benchmarking problem. So had like this multi-stage algorithm thing trying to figure out what the minimum viable model for each bit was trying to optimize every bit of it. as you build that out, right? Like you're trying to think about accuracy, bunch of other metrics and</p>\n<p>performance and cost and mostly just no one was doing anything to independently evaluate all the models and certainly not to look at the trade-offs for speed and cost. So we basically set out just to build a thing that developers could look at to see the trade-offs between all of those things measured independently across all the models and providers. Honestly, it was probably meant to be a side project when we first started doing it. Like we didn't like get together and say like, \"Hey, like we're going to stop working on this</p>\n<p>stuff and like this is going to be our main thing.\" >> When I first called you, I think uh you you hadn't decided on starting a company yet. >> That's actually true. I don't even think Paul's like like George quit his job. I hadn't quit working my legal AI thing. Like it it was genuinely a side project. >> Yeah, we built it because we needed it as people building in the space and thought, \"Oh, other people might find it useful, too. So we'll buy a domain and link it to the versel deployment that that that we had [laughter]</p>\n<p><strong>[06:09]</strong></p>\n<p>and and and and and [clears throat] tweet about it and but very quickly it started getting attention. Thank you Swix for I think doing an initial retweet and spotlighting it there this project that that we released and then very quickly though it was useful to others but very quickly it became more useful as the number of models uh released accelerated. Uh we had mixture late time 7B and it was a key >> that's a fun one. >> Yeah. Like a a open source model that really changed the landscape and opened up people's eyes to other serverless</p>\n<p>inference providers and thinking about speed, thinking about cost and so it became more useful um quite quickly. >> Yeah. What I love talking to to people like you who sit across the ecosystem is well I have theories about what people want but you have data and that's obviously more more relevant. Uh but I want to stay on the origin story a little bit more. Um when you started out I would say I think the the status quo at the time was every paper would come out and they would report their numbers</p>\n<p>versus competitor numbers and that's basically it. And uh I remember I did the leg work. I think I think everyone has some version of Excel sheet or Google sheet where you just like copy and paste the numbers from every paper and just post it up there and then sometimes they don't line up because they're independently run and so your numbers are going to look uh better uh than the or your reproductions of other people's numbers going to look worse cuz you don't hold their models correctly or whatever whatever the the excuse is. I think then Stanford Helm Percy Lang's project would also have some some of</p>\n<p>these numbers and I don't know if there's any other source that you can site the way that if I were to start artificial at the same time you guys started I would have used the Luther AI's um eval framework uh harness. >> Yep. That was some cool stuff. Um, at the end of the day, right, running these emails, it's like, if it's a simple Q&A email, all you're doing is asking a list of questions and checking if the answers are right, which shouldn't be that crazy, but it turns out there are an enormous number of things that you've</p>\n<p>got to control for. And I mean back when we started the website like one of the reasons why we were we realized that we had to run the evals ourselves and couldn't just take um results from the labs was just that they would all prompt the models differently and when you're competing over a few points then >> you can um put the answer into the model [laughter] that in the extreme and like you get crazy cases like back when um Google did Gemini 1.0 Ultra and needed a number that was better than GB4 and um like constructed um I think never published</p>\n<p>like chain of thought examples 32 of them in every topic in MLU to run it to get the score like there are so many things that you >> they never shipped ultra right >> um >> that's on this one I mean it I'm sure it existed but yeah so we were pretty sure that we needed to run them ourselves and just run them in the same way across all the models and we were we were also dead certain from the start</p>\n<p><strong>[09:09]</strong></p>\n<p>You couldn't look at those in isolation. You needed to look at them alongside the cost and performance stuff. >> Yeah. Okay. A couple technical questions. I mean, so obviously I also thought about this and I didn't do it because cost. >> Did you did you not worry about cost? Were you funded already? Clearly not, but you know. >> No, we we well we definitely weren't at the start. So like um I mean we're paying for it personally at the start. So well the numbers weren't nearly as bad a couple years ago. So we like certainly um like incurred some costs but we were probably in the order of</p>\n<p>like hundreds of dollars of spend across all the benchmarking that we were doing stuff. It was like kind of fine. >> These days that's gone up an enormous amount for a bunch of reasons that we can talk about. Um but yeah, it it wasn't that bad cuz if you can also remember that like the number of models we were dealing with was hardly any and the complexity of the stuff that we wanted to do to evaluate them was a lot less like we were just asking some Q&A type questions and then one specific thing was for a lot of evals initially</p>\n<p>we were just like sampling an answer directly without letting the models think. We weren't even doing chain of fault stuff initially and that was the most useful way to get some results initially. >> Yeah. And so for if for people who haven't done this work, literally parsing the responses is a whole thing, right? Like because sometimes the models the models can answer any way they feel fit. And sometimes they actually do have the right answer but they just return the wrong format and they will get a zero for that unless you work it into your parser and that involves more work.</p>\n<p>And so there I mean but there's an open question whether you should give it points for not following your instructions on the format. So >> it depends what you're looking at, right? Because you can if you're trying to see whether or not it can solve a particular type of reasoning problem and you don't want to test it on its ability to do answer formatting at the same time, then you might want to use an LLM's answer extractor approach to make sure that you get the answer out no matter how it answered. But these days it's mostly less of a problem. Like if you instruct a model and give it examples of what the answers should look</p>\n<p>like, it can get the answers um in your format and then you can do like a simple reax. >> Yeah. Yeah. And then uh there's other questions around I guess sometimes if you have a multiple choice question sometimes there's a bias towards the first answer. So you have to randomize the responses all these nuances you like once you dig into benchmarks you're like I don't know how anyone believes the numbers on all [laughter] these things because it's it's so it's so dark magic. You've also yeah got like the um uh different degrees of variance and different benchmarks, right? So if if you if you run four question multi-</p>\n<p>choice on a modern reasoning model at the temperature suggested by the labs for their own models, the variance that you can see on a four question multi- choice eval is pretty enormous if you only do a single run of it and has a small number of questions especially. So like one of the things that we do is run an enormous number of all of our emails when we're developing new ones and doing upgrades to our intelligence index to bring in new things so that we can dial in the right number of repeats so that</p>\n<p><strong>[12:10]</strong></p>\n<p>we can get to the 95% confidence intervals that we're comfortable with so that when we pull that together we can be confident in intelligence index to at least as tight as like a plus or minus one at a 95% confidence. >> Yeah. Again, that just adds a straight multiple to the cost. And >> yeah, so that's one of many reasons that cost has gone up uh a lot more than linearly over the last couple years. >> We um we report a cost to run the artificial analysis intelligence index on our website and currently that's assuming one repeat. Okay. um in terms</p>\n<p>of how we report it because we want to reflect a bit about the waiting of the index but um but our cost is actually a lot higher than what we report there because of the repeats. [laughter] >> Yeah. Yeah. Yeah. And probably this is true but just checking they you don't have any special deals with the labs. They don't they don't discount it. You just pay out of pocket or out of your your sort of customer funds. >> Oh there is there is a mix. So we >> so so the the the issue is that sometimes they may give you a special endpoint which >> 100%. Yeah. Yeah. Yeah. Exactly. So we</p>\n<p>are um laser focused like on everything we do on having the best independent metrics and making sure that no one can manipulate them in any way. There are quite a lot of processes we've developed over the last couple of years to make that true for like one you bring up like right here of the fact that if we're working with a lab, if they're giving us a private endpoint to evaluate a model that it is totally possible that what's sitting behind that black box is not the same as they serve on a public endpoint. We're very aware of that. We have what</p>\n<p>we call a mystery shopper policy and so and we're totally transparent with all the labs we work with about this that we will register accounts not on our own domain and run both intelligence emails and performance benchmarks without them being able to identify it. And no one's ever had a problem with that because like a thing that turns out to actually be quite a good factor in the industry is that they all want to believe that none of their competitors could manipulate what we're doing either.</p>\n<p>>> That's true. I never thought about that. I've been in a database data industry prior and there's a lot of shenanigans around benchmarkings, right? So, I'm just kind of going through the mental laundry list. Did I miss anything else in that in this category of shenanigans? [laughter] >> I mean, okay. the the the biggest one like that I'll bring up like is more of a conceptual one actually than like direct shenanigans. It's that the things that get measured become things that get targeted by what they're trying to build. Right. Exactly. So that doesn't mean anything that we should really call</p>\n<p>shenanigans. Like I'm not talking about trading on test set but if you know that you're going to be great at a particular thing. If you're a researcher, there are a whole bunch of things that you can do to try to get better at that thing that preferably are going to be helpful for a wide range of how actual users want to use the thing that you're building, but will not necessarily do that. So, for instance, the models are exceptional now at answering competition maths problems.</p>\n<p><strong>[15:11]</strong></p>\n<p>[laughter] There is some uh relevance of that type of reasoning, that type of work um to like how we might use modern coding agents and stuff um but it's clearly not one for one. So the thing that we have to be aware of is that once an eval becomes the thing that everyone's looking at, the scores can get better on it without there being a reflection of overall generalized intelligence of these models getting better. That has been true for the last couple of years, it'll be true for the next couple of years. There's no silver bullet to defeat that other than building new</p>\n<p>stuff to stay relevant and measure the capabilities that matter most to real users. >> Yeah. And we we'll cover we'll cover some of the new stuff that you guys are building as well. Uh which is cool. Like you used to just run other people's evals, but now you're coming up with your own. And I think obviously that is a a necessary path once you're at the frontier. You've exhausted all the existing one-on-one ones. I think the next point in history that I have for you is AI Grant that you guys decided to to join and and move here. >> What's what was it like? I think you</p>\n<p>were in like batch two. >> Batch four. >> Batch four. Okay. >> I mean it was great. Nat and Daniel are obviously great and it's a really cool group of companies that we were in AI grant alongside. It was really great to get Nat and Daniel on board. Obviously, they've done a whole lot of great work in the space with a lot of leading companies and were extremely aligned with the mission of what we were trying to do. Like we're not quite typical of like a lot of the other AI startups that they've invested in and they were very much here for the mission of what we</p>\n<p>want to do. Did they say any advice that really affected you in some way or like were one of the events very impactful? >> That's an interesting question. I I mean I remember fondly a bunch of the um speakers who came into fireside chats at AI Grant >> which is also like a crazy list. >> Yeah. Oh totally. Yeah. Yeah. Yeah. I I I there was something about um you know speaking to that and Daniel about the challenges of of working through a startup and just working through the questions that don't have like clear answers and how to to work through those kind of methodically um and just like</p>\n<p>work through the hard decisions uh and they've been great mentors to to us as we built artificial analysis. Another benefit for us was that other companies in the batch and other companies in AI grant are pushing the capabilities of of what AI can do at this time. And so being in contact with them, making sure that artificial analysis is is useful to them has been fantastic for for supporting us in working out how should we build out artificial analysis to continue to being uh useful to those</p>\n<p>like you know building on AI. I think to some extent I'm mixed opinion on that one because to some extent your target audience is not people in AI grants who are obviously at the frontier >> to some to some extent but then so [laughter] a lot of what the AI grant companies are doing is um taking capabilities coming out of the labs and trying to push the</p>\n<p><strong>[18:11]</strong></p>\n<p>limits of what they can do across the entire stack for building great applications which actually makes some of them pretty archetypical power users of artificial analysis. some of the people with the strongest opinions about what we're doing well and what we're not doing well and what they want to see next from us. Cuz when you're building any kind of AI application now, chances are you're using a whole bunch of different models, you're maybe switching reasonably frequently for different models and different parts of your application to optimize what you're able to to do with them at an accuracy level and to get</p>\n<p>better speed and cost characteristics. So for many of them, no, they're um like not commercial customers of ours. Like we don't charge for all that data on the website, but they are absolutely some of our power users. >> So let's talk about just the the the evals as well, right? Like you start out from the the general like MMLU and and GPQA stuff. Um what's next? How do you how do you sort of build up to um the overall index was in V1 and how did you evolve it? Okay, so first just like background like we're talking about the</p>\n<p>artificial analysis intelligence index which is our synthesis metric that we pull together currently from 10 different email data sets to give what we're pretty confident is the best single number to look at for how smart the models are. Obviously doesn't tell the whole story. That's why we published the whole website of all the charts to dive into every part of it and look at the trade-offs. But best single number. So right now it's got in it a bunch of Q&A type data sets that have been very important to the industry like a couple that you just mentioned. It's also got a</p>\n<p>couple of agentic data sets. It's got our own long context reasoning data set and some other use case focused stuff. As time goes on the things that we're most interested in that are going to be important to the capabilities that are becoming more important of AI, what developers are caring about are going to be first around agenda capabilities. So surprise surprise, we're all loving our coding agents and how the model is going to perform like that and then do similar things for different types of work are really important to us. The linking to</p>\n<p>use cases to economically valuable use cases are extremely important to us. And then we've got some of these um things that the models still struggle with like working really well over long contexts that are not going to go away as specific capabilities and use cases that we need to keep evaluating. >> Mhm. And but I guess one thing I was driving was like the V1 versus the V2 and how bad it was over time >> like how like how we've changed the index to where we are. >> Yeah, I think that reflects on well the</p>\n<p>change in the industry, right? So that's a nice way to tell that story. >> Well, V1 would be completely saturated right now [laughter] by almost every model coming out because doing things like writing the Python functions in human eval is now pretty trivial. It's easy to forget actually I think how much progress has been made in the last two years. Like we we obviously play the game constantly of like the</p>\n<p><strong>[21:13]</strong></p>\n<p>today's version versus last week's version and the week before and all of the small changes in the horse race between the current frontier and the who has the best like smaller than 10b model like right now this week right and that's very important to a lot of developers and people in especially in this particular city of San Francisco. But when you zoom out a couple of years ago, literally most of what we were doing to evaluate the models then would all be 100% solved by even pretty small models today. And that's been one of the key things, by the way, that's driven down the cost of um intelligence at every tier of intelligence. We can talk</p>\n<p>about more in a bit. So V1, V2, V3, we made things harder. We covered a wider range of use cases and we tried to get closer to things developers care about as opposed to like just the Q&A type stuff that MMLU and GPQA represented. >> Yeah. I don't know if you have anything to add there. Uh or we could just go right into showing people the benchmark and like clicking around and ask asking questions about it. >> Yeah, let's do it. >> Okay. >> This would be a pretty good way to chat about a few of the new things we've</p>\n<p>launched recently. >> Yeah. And I think a little bit about the direction that we want to take it and we want to push benchmarking. Currently the intelligence index and and and eval focus a lot on kind of raw intelligence but we kind of want to diversify how we think about intelligence and uh we can we can talk about it but kind of new evals that we've kind of built um and partnered on focus on topics like hallucination and we've got a lot of topics that I think are not covered by the current eval set</p>\n<p>that should be uh and and so we want to bring that forth but before we get into that So, so for listeners just as a time stamp right now on number one is Gemini 3 Pro High, then followed by Claude Opus at 70. Uh just 5.1 high. You don't have 5.2 yet. Uh and Kimmy K2 thinking. Wow. Still hanging in there. So those those are the top four. >> That will date this podcast quickly. [laughter] >> Yeah. Yeah. I mean I I love it. I love it. >> This time next year and go how cute. >> Like totally a quick view of that is</p>\n<p>okay. There's a lot I love this chart. This is such a favorite, right? And almost every conferences and stuff like we always put this one up first to just talk about situating where we are in this moment in history. This I think is the the visual version of what I was saying before about the zooming out and remembering how much progress there's been. If we go back to just over a year ago, before 01, before Claude Sonet 3.5, we didn't have reasoning models or coding agents as a</p>\n<p>thing and the game was very, very different. If we go back even a little bit before then, we're in the era where when you look at this chart, like Open AI was untouchable for well over a year. And I mean you would remember that time period well of like there being very open questions about whether or not AI was going to be competitive like full stop whether or not open AI would just run away with it whether we would have</p>\n<p><strong>[24:13]</strong></p>\n<p>the a few frontier labs and no one else would really be able to do anything other than consume their APIs. I am quite happy overall that the world that we have ended up in is one where >> multimodel >> absolutely and strictly more competitive every quarter over the last few years. Yeah, this year has been insane. >> Yeah, you can see it. Uh, this chart with everything added is hard hard to read currently. Um, there's so many dots on it, but I think it reflects a little bit, you know, what what we felt like how crazy it's been. >> Why 14 uh as the default? Is that a</p>\n<p>manual choice? >> Cuz you got service now and there that are, you know, less less traditional names. >> Yeah, it's models that we're kind of highlighting by default in our charts in our intelligence index. Okay. is where this >> you just have a manually cured list of stuff. >> Yeah, that's right. But um something that I actually don't think every artificial analysis user knows is that you can customize our charts and and choose what >> uh what modelsated. >> Um and so if we you know take off a few names, it gets a little easier to >> Yeah. Yeah. A little easier to read.</p>\n<p>>> Yeah. But you can uh I love that you can see the 01 jump. Look at that. September 2024 >> and the Deep Seek jump [laughter] >> which got close to OpenAI's leadership. >> They were so close >> I think. Yeah, we we remember that moment around this time uh last year actually. >> Yeah. Yeah. Yeah. Well, couple of weeks. It was it was Boxing Day in New Zealand uh when when Deepseek V3 came out and I like we'd been tracking Deepseek and a bunch of the other global players that</p>\n<p>were less known over like the second half of 2024 and had runs on the earlier ones and stuff. I I very distinctly remember Boxing Day in New Zealand. I cuz I was with family for Christmas and stuff running and getting back result by result on Deep Seek V3. Um, so this was like the the first of their V3 architecture, the 671B MOE. Um, and we were very very impressed. Like that was the moment where we were sure that DeepS was no longer just one of many players,</p>\n<p>but had jumped up to be a thing. Um, the world really noticed when they followed that up with the RL working on top of E3 and R1 succeeding like a few weeks later. Um but the groundwork that absolutely was laid with a like just extremely strong base model completely open weights that we had as the best open weights model on Boxing Day last year. >> Yep. >> Boxing Day is the the day after Christmas for for those not [laughter] >> uh I mean I'm from Singapore. A lot of us remember Boxing Day uh for for a different reason for the tsunami that</p>\n<p>happened. >> Of course. Yeah. So I was Yeah. Yeah. But that was a long time ago. So yeah. So this is the the rough uh pitch of AQI uh or is it AAQI or A II? >> I I so good good memory though. So once upon a time we did call it quality index um and we would talk about um quality performance and price but um we changed it to intelligence. >> There's been a few naming changes. We added hardware benchmarking to the site</p>\n<p><strong>[27:15]</strong></p>\n<p>and um so have benchmarks at a at a kind of system level and so then we changed our throughput metric to we now call it output speed and then [laughter] throughput makes sense at a system level. So >> got it got it. Took that name. >> Uh take me through more charts like what should what should people know? You know obviously the way you look at the site is probably different than how a beginner might look at it. >> Yeah that that's fair. We can [laughter] there's a lot of fun stuff to dive into. Um maybe so we can hit past all the like we like we have lots and lots of emails and stuff. Um the interesting ones to talk about today that' be great to bring up like a few of our recent things I think um that probably not many people</p>\n<p>be familiar with yet. So first one of those is our omniscience index. So this one is a little bit different to most of the intelligence evos that we run. We built it specifically to look at the embedded knowledge in the models and to test um hallucination by looking at when the model doesn't know the answer. So we're not able to get it correct, what's its probability of saying I don't know or giving an incorrect answer. So the</p>\n<p>metric that we use for omniscience goes from negative 100 to positive 100 because we're simply taking off a point if you give an incorrect answer to the question. We're pretty convinced that this is an example of where it makes most sense to do that because it's strictly more helpful to say I don't know instead of giving a wrong answer to factual knowledge question. And one of our goals is to shift the incentive that EVELs create for um models and the labs</p>\n<p>creating them [clears throat] to get higher scores. And almost every email across all of AI up until this point, it's been graded by simple percentage correct as the main metric, the main thing that gets hyped. And so you should take a shot at everything. There's no incentive to say I don't know. So we did that for this one here. [clears throat] >> I think there's a general uh field of calibration as well like the confidence in your answer versus the rightness of the answer.</p>\n<p>>> Yeah, completely agree. Yeah. Yeah. >> On that and one reason that we didn't do that is be uh or put that into this uh index is that we think that the the way to do that is not to ask the models how confident they are. >> I don't know maybe >> it might be though >> you put it give it a JSON field say say confidence and maybe it spits out something. >> Yeah. You know, we have done a few eval podcast over the over the years and we did one with Clementine of Hugging Face who maintains the open source leaderboard and this was one of her top requests which is some kind of hallucination/ lack of confidence</p>\n<p>calibration thing >> and so hey this is one of them and I mean like anything that we do it's not a perfect metric or the whole story of everything that you think about as hallucination um but yeah it's pretty useful and has some interesting results like one of the that we saw in the hallucination rate is that anthropics claude models at the the very left hand side here with the lowest hallucination</p>\n<p><strong>[30:17]</strong></p>\n<p>rates out of the models that we've evaluated emissions on. That is an interesting fact. I think it probably correlates with a lot of the previously not really measured vibes stuff that people like about some of the claude models. >> Is the data set public or what's is it is there a held out set? >> There's a hel set for this one. Um so it we we have published a public test set but we we've only published 10% of it. The reason is that for this one here specifically it would be very very easy to um like have data contamination</p>\n<p>because it is just factual knowledge questions. Um we will update it over time to also um prevent that but we've yeah kept most of it held out so that we can keep it reliable for a long time. It leads us to a bunch of really cool things including breakdown quite granularly by topic and so we've got some of that disclosed on the website publicly right now and there's lots more coming in terms of our ability to break out very specific topics. >> Yeah, I would be interested. Let's let's dwell a little bit on this hallucination one. I noticed that haiku hallucin hallucinates less than sauna</p>\n<p>hallucinates less than opus and would that be the other way around in a normal capability environment? I don't know what's what do you make of that? One interesting aspect is that we've found that there's not really a not a strong correlation between intelligence and hallucination rate. That's to say that the smarter the the models are in a generalist sense isn't correlated with their ability to when they don't know something say that they don't know. It's interesting that Gemini 3 Pro preview was a big leap over here Gemini 2.5</p>\n<p>flash and and and 2.5 Pro. But and if I add pro quickly here, >> I bet pro is really good. Uh actually, no. So I me I meant uh the GPT pros. >> Oh yeah, >> cuz GPT pros are rumored. We don't know for a fact that it's like eight runs and then with the LM judge on top. >> Yeah. So we saw a big jump in this is accuracy. So this is just percent that they get uh correct. And Gemini 3 Pro knew a lot more than the other models. And so big jump in accuracy, but</p>\n<p>relatively no change between the Google Gemini models between releases >> and the hallucination rate. >> Exactly. And so it's likely due to just kind of different post training recipe between the the claw models. >> Yeah. >> Um that's driven this. >> Yeah. You can uh you can partially blame us and how we define intelligence having until now not defined hallucination as u negative in the way that we think about intelligence. And so that's what we're changing. Uh I know many smart people who are confidently incorrect.</p>\n<p>[laughter] >> Look look that that that is very human very true >> and there's times and a place for that. I think our view is that hallucination rate makes sense in this context where it's around knowledge but in many uh cases people want the models to hallucinate to have a go. Often that's the case in coding or when you're trying to generate newer ideas. One eval that we added to artificial analysis is is is</p>\n<p><strong>[33:18]</strong></p>\n<p>critical point and it's really hard uh physics problems. >> Okay. >> And is it sort of like a human eval type or something different or like a frontier math type? >> It's not dissimilar to frontier frontier math. So these are kind of research questions that kind of academics in the physics physics world would be able to answer but models really struggle to answer. So the top score here is 9%. And when the people that that created this like Minway and and actually Afia who was kind of behind Sweeten >> what organization is this or is this</p>\n<p>it's Princeton >> kind of range of academics from from different academic institutions really smart people they talked about how they turn the models up in terms of the temperature [laughter] as high a temperature as they can when they're trying to explore kind of new ideas in physics as a as a thought partner just because they they want the models to hallucinate um >> yeah sometimes maybe get something newact Um so not right in every situation but um think it makes sense you know to test hallucination scenarios where it makes sense. Well, so the obvious question is uh this is one of many that there is</p>\n<p>there every lab has a system card that shows some kind of hallucination number and you've chosen to not endorse that and you've made your own and I think that's a that's a choice. Um totally in some sense the rest of artificial analysis is public benchmarks that other people can independently rerun. you provided us a service here. you have to fight the well who are we to to like do this and your answer is that we have a lot of customers and you know but like I guess how do you converge the industry on one number that actually everyone</p>\n<p>agrees is is the rate right cuz you have your numbers they have their numbers never the the two shall meet >> I mean I think I think for hallucination specifically there are a bunch of different things that you might care about reasonably and that you'd measure quite differently like we've called this a nissian hallucination rate not trying to declare the legacy. >> Humanity's last hallucination, [laughter] >> you could uh you could have some interesting naming conventions and all this stuff. Um the biggest picture answer to that and something that I actually wanted to mention just as George was explaining critical point as</p>\n<p>well is so as we go forward we are building evals internally. We're partnering with academia and partnering with AI companies to build great evals. We have pretty strong views on in various ways for different parts of the AI stack where there are things that are not being measured well or things that developers care about that should be measured more and better and we intend to be doing that. We're not obsessed necessarily with that everything we do we have to do entirely within our own team. Critical point is a cool example of where we were launch partner fraud</p>\n<p>working with academia. We've got some partnerships coming up with a couple of leading companies. Those ones obviously we have to be careful on some of the independent stuff but with the right disclosure like we're completely comfortable with that. A lot of the labs have released great data sets in the past that we've used to create success independently and so it's between all of those techniques we're going to be releasing more stuff in the future. >> Cool. Let's cover the the last couple and then we'll I want to talk about your</p>\n<p><strong>[36:19]</strong></p>\n<p>trends analysis stuff you know. >> Totally. Before that actually I have one like little factoid on if you go back up to accuracy on omniscience. An interesting thing about this accuracy metric is that it tracks more closely than anything else that we measure the total parameter count of models. >> Oo, >> makes a lot of sense intuitively, right? Because this is a knowledge eval. This is the pure knowledge metric. We're not looking at the index and the hallucination rate stuff that we think is much more about how the models are trained. This is just what facts did they recall and yeah, it tracks</p>\n<p>parameter count extremely closely. Okay. What's the rumored size of Jupit 3 Pro? [laughter] And to be clear, not confirmed for any official source, just just rumors, but rumors do fly around. Rumors >> I get I hear all sorts of numbers. I don't know what to trust. >> So if you if you draw the line on emissian's accuracy versus total parameters, we've got all the open ways models. You can squint and see that likely the leading frontier models right now are quite a lot bigger than the one trillion parameters that the open</p>\n<p>weights models cap out at. And the ones that we're looking at here, there's an interesting extra um data point that Elon Musk revealed recently about XAI that Grock 3 trillion parameters for Grock 3 and 4, 6 trillion for Gro 5, but that's not out yet. Take those together, >> have a look, you might reasonably form a view that there's a pretty good chance that Gemini 3 Pro is bigger than that, that it could be in the 5 to 10 trillion parameter range. To be clear, I have absolutely no idea. But just based on</p>\n<p>this chart, like that's where you would you would land if you have a look at it. >> Yeah. And to some extent, I actually kind of discourage people from guessing too much because what does it really matter? Like as long as they can serve it as a sustainable cost, that's about it. Like >> yeah, totally. They've also got different incentives in play compared to like open weights models who are thinking to supporting others in self- deployment for the labs who are doing inference at scale. It's I think less about total parameters in many cases when think about inference costs and and more around number of active parameters</p>\n<p>and so there's a bit of an incentive towards larger sparer models. >> Agreed. Understood. >> Yeah. Great. >> I mean obviously if you're a developer or company using these things not exactly as you say it doesn't matter. You should be looking at all the different ways that we measure intelligence. You should be looking at cost to run index number and the different ways of thinking about token efficiency and cost efficiency based on the list prices cuz that's all that matters. >> It's not as good for the content creator rumor mill where I can say well GPT4 is this small circle. Look at GPT5 is this big circle and that it used to be a</p>\n<p>thing for a while. [laughter] >> I mean but that that that is like a on its own actually very interesting one right that >> is it? [laughter] Well, no, just purely that chances are the last couple years haven't seen a dramatic scaling up in the total size of these models and so there's a lot of room to go up probably and total size of the models especially with the upcoming hardware generations. >> Yes. So, um you know taking off my</p>\n<p><strong>[39:21]</strong></p>\n<p>posting phase for a minute. [laughter] Uh yes. Yes. At the same time, I I do feel like, you know, especially coming back from Europe, people do feel like Ilia is probably right that the paradigm is doesn't have many more orders of magnitude to scale out more and therefore we need to start exploring at least a different path. >> GDP val I think it's like only like a month or so old. Um I was also very positive when it first came out. I actually talked to Tedel uh who was the the lead researcher on that. Oh >> and uh you have your own version. >> It's a fantastic data set. >> Yeah. And >> maybe I will recap for people who are</p>\n<p>still out of it. It's like 44 tasks that based on some kind of GDP cutoff that's like meant to represent broad white color work that is not just coding. >> Yeah. >> Yeah. >> Each of the tasks have a whole bunch of detailed instructions, some input files for a lot of them. It's I within the 44 is divided into like 220 22 to5 maybe um subtasks that are the the level of that we run through the agents and yeah, they're really interesting. I will say that it doesn't necessarily capture like all the stuff that people do at work. No avail is perfect. There's always going</p>\n<p>to be more things to look at largely because in order to make the tasks well enough to find that you can run them, they need to only have a handful of files and very specific instructions for that task. And so I think the easiest way to think about them are that they're like quite hard take-home exam tasks that you might do in an interview process. >> Yeah. Yeah, for listeners is not no longer like a long prompt. It is like well here's a zip file with like a spreadsheet or a PowerPoint deck or a PDF and go nuts and answer this question.</p>\n<p>>> Yeah, OpenAI released a great data set and they released a good paper which looks at performance across the different you know web chat bots on the data set. It's a great paper encourage people to read it. What we've done is taken that data set and turned it into an EVEL that can be run on any model. So we created a reference agentic harness that can run the models on the data set and then we developed a valuator approach to compare outputs that's kind of a AI enabled. So it uses Gemini 3 pro</p>\n<p>preview to compare results which we tested pretty comprehensively to ensure that it's aligned to to human human preferences. One data point there is that um even as the as as an evaluator Gemini 3 Pro interestingly doesn't do actually that well in GDP val aa >> yeah the thing that you have to watch out for with OM judge is selfreference that models usually prefer their own output uh and in this case it was not >> totally I think the the way that we're</p>\n<p>that we're thinking about the places where it makes sense to use an LLM's judge approach now like quite different to some of the early LLM's judge stuff a couple of years ago because some of that and MTV was a great project that was a good example of some of this a while ago was about judging conversations and like a lot of style type stuff. Here we've got the task that the grader grading</p>\n<p><strong>[42:22]</strong></p>\n<p>model is doing is quite different to the task of taking the test. When you're taking the test, you've got all of the agentic tools. You're working with the code interpreter and web search the file system to go through many many turns to try to create the documents. Then on the other side when we're grainy it we're running it through a pipeline to extract visual and text versions of the files and be able to provide that to Gemini and we're providing the criteria for the task and getting it to pick which one more effectively meets the criteria of the task out of two potential outcomes. It turns out that we prove that it's just very very good at getting that</p>\n<p>right matched with human preference a lot of the time because it's I think it's got the raw intelligence but it's combined with the correct representation of the outputs. the fact that the outputs were created with an agentic task that is quite different to the way the grading model works and we're comparing it against criteria not just kind of zero shot trying to ask the model to pick which one is better. >> Got it. Why is this an ELO and not a percentage like GDP val? >> So the outputs look like documents and there's video outputs or audio outputs</p>\n<p>from some of the tasks and >> so he has to make a video. >> Yeah, >> for some of the tasks. What task is that? [laughter] >> I mean, it's in it's in the data set. >> Maybe a YouTuber. >> It's a marketing video. >> Oh. >> Uh, what? >> Like model has to go find clips on the internet and try to put it together. The models are not that good at doing that one for now. To be clear, it's pretty it's pretty hard to do that with the code interpreter. [laughter] Um, and the computer stuff doesn't work quite well enough and so on and so on. But, um, yeah. >> And so there's no kind of ground truth necessarily to compare against to work</p>\n<p>out percentage correct. it's hard to come up with correct or in in incorrect there and so it's on a relative basis and so we use an ELO approach to compare outputs from uh each of the models um between between the task >> you know what you should do you should you should pay a contractor human to do the same task and then give it an ELO >> and then so you have you have human there so I think what's helpful about GDP val the open eye one is that 50% is meant to be normal human and and and</p>\n<p>maybe domain expert is higher than that but 50% was the the bar for like well if you've crossed 50 you are super human. >> Yeah. So we like haven't grounded this score in that. Exactly. I agree that it can be helpful but we wanted to generalize this to a very large number of models. It's one of the reasons that presenting as law is quite helpful and allows us to add models and it'll stay relevant for quite a long time. I also think it it it can be tricky looking at these exact tasks compared to the human performance cuz the way that you would</p>\n<p>go about it as a human is quite different to how the models would go about it. >> Yeah. [snorts] Uh I also like that you included Llama for Maverick in there. Is that like just one last like [laughter] >> No, no, no, no, no, no. It is the it is the best model released by Meta and [laughter] so it makes it into the homepage default set still for now. Other inclusion</p>\n<p><strong>[45:23]</strong></p>\n<p>that's quite interesting is we also ran it across the latest versions of the web chat bots and so we have >> Oh, that's right. Oh, sorry. I Yeah, I completely missed that. Okay. >> No, not at all. So that that which has a checkered pattern. >> So So that is their harness, not yours is what you're saying. >> Exactly. And what's really interesting is that if you compare for instance Claude 4.5 Opus using the Claude web chatbot, it performs worse than the model in our agentic harness. >> And so in every case, the model performs better in our agentic harness than its</p>\n<p>web chatbot counterpart, the harness that they created. >> Oh, my backwards explanation for that would be that well, it's meant for consumer use cases. And here you're pushing it for something. >> The conraints are different and the amount of freedom you can give the model is different. Also, you like have a cost goal. We Yeah. Let the models work as long as they want basically. Yeah. >> You copy paste manually into the chatbot. >> Yeah. >> Yeah. >> That's >> that was how we got the chatbot reference. Yeah. >> We're not going to be keeping those updated at like quite the same scale as on the [laughter] hundreds of models on</p>\n<p>>> So, I don't know. Talk to browser base. They'll they'll automate it for you, you know, like >> true. Yeah, we should >> uh I have thought about like well we should turn these chatbot versions into an API because they are legitimately different agents in themselves. >> Yes. Yeah. And that's grown a huge amount over the last year, right? Like the tools that are available have actually diverged in my opinion a fair bit across the major chatbot apps and the amount of data sources that you can connect them to have gone up a lot. Meaning that your experience and the way you're using the model is more different</p>\n<p>than ever. what tools and what data connections come to mind when you say what's interesting what what what's notable work that people have done. >> Ah okay. So my favorite example on this is that until very recently I would argue that it was basically impossible to get an LLM to draft an email for me in any useful way because most times you're sending an email. You're not just writing something for the sake of writing it. Chances are context required is a whole bunch of historical emails. Maybe it's notes that you've made. Maybe it's meeting notes. Maybe it's um</p>\n<p>pulling something from your um any of like wherever you at work store stuff. So for me like Google Drive, one drive um in our superbase databases if we need to do some analysis on some data or something preferably model can be plugged into all of those things and can go do some useful work based on it. The things that like I find most impressive currently that I am somewhat surprised work really well in late 2025 are that I can have models use superbase MCP to >> query</p>\n<p>>> read only of course run a whole bunch of SQL queries to do pretty significant data analysis and make charts and stuff and can read my Gmail and my notion >> and okay you actually use that that's good that's that's good is that a cloud thing >> to various degrees supported both JBD and claude right now I would say that this stuff like barely works in fairness right now. [laughter] Um like</p>\n<p><strong>[48:24]</strong></p>\n<p>>> because people are actually going to try this after they hear. >> If you get an email from Mic, odds are it wasn't written by a chatbot. >> No. So yeah, I think it is true that I have never actually sent anyone an email drafted by a chatbot yet. Um and so >> but [laughter] you can you can feel it, right? And this time this time next year we'll come back and see where it's going. >> Totally. >> Um Superbase shout out another famous Kiwi. Uh I don't know if you've you've any conversations with him about anything in particular on on AI building and AI infra. >> We have had Twitter DMs um with with him because we're quite big uh Superbase</p>\n<p>users and and power users and we probably do some things more manually than we should in in Superbase. [laughter] >> So he's just the support line because you're you're >> a little bit yeah been being super friendly. [laughter] One extra um point regarding um GDP val AA is that on the basis of the overperformance of the models compared to the the chat bots. Turns out we realized that oh like our reference harness that we built actually works quite well on like g generalist agentic</p>\n<p>tasks. >> This proves it in a sense. And so the agent harness is very minimalist. I think it follows some of the ideas that are in clawed code. And we all that we give it is context management capabilities a web search web browsing uh tool uh code execution uh environment anything else >> I mean we can equip it with more tools but like by default yeah that's we we give it for GDPL tool to uh view an image specifically um because the models</p>\n<p>you know can just use a terminal to pull stuff in text form into context but to pull visual stuff into context we had to give them a custom tool but yeah exactly you to explain an expert. >> No, so it's it it we turned out that we created a good generalist agentic harness and so we um released that on on GitHub yesterday. It's called Stirrup. So if people want to check it out and and it's a great um you know base for you know generalist building a generalist agent. >> It is kind of >> for more specific tasks. I'd say the best way to use it is get clone and then</p>\n<p>have your favorite coding agent make changes to it to do whatever you want because it's not that many lines of code and the coding agents can work with it super well. >> Well, that's nice for the the community to explore and share and hack on it. I think maybe in in in other similar environments the terminal bench guys have done uh start harbor uh and so it's it's a bundle of well we need our minimal harness which for them is terminus. Yep. And we also need the RL environment or docker deployment thing to to run independently. So I don't know</p>\n<p>if you've looked into hardware at all. Is that is that like a standard that people want to adopt? >> Yeah, we've looked at it from a eval um perspective and we love terminal bench and and host benchmarks of of terminal bench on on artificial analysis. Um we've looked at it from a from a coding agent um perspective but could see it being a great um basis for any kind of</p>\n<p><strong>[51:26]</strong></p>\n<p>agents. I think where we're getting to is that these models have gotten smart enough, they gotten better better at tools that they can perform better when just given a minimalist set of tools and and let them run. Let the model control the the agentic workflow rather than using another framework that's a bit more built out that tries to dictate the dictate the flow. >> Awesome. Let's cover openness index and then let's go into the report stuff. Uh so that's the that's the last of the proprietary numbers I guess. I don't know how you sort of classify all these. >> Yeah. Or call call it let's call it the</p>\n<p>last of like the the three new things that we're talking about from like the last few weeks. Um because I mean there's a we do a mix of stuff that where we're using open source where we open source and what we do and um proprietary stuff that we don't always open source like long context reasoning data said last year we did open source um and then all of the work on performance benchmarks across the site. Some of them we looking to open source but some of them like we're constantly iterating on and so on and so on. There's a huge mix I would say just of like stuff that is open source and not across the side.</p>\n<p>>> So that's a LCR for people. >> Yeah. Yeah. >> But let's talk about open >> let's talk about open syntax. This here is call it like a new way to think about how open models are. We for a long time have tracked where the models are open weights and what the licenses on them are. And that's like pretty useful that tells you what you're allowed to do with the weights of a model. But there is this whole other dimension to how open models are that is pretty important that we haven't tracked until now. And that's how much is disclosed about how it was</p>\n<p>made. So transparency about data, pre-training data and post- training data and whether you're allowed to use that data and transparency about methodology and training code. >> Mhm. >> So basically those are the components. We bring them together to score an openness index for models so that you can in one place get this full picture of how open models are. >> I feel like I've seen a couple other people try to do this but it they're not maintained. I I do think this does matter. I don't know what the numbers</p>\n<p>mean apart from is there a max number? Is this out of 20? >> It's out of 18 currently. And so we've got an openness index um page but essentially these are points. So you get points for being more open across these different categories and the the maximum you can achieve is 18. So AI 2 with their extremely open 32B think model is a leader in a sense >> with hugging face. >> Oh with their with their smaller model. Um it's coming soon. I think we need to run we need to get the intelligence benchmarks run on the side.</p>\n<p>>> We can't have an open index and not include hugging face. >> We love hugging face. We'll have that have that up very soon. >> I mean uh you know the refined web and all all that stuff. It's uh it's amazing. Or is it called fine web? Fine web. >> Fine web. Yeah. Yeah. Totally. Yeah. One of the reasons this is cool, right, is that if you're trying to understand the holistic picture of the models and what you can do with all the stuff the company's contributing, this gives you that picture. And so we are going to</p>\n<p><strong>[54:26]</strong></p>\n<p>keep it up to date alongside all the models that we do intelligent settings on on the site. And it's just an extra view to understand. >> Can you scroll down to the the trade-offs chart? Yeah. Yeah, that one. Yeah. This this really matters, right? Obviously, cuz you can be super open but dumb. >> I mean, [laughter] >> the slant obviously goes the wrong way here, right? And >> a lot of people would like to see labs hill climb on the and target the open. This is the access to to hill climb. Yeah. >> Unfortunately, it might be fundamentally true that the the slum will always go this direction because once you open</p>\n<p>something up, then everyone else can get to the level of what you open up. >> Well, so let me let me tweak your point system, right? like you have these like numbers on the point system and it go up to 18, you know, but like just because I have a little bit of open data doesn't mean I'm necessarily that much better in someone who put a lot of effort into their open ways that is smarter. So I might I might just mess with the point system to make sure that like I'm accurately representing the the contribution to the open so openness. It is hard to wait for the materiality of the contribution to to open source like</p>\n<p>it's we tried to make it so that it is quite well defined and no one can disagree about like which um category things should be and so we're not saying like this was a big contribution or a small contribution in terms of um impact on the industry or anything. It's just like how much of your data did you release. I would say that it is still valid to say that we train a model that's not that smart, maybe even not at the frontier for a particular size category, but we chose to open up all the data, all the training code. That is a very useful exercise for the industry</p>\n<p>and we want to recognize that even if the smartest model in the category >> Yeah. Yeah. And also special shout out to Nvidia Neotron which doesn't get enough credit for the amount of stuff that they do and honestly it's a sales enablement for Nvidia as well. like the fact that they can do this as a side project. >> Totally. [laughter] But I mean, but it is true that Nvidia have actually put an enormous amount of effort over the last year especially into the neatron models and >> and so many people actually use it for like synthetic data and stuff like it's it's uh it's a pretty interesting secret of the industry that uh Nvidia holds up all these guys. [laughter]</p>\n<p>>> I mean it's in their interest for there to be more AI. Uh so obviously I think you want to push openness as as having an index every index that you push like has encodes some kind of opinion or value. >> Yes. >> I think one of the openest questions from this year was people messing with the the the license. And so Llama had this like if you have 700 million daily active users you're not allowed to use our model or you have to talk to us something like that. So basically like what are your customers telling you about the kind of licensing worries that</p>\n<p>they have right because obviously most people will never hit 700 million users. >> We have like a detailed breakdown of that in the openness index and that was actually one of the initial questions like took us down the route of wanting to do this. Um [laughter] cuz yeah the simplest thing that like our opinion is is that there is a lot of advantage to having like an official OSI license like MIT or Apache 2 because</p>\n<p><strong>[57:28]</strong></p>\n<p>then the box is just checked. You don't even need to read it because it's just Apache 2 and you can do whatever you want and it's fine. There are often very good reasons that companies don't want to release language models with those completely open licenses. The index tells you. So if you get the top category, it's one of those licenses, you're totally good. And then we've got um some lower categories for when attribution is required. Um and then when commercial use is not allowed. Yeah, they're there. >> So that's the open openness index. Thank you for doing all those all those works. Let's talk a little bit or at least end</p>\n<p>the pod on just the the trend reports that you guys do which is kind of a bit of the bread and butter how you make money. Highly encourage everyone to see George's talk at World's Fair which gives a little bit of a preview and you're you were very excited about talking about the smiling curve or I don't know what you call it. >> Yeah. Yeah. Yeah. Yeah. Yeah. Let's talk about that one. >> Let's explain it for people and and I might I might actually put put it up um because I don't have it >> got to copy the slide. That'll be that'll be excellent. >> It's important for people to have in their head because Yeah. People only get the marketing message from the labs that oh we're cutting cost all the time.</p>\n<p>>> Yeah. Yeah. But it's it's true. It's just that it's [laughter] not the whole picture. So okay, a couple of like the big trends that we track at artificial analysis over time and that like we're always showing charts of on the trends page and these reports and stuff. One that the cost of intelligence has been falling dramatically over the last couple of years. The best way to think about that is that the cost for each terror of intelligence has been dropping. The like one fact on that is that you can get intelligence at the level of GPD4 for over 100 times cheaper than GPD4 was at launch right now.</p>\n<p>>> I think my number is a thousand actually >> if you look at um the Amazon Nova models which are very very cheap. >> Yeah. like my my conservative uh statement is normally like but in fairness this slide like I we were actually saying before the podcast right is like maybe six months old now and it's conceptually still correct but like could actually probably do on the exact numbers because like the market's moving so quickly. >> Yeah, feel free to kick it off. I I mean we'll have to try I told people to watch the world's fair talk but let's let's introduce what context makes you make</p>\n<p>something like this. There are two trends that seem to not make sense together. Both of which we talk a lot about at artificial analysis and are very important to developers building stuff in AI. The first is that the cost of intelligence for each level of intelligence has been dropping dramatically over the last couple of years. We track the cost to run artificial analysis intelligence index for each bucket of intelligence index scores. and each bucket you just see the line go down really really quickly and</p>\n<p>actually go down more quickly for each new level of intelligence that's been achieved over the last couple of years. So the rate of that cost plan has actually been going up. So we've got that being true and yet it is clearly possible to spend quite a lot more on AI inference now than it was a couple of years ago. Nvidia stock go up. [laughter] >> It's going it's going really up. Uh I just heard from a friend's startup that</p>\n<p><strong>[60:30]</strong></p>\n<p>just went to the age of zero. They're spending $5,000 per employee on coding agents spend alone. >> That's ridiculous. >> That's an impressive number. We need to get our numbers up. We're uh we're not going to be hitting. >> I was like, it's so high that I'm like, are you doing something wrong? >> Yeah, [laughter] cuz there are some efficiency questions along the way, but like you can make AI inference useful to that level in a bunch of ways that I can imagine, right? Um I I don't think that's that nuts. Um but basically the reason we made this slide to answer the question right is to show that the crazy thing is that it is</p>\n<p>actually true that we've had this 100x to a,000x decline in the cost of GP4 level intelligence on the left hand side and yet on the right hand side because the multipliers are so big for the fact that even though small models can do GB4 level now we still want to use big models and probably bigger than ever models to um do frontier level intelligence. We've got reasoning models using tokens and then we're throwing them these them in these agentic workflows where they're consuming enormous numbers of input tokens and making enormous numbers of output tokens working for a really long time. Those</p>\n<p>two things taken together get you back to we can spend enormously more today than we could a couple years ago. >> Yep, I think that's right. There's a number of drivers at play and we kind of outline kind of six key ones here. Um but you know as complex it's changing quickly. All of these have changed very dramatically in the last uh in the last 12 months. Let's pick on hardware efficiency since you you also have you also track hardware stuff and I think the general assertion or the the message is that the efficiency from nextg Nvidia chips is actually not 4x or you have</p>\n<p>what 3x or 4x you have 3x in here and it's it's like 2x maybe or it's more of like a power story rather than like a a sheer sort of compute tokens efficiency story but yeah what's going on in in hardware >> okay so the the the unfortunately uh is it depends and it just depends massively on like so many things across a bunch of different types of workloads and ways to think about it. So one of the simplest ways to think about this is to take single relevant model to think about</p>\n<p>serving it at speeds that are realistic for what you actually might want to hit and can afford to hit and then think about the throughput per GPU that you can achieve serving the model at those speeds. One of the reasons that's important is that there's a trade-off between the throughput per GPU that you can achieve and the per user speed that you can achieve. And as in it costs more to serve stuff fast to users. When you run all of that for especially big sparse models, you can get a lot better than 2 or 3x gain going from Hopper to Blackwell generation Nvidia. I am, this</p>\n<p>shouldn't be too controversial a statement, but like I'm pretty confident that Blackwell has delivered pretty enormous gains and that the next couple of years of Nividia's road map are going to continue to deliver quite enormous gains and that those will actually come through as lower total cost per token to the companies that are running models on</p>\n<p><strong>[63:32]</strong></p>\n<p>them and will allow bigger models, will allow way more tokens to be made for lower cost and that that's going to continue these things also stack on all of the software and model improvements being made. So basically like my prediction across like both sides of that like smile chart uh that we're going to see the left hand side continue to be true and probably like for another order of magnitude and the right hand side continue to be true for another order of magnitude and that going to enable a whole lot of things. >> Okay. Well, I'll push on let's go back to the the small chart. I'll push back</p>\n<p>on sparity, right? Uh we've gone a long way on sparity. Deepseek was a major pusher of fine grain experts let's call it right I have a mental number of sparsity in terms of let's say active params versus total params and that number went from >> 25% let's say down to like 15 right you obviously can't really go below I don't know five >> so there's a lower limit to to sparity is what I'm saying >> I don't know that that's obvious actually >> all right</p>\n<p>>> um there must be a limit somewhere right >> yeah exactly But we've got numbers in the wild that are quite a lot lower than that right now. So the GBD OSS models like the big ones at about 5% um active. Kimmy K2 is at like 3% active. Oh, okay. >> I think pretty sure I I've looked at those numbers. I've calculated them. I don't remember. Yeah. But I I remember thinking like this must be it. your your 5% is is exactly is like around the ballpark for the for the open weights</p>\n<p>models of of what's released today. I think one interesting that gives me kind of pause when thinking that it won't go the sparity won't go higher or the number of percentage of active parameters lower is that we in our benchmark see a lot of performance to correlated more with total parameters than active and not that correlated with how sparse like the models are. our accuracy benchmark as part of AA omniscience. It's very correlated with total. It's not correlated with with active uh parameters which I think is</p>\n<p>very at all which is very very interesting and so I think yeah there could there could be quite a bit um to go here. >> Awesome. Well, we don't have that much time but I I did want to leave some room to cover reasoning and non-reasoning models and token efficiency. >> Let's do that. So at a at a super high level, people have to classify this binary thing of reasoning versus non-reasoning. >> People who are insider have some discomfort with that because basically you just have the think tag or no think tag. >> How have you guys decided to approach this? And also how does that laid out in over the course of the year where we</p>\n<p>have things like GT5 which is a model router? >> Let's say GVD5 and CH GVD the consumer experience is a model router. when you're hitting the API like we can you can pick the different versions and you can pick reasoning strength of the different versions but that that goes to why this is now such a complex thing. So earlier this year and probably when you and George last spoke for the AI engineers World's Fair, we had this</p>\n<p><strong>[66:33]</strong></p>\n<p>great slide that was super easy where we would show that the average reasoning model is using 10 times the number of tokens per query in our intelligence index as the average non-reasoning model. And there was this moment where that was a pretty clear distinction and extremely useful to look at it just like that. definitely no longer the case. Not least because you can think about reasoning strength for a bunch of these different models, but particularly because different models have wildly different token efficiency now, more than an order of magnitude in difference. That means that the way that you probably need to think about cost</p>\n<p>for any application is to use something like our cost to run intelligence index metric as the starting point for what it's going to look like for these different models, these different reasoning strengths and this continuous spectrum from non-reasoning to reasoning. That's basically like where we're at. We will still show reasoning and unresing and define reasoning as when there is that separated chain of thought that you're getting in a different parameter in an API normally. But it doesn't necessarily anymore mean that that model is actually going to have longer end to end latency that it's</p>\n<p>going to use more tokens than something that is branded on a non-reasoning model for the same task. >> That's true. I think 5.1 was it and then 5. Codeex had these this chart which is super nice of this like let's say bottom 10 percentile query being faster but top 10 percentile being longer and that's a kind of the efficiency chart you want to see right? >> Yep. That so so that is a an extra thing. Let's say [laughter] let's say let's say that we've got that's a really important extra thing though, right?</p>\n<p>That you've got not just the average number of tokens being used by the model which we cover really well right now. But the behavior that you want in the model is it to use more tokens when it needs more tokens and not to use more tokens when it doesn't need more tokens. So that's what OpenAI were basically claiming that 5.1 CEX is better at. We don't actually publish anything on this right now but have tracked it a bunch internally in our internal analytics on evals across all the models that we run where we look at the difficulty the questions and the correlation between token usage and difficulty and net net surprise surprise like models have got</p>\n<p>better at doing that over the course of this year I think going into next year that's going to be really important especially as you multiply it by the number of steps in an agentic workflow that a model has to take to get to an answer we are going to care a lot about token efficiency and number of turns efficiency for getting to what we want. >> Which would you rather have token efficiency or number of turns efficiency? >> Um >> or [snorts] like which is more important to work on? >> Like it depends on the application and both are going to be really important. >> Uh yeah, retail tobench airline.</p>\n<p>>> Yeah. Interestingly in TA um T2 bench telecom run you know on a per token basis more expensive models like a GBD5 compared to some smaller open source models because the um some of the GB5 for instance got to the answer faster and so it was able to resolve the</p>\n<p><strong>[69:34]</strong></p>\n<p>customer's query faster and fewer turns and maybe it used more tokens per turn but it certainly cost more per token so you would always rather use 5 in in in that scenario. And so I think that's what that's where we're getting to. I think number of turns is is going to be a metric that we're going to be talking about a lot more and uh I think it'll be something that people want to really start to think about uh a lot more. >> There's a trade-off in benchmarking here where most benchmarks need to be one turn to be autonomous, to be parallelized and all that, but most a lot of real life use cases need to be multi-turn and especially like quick</p>\n<p>multi-turns so you can align. Yeah. >> Yeah. I mean I I would say that historically benchmarks have been single turn but I wouldn't say they need to be at all into the future right like we have a couple of agentic benchmarks in the index right now and GDP val that we were talking about we let the models do up to 100 turns in um our stirrup agents to do that eval and we're going to build similar stuff like that in the future it definitely is hard and you've got whole kinds of infrastructure problems to run that and exactly as you say parallelize it because we need to run that on</p>\n<p>hundreds of models and we want to do that really fast when new models come out and when labs want us to run it on their models, but you can do it. We're putting in the work to build that stuff and it's going to be great. >> Okay, so we've covered I mean there's a lot more to cover and you haven't even touched on multimodal which is huge. >> We also do speech benchmarking, image benchmarking, uh video benchmarking hardware. >> I like the way that you've done it because very smart which is uh video takes a long time. >> So you pre-generate, right? So then people just pick their preferences and</p>\n<p>you can see the the overall arena results and you also avoid like any sensitivity issues around like unsafe content that that is being generated. >> Yeah. And you can see it as a good good thing or bad thing depending on what your view is. But it means that we have a quite active creative direction approach to trying to understand what creative professionals and users want to do with those image and video models and so that we can be directing the arenas and our categories toward gathering votes on what people care about. One</p>\n<p>call out actually to listeners like if you are using our arenas is that you can submit requests to us for things that we should cover. I didn't know that >> underststudied categories, areas that you think the models are bad at and the labs don't focus on enough. Like if you want something solved, one of the levers that you have is send us a couple of prompts on it. We might be able to get a category going on it. And this thing that we were talking about earlier, right, that once things get measured, they can get targeted. You can make that work for you. >> For me as a content creator,</p>\n<p>infographics >> very needed. I took the latest Deepseek paper and uh I you know they had some descriptions of their search agents and their coding agents and I put it in I created an infographic and um I I I just think like that's an industrial use case that doesn't require a lot of I guess design taste but just requires some like you need to conform to some preset references which is something that uh that is increasingly important especially in like the nano banana</p>\n<p><strong>[72:35]</strong></p>\n<p>series but um yeah I think like open is releasing image 2 soon which uh is going to have So I I think like it's it's all like of a kind where people need to incentivize like workhorse use cases and not just art. I don't know. >> Totally. Yeah. >> What are we going to be talking about next year? Like what's what's like what's emerging that you're seeing and like maybe not in the discussion. >> The first answer that I'll give to that as the the boring answer is that on most of our charts the lines go in a particular direction and our overall prediction is the lines are going to keep going that direction. We're going</p>\n<p>to do a lot and do a lot to be as useful as possible to developers and companies to measure what's important on every one of those and along those lines. But I think we're going to talk about similar stuff. It's just that we're going to have continued on this trajectory for another year and things are going to feel pretty different because of that happening. I know this is the boring answer to that question. >> No, no, I mean I'm a fan of things uh that truths that don't change because you can build and plan for that. And I think in media in general, in the podcast business, newsletter business, Twitter business, people are addicted to</p>\n<p>change. Like, oh, everything's breaking, everything's No, like there's some truths that are just constants that you can plan on and build. And yeah, >> I I think one of the truths is that the demand for AI intelligence and smarter AI intelligence is going to be insatiable. Um, some people disagree that okay, once we reach certain thresholds, then you don't need more intelligence. I think to that I ask people, have they ever worked with or managed someone in a work environment and wouldn't press the button that they</p>\n<p>were smarter to make them smarter or better at their job or or would they never press that for themselves? And I'm not sure that that's that's the case. But I think for artificial analysis, we'll keep benchmarking uh raw intelligence, but we also want to think about it and explore models more deeply across other axes as well. I think hallucination's the start of that, but we're getting into wanting to support people in understanding, okay, the behavior, the person personalities of the models to help people make more nuanced uh decisions.</p>\n<p>>> You're going to have a personality bench. >> Maybe [laughter] that is a direction that Chaji Open is leaning into a lot. Uh so if you manage to solve that, you should definitely talk to uh Fiji and run. Oh, okay. Um, yeah. So, what is going to be included in let's say like a V3 of the intelligence index because obviously you're going to saturate in March. >> Why don't we break it now? >> How soon does the podcast come out? >> Whenever you want. Okay. So, so we're at V3 right now. So, so the the the version that we that's going inside is is V3. V4 is what we're going to call the next,</p>\n<p>you know, major update. Surprise, surprise. We're going to be adding several of the things that we've actually talked about today that we've launched over the last few weeks. So it's not that's not going to be wildly shocking, but some of the things that are most exciting is that adding GDP [snorts] is going to give us this general agentic performance in a really strong way in intelligence index. Adding critical point the um physics eval was talking about similar to frontier math that gives us completely new view with a</p>\n<p><strong>[75:37]</strong></p>\n<p>brand new data set of very very hard research problems. We are going to be using omniscience and we are going to be using hallucination rate. The exact ways that all of those are going to come together. Um >> the waiting is going to be hard cuz the numbers are different. Yeah, we're going to make sure that we don't do anything to cause odd distortions and stuff that could be misleading. >> But every time you version it, you have a onetime reset of the >> Exactly. you know. >> Yeah. >> Yep. That's exactly how we think about it. We will make sure that within each version number that there's no um drift in any of the scores so that people can rely on them and reference them. Um you just have to watch out for that version</p>\n<p>number. Once it's v4.1, those numbers won't be compatible with v4. >> Of course, uh there there is a little bit of debate over the the accuracy of bench. I don't know if you you're clued in to what's going on. Apparently like a very high number of top chests are impossible. >> Potentially for the earlier versions T2 bench telecom we're pretty convinced is pretty good. If anything the only issue there is that models have got very good at doing it and so like anything >> top three. >> Yeah.</p>\n<p>>> Yeah. >> On we go. >> Yeah. On we go. Okay. Well, thank you so much for uh providing such a great service to the industry. I'm I'm glad to at least know you guys as before you got famous and now now you are famous. [laughter] So >> Oh, look our pleasure and we really appreciate your support along the way. Like I I wasn't kidding at the start, right? That it was a um quite material moment for us like when artificial analysis was covered on later in space. >> Some random guy in San Francisco mentions you and >> No. So I I I was a fan of late in space for like a year before you mentioned us. So I'd been I'd been I'd been listening. Um I don't think I was um like familiar with um like you personally yet at that</p>\n<p>point but like I was I listened to your voice probably for many many [laughter] hours and so once like you mentioned and then like got to get to know you and like meet you for the first time nearly a couple years ago like it was really cool honestly. So yeah it's great to be here >> and thanks for you know being such a great member of the community and kind of spotlighting you know projects which don't don't have attention and and bringing them to your to your audience. >> Yeah. Uh well actually so it wasn't me right uh someone in the discord dropped it in our in our discord and that's I I rely on our community and it kind of feeds itself right so uh so someone</p>\n<p>brought it to my attention I don't know who we should probably go back and check but once I saw it I was like this is this looks good this is something I always wanted I I I wanted to build it I I was too shy or dumb or lazy to build it and you guys did and um not now it's a whole thing so thank you >> built some really cool other stuff like like this pod [laughter] >> yeah totally >> so thank you >> that's Great. Cool. Thank you. [music]</p>"}