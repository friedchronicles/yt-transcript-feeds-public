{"video_id": "Z0x99Uu4rJc", "title": "AGI Might Not Need Continual Learning \u2013 Dario Amodei", "link": "https://www.youtube.com/watch?v=Z0x99Uu4rJc", "published": "2026-02-13T20:45:02+00:00", "summary": "Full episode: https://youtu.be/n1E9IZfvGMA\nMe on twitter: https://x.com/dwarkesh_sp\n\nDario Amodei thinks we are just a few years away from \u201ca country of geniuses in a data center\u201d. In this episode, we discuss what to make of the scaling hypothesis in the current RL regime, how AI will diffuse throughout the economy, whether Anthropic is underinvesting in compute given their timelines, how frontier labs will ever make money, whether regulation will destroy the boons of this technology, US-China competition, and much more.", "transcript_html": "<p><strong>[00:00]</strong></p>\n<p>A million tokens is a lot. That's that's you know that can be days of human learning, right? You know, if you think about the model, you know, you know, kind of read reading reading a million words, you know, you know, it takes me how long would it take me to read a million I mean, you know, like days or weeks at least. Um uh so you have these two things and and I think these two these two things within the existing paradigm may just be enough to get you the country of geniuses in the data center. I don't know for sure, but I think they're going to get you a large fraction of it. There may be gaps, but I I certainly think just as things are,</p>\n<p>this I believe is enough to generate trillions of dollars of revenue. That's one that's all one. Two is this idea of continual learning. This idea of a single model learning on the job. Um I think we're working on that too. And I think there's a good chance that in the next year or two we also make we also solve that. Um I I again I I I I you know I think you get most of the way there without it. I think the trillions of dollars of of you know the the I</p>\n<p>think the trillions of dollars a year market maybe all the national security implications and the safety implications that I wrote about in adolescence if technology can happen without it. But I I I also think we and I imagine others are working on it. And I think there's a good chance that that you know that we get there within the next year or two. There are a bunch of ideas. I won't go into all of them in detail but um you know one is just make the context longer. There's there's nothing preventing longer context from working. You just have to train at longer context</p>\n<p>and then learn to to serve them at inference. And both of those are engineering problems that we are working on and that I would assume others are working on as well. >> Yeah. So this context lines increase, it seemed like there was a period from 2020 to 2023 where from GBD3 to GBD4 Turbo, there was an increase from like 2,000 context lines to 128K. I feel like for the next for the twoish years since then, we've been in the sameish ballpark. Yeah. >> And when model context lines get much longer than that, people report qualitative degradation in the ability of the model to consider that full</p>\n<p>context. Um, so I'm curious what you're internally seeing that makes you think like, oh, 10 million contexts, 100 million contexts to get human like six months learning billion billion context. >> This isn't a research problem. This is a this is an engineering and inference problem, right? If you want to serve long contexts, you have to like store your entire KV cache. You have to, you know, um, uh, you know, it's it's it's it's difficult to store all the memory in the GPUs to juggle the memory around. I don't even know the detail, you know, at this point. this is at a level of detail that that that that I'm no longer</p>\n<p>able to follow although you know I I knew it in the GPD3 era of like you know these are the weights these are the activations you have to store um uh but you know >> you know these days the whole thing is flipped because we have models and and and kind of all of that but um uh and and this degradation you're talking about like again without getting too specific like a question I would ask is like there's two things there's the</p>\n<p><strong>[03:01]</strong></p>\n<p>context length you train at and there's a context length that you serve at if If you train at a small context length and then try to serve at a long contract length like maybe you get these degradations. >> It's better than nothing. You might still offer it but you get these degradations and maybe it's harder to train at a long context length. Yeah. So, you know, there's there's a lot. If you enjoyed this clip, you can watch the full episode here and subscribe for more clips. Thanks.</p>"}