{"video_id": "gJ8pa9NiWm4", "title": "Was 2025 A Good Or Bad Year for AI? (Cal Newport and Ed Zitron Break it Down) | Cal Newport", "link": "https://www.youtube.com/watch?v=gJ8pa9NiWm4", "published": "2026-01-05T11:01:26+00:00", "summary": "Cal Newport talks with Ed Zitron to judge if 2025 was a good or bad year for AI in episode 386 of the Deep Questions podcast. \n\nBuy Cal Newport's latest book, \u201cSlow Productivity\u201d at www.calnewport.com/slow \n\n2025 was a year that was saturated in AI news, from Deep Seek, through claims of economic \u201cblood baths,\u201d to to GPT-5, Sora, and Chatbot girlfriends. Frankly, it was exhausting. As we now look back on 2025 an interesting question arises: all in all, did this end up being a good or bad year for AI? To help me answer this question, I\u2019m joined by hard-hitting AI commentator Ed Zitron, who's been everywhere in the media in recent months helping to make sense of the wild claims being thrown in the general public\u2019s direction. Together we go through the biggest AI stories of the year to try to make sense of what just happened. \n\nDownload my FREE Deep Life Guide HERE: https://bit.ly/3QBIcug\n\nListen to Episode Here:  https://www.thedeeplife.com/listen/\n\nGet your questions answered by Cal! Here\u2019s the link: https://bit.ly/3U3sTvo\n\nLinks:\nGet a signed copy of Cal\u2019s \u201cSlow Productivity\u201d at https://peoplesbooktakoma.com/event/cal-newport/ \nCal\u2019s monthly book directory: bramses.notion.site/059db2641def4a88988b4d2cee4657ba?\nhttps://www.bbc.com/news/articles/c5yv5976z9po\nhttp://www.axios.com/2025/01/23/davos-2025-ai-agents\nhttps://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/\nhttps://openai.com/index/sora/\nhttps://openai.com/index/introducing-gpt-4-5/\nhttps://ai-2027.com/\nhttps://fortune.com/2025/05/28/anthropic-ceo-warning-ai-job-loss/\nhttps://www.media.mit.edu/publications/your-brain-on-chatgpt/\nhttps://www.usatoday.com/story/tech/2025/08/07/chat-gpt-5-release-date-open-ai/85566627007/#:~:text=GPT%2D5%20release%20date,release%20date%20for%20Part%202\nhttps://www.newyorker.com/culture/open-questions/what-if-ai-doesnt-get-much-better-than-this\nhttps://www.wsj.com/tech/ai/ai-bubble-building-spree-55ee6128\nhttps://nvidianews.nvidia.com/news/openai-and-nvidia-announce-strategic-partnership-to-deploy-10gw-of-nvidia-systems\nhttps://www.nytimes.com/2025/10/02/technology/openai-sora-video-app.html\nhttps://www.anthropic.com/news/claude-opus-4-5\nhttps://www.ft.com/content/064bbca0-1cb2-45ab-85f4-25fdfc318d89 \nhttps://www.youtube.com/watch?v=Z_WEmjygNK0\n\nThanks to our Sponsors: \n\nThis episode is sponsored by Better Help:\nhttps://www.betterhelp.com/deepquestions\nhttps://www.reclaim.ai/cal\nhttps://www.expressvpn.com/deep\nhttps://www.calderalab.com/deep\n\n\n0:00 Cal talks about AI\n3:03 Interview with Ed Zitron\n1:58:05 Cal Reacts to Comments\n\nConnect with Cal Newport:\n\n\ud83d\udd34Visit Cal's BLOG and website:             https://calnewport.com/blog/\n\ud83d\udd34Check out Cal's books:                         https://calnewport.com/writing/\n\ud83d\udd34Check out The Deep Life:                     https://thedeeplife.com\n\nAbout Cal Newport:\nCal Newport is a computer science professor at Georgetown University. In addition to his academic research, he writes about the intersection of digital technology and culture. Cal's particularly interested in our struggle to deploy these tools in ways that support instead of subvert the things we care about in both our personal and professional lives.\n\nCal is a New York Times bestselling author of seven books, including, most recently, A World Without Email, Digital Minimalism, and Deep Work. He's also the creator of The Time-Block Planner.\n\nThe videos are considered to be used under the \"Fair Use Doctrine\" of United States Copyright Law, Title 17 U.S. Code Sections 107-118. Videos are used for editorial and educational purposes only and I do not claim ownership of any original video content. I don't use said video clips in advertisements, marketing or for direct financial gain. All video content in each clip is considered owned by the individual broadcast companies.\n\n#CalNewport #DeepWork #DeepLife #DeepQuestions #TimeblockPlanner\n#WorldWithoutEmail #DeepQuestionsPodcast", "transcript_html": "<p><strong>[00:00]</strong></p>\n<p>So much happened in the world of AI in 2025 that it can actually be hard to keep track of it all. I mean, remember Deepseek? That was in 2025. as was Dario Amade saying that we were going to lose half of white collar jobs to AI as well as GPT5's release the release of Sora AI looking like the best investment ever followed by AI being described as a giant bubble that was going to bring down the economy followed by that bubble being described as actually not being so bad. This was also the year where Nvidia CEO Jensen Wong</p>\n<p>took the stage in a conference wearing a jacket that well, I'll be honest. Uh, looks like it came from the prop department from a Mad Max movie. Jesse, let's put this on the screen here. I mean, dude, you're a computer scientist. You're in a racer jacket. I love it. I'm here for it. What I'm trying to say is a lot happened in the world of AI in the year that just ended. And the key question that I've been grappling with is did this year end up being a great year for AI or a terrible one? I would</p>\n<p>believe either answer and so much happened. It could be really hard to try to keep it all straight. So here's what we're going to do today. We're going to try to get an answer to that query. To help me in these efforts, I've invited to join me Ed Zitron. I I think one of the the big missed stories of AI in 2025 is Zitron himself who hosts the better offline podcast and writes the where's your eds at Substack. Uh he rose to become I think one of the more uh informed and important AI commentators out there. The secret to Ed success is</p>\n<p>pretty simple. He just does his homework. Like he actually talks to sources. He talks to reporters. He reads earning reports. He gets leaked information. He talks to people within these companies. He puts together the pieces. old-fashioned shoe leather reporting on what's actually happening with these businesses as opposed to reporting on the stories these businesses are telling about what their technology may or may not do. My honest opinion or at least my humble opinion, I think he's probably the most important AI commentator that you haven't yet heard about. And so Ed is going to join me and what we've done is we've pulled</p>\n<p>the biggest AI stories of 2025, one per month for the entire year. We're going to go through them in order and Ed is going to help us make sense of what was going on behind the scenes and what these stories actually mean for the AI industry writ large. We'll end up with a conclusion of just how good or bad this year actually was for AI technology. So by the time that we are done with this episode, you will be more or less fully up to speed with where we are at this moment in the world of AI and what is likely to happen in the near future. All</p>\n<p>right, so let's get into this episode. As always, I'm Cal Newport and this is Deep Questions. Today's episode, was 2025 a great year or a terrible year for AI? And we'll get right into this after the music. [music]</p>\n<p><strong>[03:04]</strong></p>\n<p>All right, so Ed, we got a lot to figure out. Um, I got to point out something first, though. Okay. >> I This is something I don't normally do, but for those who are watching, I put on a jacket. >> Nice. >> To try [music] to uh compensate for your English accent. >> A jacket for the British. >> I think it's going to make me look a little bit more uh scholarly and arudite. That's my That That was my strategy. >> I'm I'm wearing a sweater that I've worn once and I'm like, I guess I'm warm, but I look weird. But it's fine. That's That's my bit. >> Yeah. But you sound You know, but it sounds</p>\n<p>>> I sound British and I can't hide that. >> Yes. And so that that gives you an advantage on me. But I think my my blazer will kind of balance it out. But yeah, the fact >> I think we should be good. >> You're wearing a you're wearing a sweater in it in Las Vegas though. So that is that should take. >> It gets cold here. It gets cold here sometimes. >> I don't believe I went once in July. I'll never believe anything. >> Yeah. Okay. That I can understand that. >> All right. So we're going to try to figure out what the hell happened in 2025. Right. It's you and I both were covering AI in that year. It felt like all the things happened. >> Yeah. There was no quiet period in that</p>\n<p>year from the AI front. And so what I wanted to do is uh go through month by month and hit some of the big >> headlines >> and you and I will try to figure out what was that. Was that good news or bad news for AI? What actually happened? So it's going to be like a trip down a sort of frustrating memory lane. All right, let's start in January. I actually forgot that this was in 2025. I thought it was earlier. Man, it was a long year. All right, in January we get uh Deep Seek. Deep Seek. >> Here is Deepseek, the Chinese AI app</p>\n<p>that has the world talking. Let me read the first sentence of a BBC article from that period. Deepseek, a Chinese artificial intelligence startup, made headlines worldwide after it topped ad download charts and caused US tech stocks to sink. In January, it released its latest model, Deepseek R1, which uh it said rival technology developed by chat GPT maker Open AIs and its capabilities while costing far less to create. This was like a huge deal that no one talks about anymore. Uh, explain to my listeners what the hell is</p>\n<p>Deepseek? >> So, Deep Seek was a really interesting one. I remember I was on a plane. I was I was just it was I just got started to move back to New York and such like I spend a lot of time there and I remember reading about this thing and what it was was that it was a model that was trained for less money than other American models. So, American models that cost like $50, $100 million or more to train. Deep Seek apparently cost $5.3 million, I think, to train. It's really weird</p>\n<p>because it spooked the entire market. Like, everyone freaked out. And I remember >> I remember thinking this is an obtuse story to freak people out. >> Like, it was just like even trying to explain cuz I did like a a lot of media at the time. I was explaining it to people. I was shocked that people even had any interest in model training. But the big thing that spooked people was it was kind of the thing that shown a spotlight on the Nvidia problem, which is that Nvidia is like the only company</p>\n<p><strong>[06:04]</strong></p>\n<p>really making money in this era or and I think that people started to realize, oh crap, our entire stock market is based on that. And it also made it clear that all the American model companies don't really give a crap about any kind of efficiency or anything. And the reaction to it was great. Sam Orman suggested we ban it. That was my favorite bit. They were like, \"Ah, yeah, the sneaky Chinese are gonna It's because they might be able to um see inside things. We can't possibly trust them.\" What was really</p>\n<p>good as well was part of the I literally was just reading about this yesterday. Part of what was funny about it was part of OpenAI's complaint was, \"Yeah, they might do IP theft.\" It's like, \"No, we only let American large language models do that. We couldn't possibly have the Chinese take away our our plagiarism machines. No. And what was great >> world leaders and plagiarism. Yeah, >> exactly. We can't have the Chinese steal our things. That's our job. But what was also interesting was they were like, should we sue them? Because there's a process called distillation where you</p>\n<p>basically take another models outputs and you use them to train another model. That's a very truncated version. And it was you they used chat GPT outputs to train DeepSeek and that made people pissy. The other thing was was it was a reasoning model the R1 model and OpenAI had at the time only been out for a few months with its reasoning model 01 I think it was. >> Yeah, that was a December 2024 release. >> I think it was September >> or September. Okay. >> Yeah, it was September because it was the runup to that was this whole thing.</p>\n<p>People like, \"Oh, it's called Qstar. It's called Strawberry. It's going to change everything.\" It didn't change anything. It really it actually it did change something. Reasoning models gave them more excuses to burn AI compute. But yeah, this whole thing it was great for me. I did a bunch of media hits about it. But it was peculiar because it was like quite a nuance story and then you saw all of this xenophobic stuff being like, \"Oh, oh well the the Chinese are they're lying. They're lying.\" They put out a paper about this. They showed people how it was done because they</p>\n<p>trained. They had to find a cheaper way to train because they only had they had I think I forget maybe they had 8 800 chips. They had quite old GPUs. And the the thing is it wouldn't talk about TNM and Square and people were like oh look this is proof that it's bad. It's like yeah that it is bad. It does that but are you shocked that something that came out of China had censorship? >> Yeah. But it was Yeah. Go on. >> What's what's amazing about the story though is it went away. Yes. Like I mean all the points you're talking about are fair points and and like the biggest destabilizing point for the industry was</p>\n<p>this idea of you don't need the very largest data centers. You don't need like the custom AC Microsoft 40,000 GPU data centers to build really useful language model based AI. >> But those big companies are dependent on the idea that only they can do it. And so it was almost like people didn't >> want that to be true. But we just forgot</p>\n<p><strong>[09:05]</strong></p>\n<p>about >> Yeah, we memoryhold this bad because at the time it was like surely because I got asked quite a lot like OpenAI surely they're going to make a cheaper model now. >> They didn't. >> Anthropic, right? Because they if they if they said that that was possible, if their idea was we're now going to spend 5 million versus 100 million on this, they would then have very little justification for raising so much money. >> But what about like the nano models? But then it didn't then open AI thing is >> they have some cheaper to use models >> cheaper to use and that's the thing</p>\n<p>people love to use this as proof that the cost of inference is coming down. So inference being how an output is done >> and they're like well the models are cheaper. It's like yeah if you sell something cheaper it's now cheaper for someone to buy. There's no proof that that's actually cheaper to run and indeed it would have been so easy for them to just say actually this is a cheaper model. It costs this much. The fact they didn't means that it's still unprofitable, which is crazy. But the thing is, even DeepSeek's models aren't profit. Like, no one proved that they're</p>\n<p>profitable to run. And so, but I think everyone memoryholed it because >> they I don't know. I think the media just was willing to >> that there was this just a narrative that they could just >> get rid of. the the Chinese angle must have made a difference because yeah to me an even bigger story which I didn't really know till talking to a source at the end of um 2025 the biggest story that's not being talked about is if you look at coding agents and you look at cursor in particular to me the big economic story was the fact that cursor at some point quietly just said we're</p>\n<p>going to train our own model we don't need a frontier we don't need a frontier model we'll start with open source weights and train them themselves now whether or not that model is profitable for them I think They've been working has been working on their own models forever. >> Yeah. >> Ever and ever. Maybe the reason they they raised $2.3 billion. So maybe the plan is for them to train their own one. >> But it's they also there was a story back in September that Tom Dan over at Newcomer. Apparently someone said that um Curser is sending 100% of their</p>\n<p>revenue to Anthropic. >> So they're still they're one of Anthropic's largest customers. So it's >> I see >> it's really interesting though that they're trying and they've gone very hard on composer and things like that. So maybe they are trying that for real now. Maybe I mean they're capitalized but the question is to what end is it more profitable because if it ends up just being unprofitable and they don't pay anthropic that would also be very funny. >> No I mean I think the future has to be and we we again that's more later if we need to. I think the future is really going to be small models, models that</p>\n<p>fit on on machine, right? The only thing that's profitable is I'm not spending any dollars to run inference because this two billion parameter model, which is only really trained to do the very narrow thing I needed to do, which is like understand your spreadsheet program and help you do it. Can run on your phone. It can run on your chip. You're paying for the electricity. Like that's got to be the only way that this is profitable. But those companies cannot be large companies. Now you have a you</p>\n<p><strong>[12:06]</strong></p>\n<p>have 10,000 smaller companies instead of open AAI as the new Microsoft. >> Yeah. I I also think the small language model stuff because small language model is just a large language model with less parameters and while it is possible to do edge I just wonder how useful those edge like you can run on device but how long does it take to run a device? Uh there what Nvidia put out something called DGX Spark Box thing that can run large language models. The question is at the end of all this is is it worth it? Like is spending maybe it is worth</p>\n<p>three grand, 12 grand, whatever for one of these machines, but the companies building these models are not optimizing for that. They're not building models with that in mind. Nvidia has built that box so they have something else to sell. It's been in the works for a while, but it's like we don't no one I I can't find anyone who has run client side who has used it like in that manner who's like, \"Oh, I do all my coding, but I do an ondevice one.\" I'm sure they exist, but the fact there isn't a growing community</p>\n<p>of that suggests that that might not be viable either, but I think any future large language models will have to be on device. It's just the question is does that happen at any kind of scale? >> Yeah. All right. So other thing in January was agents. Uh I tracked this down recently. This is when >> the chief product officer of OpenAI uh said the quote that then got translated by Axios into 2025 is the year of AI agents. Axios does this by the way. I don't know if you've seen this as a reporter. It's really a pain.</p>\n<p>>> They invent a quote. >> They paraphrase what someone said into a better form and then we'll say like this headline is 2025 is the year of agents. Open AI CPO says you would assume that means that the Open AI CPO said 2025 is the year of AI agents. He did not. Now he talked about 20 he said things that was less quotable. They did the same thing with the blood bath and Dario Amade. He never actually said it was going to be a blood bath, but they had a headline that said it this year is going to be a blood bath. Dario Amade says. So anyways, I watched the</p>\n<p>>> no Axio had a quote as well where it was like this is proof that AI is taking jobs and you read the study and it's one line saying yeah we kind of see some effect. It's it's very frustrating because it's it helps. It's just marketing. It's not helping. Sorry I'm just going to rant. >> Yeah, I know. So it does it does. [laughter] I actually told I was I was talking to I was talking to uh speaking of a skeptics I was talking to to the Gary Marcus not long ago and he happened to be on his way to do do something at Axios and I was like you got to tell</p>\n<p>them to stop doing the headlines because I keep getting dinged by fact checkers afterwards like we cannot find evidence of this. All right but anyways um early in 2025 this is when we got >> agent excitement and I think this kicked it off and then at around the same time is when Sam Alman wrote a blog post that said they're going to join Yeah. reflections which was probably going to agents will probably join the workforce uh this year and material impact their</p>\n<p><strong>[15:08]</strong></p>\n<p>output. So why why would what are a why do they start talking about AI agents sort of out of nowhere in early 2025? Well, because they needed something to keep selling this crap >> and they launched operator within January, I think maybe that was February, and it didn't work. And >> this is a failure across the board with the the media. They all went, \"Yeah, operator, it can take actions in a >> No, it can't. Wait, okay, it can take actions in the same way that if I just throw a brick, it will that is me, I don't know, playing a game. If you</p>\n<p>consider like you can take abstractions from abstractions all you want but yeah they it's this ide it's just marketing it was marketing and mythology that agents because agents much like the term AI is a marketing term and that's an Emily Bender Alex Hannannah quote there is this thing of agents conjure up this image in your head of like oh an agent that goes out and does something for you. Now, agents going back to 2023 literally just meant chatbot. Like that was what it originally meant. But agents</p>\n<p>within this era were meant to meant to be digital labor. And I take that from Mark Benny off and Salesforce and agent force where they're like, \"Oh, it's digital labor.\" Sam Orman's comment was, and he always says may or probably, but it's like agents may join the workforce. Egregious lie. >> Egregious lie. There was no proof at that time that it was even possible to do it. And guess what? Where we are today, it's not possible either. >> We do have a kod because at the at the end of 2025, we'll get to a news story</p>\n<p>where, spoiler alert, open AAI takes all their resources away from agents because it wasn't working, but they were they were they were excited about it. I found the bin off some binoff quotes. By the way, >> speaking of egregious, uh he didn't say probably join the workforce like Sam did. He said it is going to not only are they going to revolutionize the workforce, they're going to create two to five trillion dollars in economic activity. 2 to 5 trillion dollars from agents.</p>\n<p>>> Wow. >> So, yeah. So, you know, uh almost there. All right. So, agents become a big thing. What were they to me? I always think like what is the there's a different flavor the AI companies are typically pushing like they have to have a a flavor of excitement because of the investment train. Where were we? I don't really remember like coming out of 2024 that was more what like a AGI super intelligence like there was there was they were pushing a different message back then because it I remember thinking this shift towards agents and therefore a shift towards we will just be in the workplace helping your bottom line that</p>\n<p>felt like at the time a shift towards a more pragmatic >> vision. I don't know if I agree. I think at that time that was when you started hearing people say coding agents. >> Yeah. >> And coding agents was their favorite one. And I'm sure you have some stories coming up where coding agents, oh, you can get them and later in the year Anthropic spread some around this as well where it was they go out and do things autonomously for you. That was</p>\n<p><strong>[18:09]</strong></p>\n<p>the whole thing that was being pushed. I know because I read every agent story cuz I found it so repulsive. The idea was that they clearly in the last year in 2024 they kind of squeezed all they could have out of chat bots. like no one was like finding new they couldn't do more things. They weren't really sure what to do. So they went agents are coming and what will they do? What do you want them to do? Because they might might do that. They can't but they might. What if they did? Wouldn't that be good? Please pay me. >> Well, wait. But so I I think by the time</p>\n<p>this interview airs, I have a a piece out on agents. I was and I talked to I couldn't talk to anyone in the industry but I talked to someone industry adjacent someone who made the uh the the main benchmark you use to evaluate coding agent and like so here's the story >> SWE bench >> uh terminal bench >> ah >> yeah so I I so I was I was going deep on what are these agents and here's what I learned okay so there's it's there's two ways AI is helping coders and they get mixed up so there's sort of the tab</p>\n<p>complete way which is goes back early codeex 202 21 even pre- chat GPT which is uh it's all based on oneshot queries to a language model. So that is like I'm I'm writing some code right now and I wanted to like finish I'm trying to like write a function to do something just like finish this thing immediately that I'm writing right >> and that is powered by like you make one query to a language model. So behind the scenes it's here's the code that we've written so far. How do you think I should finish what I'm writing? So like that's right in the sweet spot of LLMs where you're trying to complete or</p>\n<p>autocomplete >> because that's basically how they work. >> That's basing the next token >> and programming languages are highly structured. So they're very predictable. So like that's but that's been around pre-hat GPT but those work pretty well. Those are now integrated into most development environments is and you my students all call it tab complete. You just oh I don't want to >> that's cursor calls it that as well. I don't know if you'd say they work really well. Carl Brown from the Internet of Bugs describes it as makes the easy things easier and the hard things harder. >> Well, that's fair enough. It makes it it makes intro computer science problems.</p>\n<p>>> Yeah. Which is it still has utility which like that's something >> the main like when I talk to real programmers like the main thing they they say is useful is they don't have to look up interfaces for function and libraries. Yeah. >> So it's you're like okay I could just tab complete a function call here. Uh oh this is all the parameters. Okay. I wouldn't I would otherwise I would have had to Google uh stack overload and whatever. >> But the thing is with that though is that's useful but if it's quering libraries could it not get that wrong? I mean you have still have to check its work but maybe that is quicker.</p>\n<p>>> Yeah it is. Yeah. Yeah. So caveat mour then you had agents emerge right which um can do vibe coding right. So so like agents was more like uh I want you to create a prototype of a dashboard. I want you to add this to a personal website. And it does um multiple steps. So it's making multiple queries to an LLM. Uh so it'll it'll ask the LLM for</p>\n<p><strong>[21:09]</strong></p>\n<p>like it'll explain what's going on. Here's the tools available. What's the plan? And then it'll go step by step. Okay, here's the output of that last step. What do you want me to do next? So it's a program that's executing that's what makes it an agent is that it's executing multiple steps each of which is based on its own LLM query. What I learned from the the people in the field is in one sense this worked really well in that it can vibe code. If you say like I want you to produce like a prototype of whatever it could actually get through multiple steps and produce a prototype of whatever. And it turns out it was like pretty good at this because</p>\n<p>all of the stuff you have to do to create a computer program you can do as textbased commands in a terminal. So so the the tools that the LLM had to work with were uh textbased commands. All of these coding agents operate in a textbased terminal environment. So on the one hand, like it could do this vibe coding stuff pretty well in the sense of like it could actually produce a program that more or less cow. >> But wait, I'm going to let you push back in a second cuz I'm going to push back on myself first. >> But on the other hand, these things that were being vibe coded have no economic utility. So like in the abstract right</p>\n<p>if you're like can we have a multi-step multi-step execution call an LM much bunch of times and do a bunch of stuff on its behalf and on the other end end up with uh an updated website or a web-based dashboard that like more or less does what you say that it can do that but the economic utility of that is very limited do that >> can it do that because that's the thing vibe vibe coding I believe is one of the greatest frauds of all time >> because if you go on like replet or lovable or what have you subreddits, you</p>\n<p>can see people struggling with basic things. While it may get one thing right once, it's not going to get it done every time. And on top of that, vibe coding does like if vibe coding is I am a non-technical person building software. It is a lie. It's a fraudulent thing because you cannot just vibe code without knowing stuff. You have to know how to read code because something will break. You will not make stable or reliable or secure software. on top of that the multi >> but I have seen it it does create if you</p>\n<p>need um there's like a real use case right for someone I know if it really is just like I want a web- based interface for the silent auction for my kids school >> and I don't know how to program like it can do that that that it can do >> can maybe because large language model component it's like will this work this time >> oh yeah it might take you but you can you can work with it and finally get that thing to work without really having to know, but you do kind of still have to learn a lot of this stuff. Fair enough. Because the people I've talked</p>\n<p>to doing this without coding background, they now know a lot about setting up accounts on these different like AWS clouds and different coding environments. And you do have to learn. >> Yeah. >> Which kind of at that point that's not an agent at that point. That's just that's just generative code that may or may not work. And then you need to do all the infrastructural things. It gets back to the thing of what is a software engineer? And it's like a software engineer is not just writing code. I</p>\n<p><strong>[24:11]</strong></p>\n<p>just think that even now, and I'm not saying this is a criticism necessarily, even then the marketing is so powerful that even we fell into that trope of like, well, it can do this. >> Can it can it though? Can it do it every time? How much how replicable is this process? How realistic is this? I think you're right though. It's like it has some use and I've heard people say this as well with like MVPs like you need to fudge something together for an investor and you need to do it quick and dirty. It doesn't have to be perfect but it has to kind of</p>\n<p>>> resemble the form and function. I've heard people do that and it's great. >> Yeah, that's what I've heard and I've heard dashboards. People like dashboards. So we want internally um so yeah. Okay. All right. We're going to take a quick break here to hear from some of the sponsors that make this show possible. I want to talk about our friends at ExpressVPN. So, I just heard something mind-blowing. Netflix has more than 18,000 titles globally, but only 7,000 of those titles are available in the US, you're missing out on literally thousands of great shows. Unless you're</p>\n<p>using ExpressVPN. You see, in addition to the worldclass protection they give for your internet activities, ExpressVPN lets you change the location from which the internet thinks you are uh coming from, which means you can change where Netflix thinks you're actually located in the world. ExpressVPN has servers in over 105 countries or in exactly 105 countries rather and all 50 United States. So you can gain access to thousands of new shows and never run out of stuff to watch. So, for example, when I was in London recently and I was using</p>\n<p>Netflix in the hotel, there's shows there that we don't get here in the US like Top Boy or Park. If I wanted to watch those shows right now, it would be super easy. I would just have to open the ExpressVPN app, uh, select United Kingdom, refresh Netflix, boom, now I'm seeing the United Kingdom shows. Now, if you're going to use VPN, use ExpressVPN. and it's the one I recommend because it's easy, it works on all devices, and it's rated number one by top tech reviewers like CNET and The Verge. So, be smart and stop paying full price for streaming services and only getting</p>\n<p>access to a fraction of their content. Get your money's worth at expressvpn.com/deep. Don't forget to use my link at expressvpn.com/deep to get up to four extra months of ExpressVPN. This episode is also sponsored by Better Help. This new year, you don't need a new you. You just need to feel lighter. Think about it. Every January, we're told to add more. More goals, more hustle, more change. But what if feeling better in 2026 isn't about adding? What</p>\n<p>if it's about letting go? Letting go of what's been heavy or stressful or holding you back? Therapy with Better Help can help you see what's been weighing you down. With a licensed therapist, you'll gain clarity, perspective, and emotional space for the possibilities ahead. You don't have to reinvent yourself to move forward. You just have to make room for the lighter, truer version of who you already are.</p>\n<p><strong>[27:12]</strong></p>\n<p>And if you're considering therapy, consider BetterHelp. With over 30,000 therapists, BetterHelp is one of the world's largest online therapy platforms, having served over 5 million people globally. and it works with an average rating of 4.9 out of five stars for a live session based on 1.7 million client reviews. Make this year the year of you letting go of what's heavy with BetterHelp. You can't step into a lighter version of yourself without leaving behind what's been weighing you down. And therapy can help you clear space. Sign up and get 10% off at</p>\n<p>betterhelp.com/deepquests. That's betterhp.com deepquests. All right, let's get back to my conversation with Ed. We're in January, man. Okay, we got to roll. All right, February, I know. I know. So, it's not going well for AI so far. I mean, well, let's get to February. February, I think, was the quietest month of 2025. >> Um, there were two models released that I don't remember at all, and I want to see if you remember these at all, too.</p>\n<p>The two models that were released in February 2025, Gemini 2.0 Oh, and this one I really forgot. OpenAI's GPT45, which I ended up learning about later. This was them f tell me if I have this right. This was going back to the scaling, you know, the scaling article I wrote. I talked to you for that. Um, this was the result of the project they started right after GPT4 where they said, \"We're going to make the model 10 times larger. We're going to make our data center 10 times larger, and the result is going to be how 9,000.\" And it wasn't. And it was the like, oh crap</p>\n<p>moment where they were like, oh, >> and this is what they eventually released out of that. I think was four or five. >> Yes. And I absolutely knew that this was coming. So I have the tweet up and I want to read just a little bit of this because it really tells a beautiful story. >> Wait, this is your What are you reading? Your from February Sam from February. All right. >> Okay. GPT45 is ready. Good news. It's the first model that feels like talking to a thoughtful person to me. I've had several moments where I've sat back in my chair and been astonished at getting actually good advice from an AI. Bad</p>\n<p>news. It's a giant expensive model. We really wanted to launch it to plus and pro at the same time, but we've been growing a lot and are out of GPUs. We will be adding tens of thousands of GPUs next week and roll it out to the plus tier. Then hundreds of thousands of GPUs coming soon. I'm pretty sure y'all will use every one we can rack up. This isn't how we want to operate, but it's hard to perfect perfectly perfectly even predict growth surges that lead to GPU shortages. A heads up, this isn't a reasoning model and won't crush benchmarks. It's a different kind of</p>\n<p>intelligence and there's a magic to it I haven't felt before. Really excited for people to try it. Yeah, this was when you're right. This is when people started going, hm, I I don't know about clammy Sam. >> They're starting to get a little bit worried. Um, >> never want to hear magic. It's magic though. >> Magic. It It's so good that I can't tell you why it's good. It's magic. >> It's just going to be magic. Yeah.</p>\n<p><strong>[30:14]</strong></p>\n<p>>> Cool. People give him billions of dollars. >> So, from what I understand was [sighs] they so they based my reporting for an article we'll get to later. They knew about a year before this that they were in trouble going into summer 2024 for sure. they uh this project arion their sort of next large model training after GPT4 was not generating the same leaps in performance that they had seen before and so this became a problem. This is why my understand is in the fall of 2024, tell me if I have this more or less right, they began switching to</p>\n<p>talking about things like 01 um reasoning models, models that were tuned. So they were they were on not even on this 45 base, but like I think those original ones were actually tuned off of the GPT4 base, right? So they were taking >> I think so, but Orion was such a mess. There was a Wall Street Journal story towards the end of 2024 where it was like it's costing a bunch of money and it keeps it isn't getting better. >> Yeah. >> And I I think that they were just >> and that was pure scaling. That [clears throat] was their pure last pure</p>\n<p>scaling play is they did the exact same thing they had done >> for GPT4 >> and they're like let's just do that bigger >> and it was so that's expensive because that's a big model and it just wasn't getting much better and that's why my understanding is they switch towards these tuning things because now like well what we'll do is we'll tune an existing model to do well on different benchmarks or give them specific features and talk about those particular features. So like reasoning is what really matters, >> not uh you know this model is just like >> it's clearly much more better at</p>\n<p>everything. Like that was kind of the GPT4 experience for a lot of people. >> What I think it is is it's test time compute. It's just reasoning as in like instead of I ask it to write a fanfic about Scooby-Doo doing Tianaan Square, it instead of it just burping that out, it breaks it down into steps of what is Scooby-Doo, what is Tiana Square and so on and so forth. Yeah. >> And then and that is the only way they started they were seeing reliable benchmark improvements. >> So walk us through that. So you you would you would query the LLM first to</p>\n<p>be like kind of break down the user's prompt. Break it down into multiple things and then they would make multiple queries to an LLM on different parts of this and put it all together. >> It's a little simpler. It's usually with an LLM before reasoning it. You would ask it this is very simple. You would ask it a thing. It would spit out an output. Instead here, when it spits out the first outputs, it's not sending them to the user. It's actually taking a query and saying, \"What is the user asking? Here are the steps.\" And this is all output tokens, so it's expensive, but it says, \"Okay, these are the things</p>\n<p>that I think the user wants to do. Time to generate something for each bit to make sure that I'm doing it right.\" And then the output happens. This allowed for improvements on benchmarks, and it had good returns on coding in particular. It also chewed up way more compute and this helped everyone because the benefits of just training models by shoving a bunch of training data into</p>\n<p><strong>[33:15]</strong></p>\n<p>them those are we hit the diminishing returns at the end of 2024 as well. On top of that, um there's the post-training aspect of basically correcting correcting behavior, saying this is a good output, this is a bad output. That's also where they saw it. And actually, I realize I'm getting ahead of myself. At GTC in March, so the next month, Jensen, >> we're jumping ahead to March now. So, what tell us about this? So in GTC in March two I don't know if you've got all the Nvidia stories because there's some weird stuff but Jensen Huang on the big screen showed like that pre-training so</p>\n<p>the shoving the data in we're past that era we're into post-training and inference now >> I think we on the episode on an episode where we used some audio from you as well I showed I think exactly that part of his speech. So was that that was in March of 2025 >> the big conference, right? Yeah. And that's where he was. Just as an aside, because we talked about this in the intro of the show today, why does this computer scientist in his like 50s who makes graphic chips with glasses insist</p>\n<p>on wearing jackets that seem like they came out of the prop department for MadMax Fury Road? What is going on there? >> I will defend. I had the men'swear guy on my show to talk about Jensen Hong's jackets. They're sick. >> Yeah. However, >> they got zippers everywhere. >> Yeah, he really needs to stop doing the racer jackets, though. Those don't those don't look right. But >> the GTC jacket [clears throat] was cooler because it looked like a psychedelic alligator skin. But like he often is wearing like race car jack like motorcycle racer jackets >> with all the zips.</p>\n<p>>> I love leather jackets. I can't pull off the races though. Nevertheless, he also during that put up a big a big picture that said we've s we've it heavily hit he was like we've done this many hoppers that we've shipped and now we've done 3.6 million black wells and ended up in an analyst Q&A having to correct himself to say oh I didn't say shipped I meant ordered and also it wasn't 3 it wasn't 3.6 or 3.2 2 million. It was actually half that because each GPU has two GPUs</p>\n<p>in it cuz it's he counts by the die. It was when you started to see Nvidia start doing their kind of riddles >> where it's like, oh, we didn't ship, we we we sold and they've been ordered from four of the last h the largest hyperscalers. And it was interesting because it was the last I think that March one actually brought Nvidia kind of back to life a bit because looking at the stock and the time they were kind of trundling and trembling and then they</p>\n<p>fell down towards the end of April. So this was an attempt to restart the hype cycle. But what was what's interesting as well was all he did was basically say, \"Yeah, we're going to have even bigger, more huger GPUs and everyone will buy them. We've sold so many and we love selling them and they're so good and they're so expensive >> and they are well and they were selling a a lot though like from their perspective they were yeah >> but</p>\n<p><strong>[36:15]</strong></p>\n<p>>> but the big other thing of that speech if it's the one I'm thinking about right is that that's where he explained for the media this way I took in the analyst why like the Wall Street Journal's coverage from the year before of like on the information's coverage of of Orion struggling this or that wasn't a problem and he made it very clear he was like look we were in a scaling era and that made us this good and now we're in the postcaling era where we do tuning >> that also requires a lot of of GPUs and that's going to keep us going. So he just sort of explained it in a way that I don't think it had been so clear</p>\n<p>before and then we were sort of off to the the media was back like okay we're good we're on track for things continuing to get we don't know what any of these words mean but the graph kept going up >> even as you pointed towards the postcaling age so that was I think like the first real explanation of like something that ch open AI wasn't really talking about it yet they were like oh >> it was a subtle shift as well because one of the great myths of the AI bubble is that all of those GPUs were for training. Very convenient to say that because if it's for training, well, we need them. That's the only way the</p>\n<p>models get bigger. I realize this is a few months in the future, but there was an MIT tech review piece in May of last year where it said that 80 to 90% of compute is actually inference. So, the truth is all those GPUs aren't building better models. It's just running the bloody things. Yeah. And I think that this GTC with Jensen Hang was an attempt to bridge that age to say actually the returns, the bigger benchmark scores that we love to see, they're going to come from actually renting more GPUs</p>\n<p>just to run the models, but we can make the models better by using more compute. Please buy GPUs as opposed to saying all of the all of the compute that we're going to use is frontloaded to build the models. No, we need all this compute and you need to sell these GP buy these GPUs even because these models to make them smart need the compute to stay smart. >> I see. So this was bad news for the AI companies and good news for Nvidia. This is why they >> but the AI companies don't like this because they're saying this is going to</p>\n<p>make it more expensive to deploy and run these products. That's going to hurt our profitability. Nvidia was saying to shareholders essentially uh this is actually better for us as the chip sellers because you know you need all of our chips just to use the product. It's not like oh you need you know once we train it then it's going to be cheap to deploy and maybe at some point hey it's you now we're shifting to a world where just running the product requires a ton of chips and so like we're great with the market if anything is bigger for us. >> The thing is if you look around the startups though they love this. They love saying test time compute. They love</p>\n<p>it. Uh they love saying test time computer um sorry uh but they really do they loved it because it was a way of saying well we need because think about it from a startup's perspective startup if they say I need a bunch of money for training that's a one-off operation or maybe a couple times a year I need a bunch of money for compute I'm going to need a bunch of money cursor and this happens I realized later in 2025 ended up raising like $3 billion that year</p>\n<p><strong>[39:16]</strong></p>\n<p>they didn't raise that for trading they raised that to keep running their bloody operations open AAI building out all these data centers building them out. Um, they were doing that because the inference cost of running these models, the inference scale to actually provide a service at any kind of scale required all the GPUs. So, it was really just kind of a cartel operation type thing where everyone and propaganda as well that yeah, we actually need all these GPUs just because these services are so powerful when the real word is lossy. These services are just inefficient crap piles. They're they're slovenly. It's</p>\n<p>like the right the very almost the opposite of what deepseek was about though deepseek I think I think deepseek required less GPUs for inference as well but it's like in the face of that deepseek story it's almost like the American AI industry came up with a reason why deepseek was both wrong and actually nothing to think about stop thinking about it stop thinking about deepseek and they did by by the end of April people had forgotten about it >> it's the F-150 strategy Asia starts producing like cheap reliable cars and instead of Detroit saying like well</p>\n<p>we'll also have to now create like cheaper light. That's what people want. They're like, \"No, we're going to convince half the country they have to spend $80,000 on a completely souped up pickup truck that's capable of pulling >> baby >> Raptor. We need a Raptor.\" All right. So, then we jump to April. You got this shift. Maybe you know where this came from. This seemed like a shift out of nowhere. So, now we have like leading up to these this talk of it's like very business focused. It's going to be agents. It's going to be like the age of uh testime computing is going to be the future. This or that. Then April we get um AI 2027 and suddenly everyone for a</p>\n<p>while is back to talking about AI doom and super intelligence. So for people who don't know AI 2027 was a >> like a fanfiction. I don't it was it was a story that had animated graphs >> and here let me read their description. We predict that the impact of superhuman AI over the next decade will be enormous exceeding that of the industrial revolution. We wrote a scenario that represents our best guess about what it might look like. It's informed by trend extrapolations. That's the whole thing, by the way. Uh, war games, expert</p>\n<p>feedback, experience, open AI, and previous forecasting successes. This scared the hell out of a lot of people. I mean, like, >> I just I just spent yesterday reading this thing and pulling it apart. So, I'm I'm I know all about I >> But the key is 2027, they had a non-trivial percentage of the extinction of humankind. I think that's the that's the headline thing from this, right? That that like in two years that could be the end of humankind due to AI extermination. >> So I want to be clear of who these people are. Daniel uh Koka Taljo I think his name worked at OpenAI for 2 years on</p>\n<p>the governance team. He was previously a PhD philosophy student at UNC Chapel Hill. On top of this, Daniel quit and he quit in the middle of June 2024 claiming that OpenAI was secretive and didn't care about super intelligence. Now, you'd think if you and they was named as</p>\n<p><strong>[42:16]</strong></p>\n<p>a whistleblower by Kevin Roose at the New York Times. You know what whistleblowers tend to do? They tend to blow a whistle. Daniel didn't. Daniel didn't actually say anything. Daniel had nothing to share other than he wrote a scenario in 2021 that was sort of accurate about what the future might be. And AI 2027 is him. the the flipping Star Codeex guy who was a psychiatrist who named Nick Land, a guy with a theory of hyper racism as one of his favorite writers. Like the people that wrote this are not scientists. They don't really know anything about anything. The whole</p>\n<p>thing is written. It's like thousands of terribly written words, lots of scary numbers. But when you read it, it hinges on one idea, just one that in 2025, open AI, sorry, open brain, um, open brain in that's their fictional company in the scenario, >> which could be anyone. Open brain invades the thing called agent one, which can do AI research. That is the entire hinging of the piece. It can do a Do they define what that means? No,</p>\n<p>>> they never define it. So just just to be clear, this thing this thing that was written to scare people to grift to help was the AI safety research nonprofit that's connected to the effective altruists. Anywh who the whole thing hinges on this idea that they invented an AI that could research how to build an AI they wanted. That's the entire game. They wrap it in the trappings of finance and technical sounding things. And there is a bit in it where it talks about neural functions and then cites a meta paper. When you go and read the</p>\n<p>meta paper, it does not site anything of the sort. The thing they quote is unrelated neurles whatever it was. It's an effective altruist thing. It's from less wrong or it's >> sorry this thing really deeply pissed me off because I had people calling me I had people friends of mine people I love and respect who were terrified by this and that was the intention. Sorry, I'm freshly pissed about this, >> but you got because I I I read it at the time as well and I came to the same conclusion as you was like, is anyone picking up this whole thing hinges on</p>\n<p>they never addressed the question of how do you build a super intelligent AI? Like what's it going to look like? Like what's the architecture? How does this work? They just said we'll build an AI that could build a better AI and that'll build a better AI and a better AI and then they'll just they'll figure it out. The AIS will figure it out. But I, as I keep emphasizing, we do not know how to build a language model that can produce a a software for AI that's better than what any human can produce. That's not how language models work. >> They they it's they can produce the type of code they've been trained on. So</p>\n<p>unless you could train the AI and tune it with lots of examples of really better AI, it can't build better AI, right? It can't leap beyond what it's trained on. And no one is even close to this. We talked about this earlier. We're tab completing function calls with AI right now so we don't have to look things up and vibe coding buggy dashboards. It's unclear how did you get</p>\n<p><strong>[45:16]</strong></p>\n<p>from that to brand new models of human intelligence that that the collective AI community and decades of work couldn't figure out on their own. I'm not quite sure where that leap happened. So I was frustrated by this one. Um I was frustrated by it as well because it all came down to the same it's all it's all the f uh argument, right? So, like the the way I talk about I rant about this on my show a lot, Ed, but the way I talk about it is what happened was is that the existential risk community came out of effective altruism in the 201s, which was a community that looked at um we want to look at um existential risks</p>\n<p>that might be unlikely but have really high impacts if they happen like asteroid hits, pandemics, and super intelligent AI because, you know, we're doing our rationalist thing and like the expected value of spending money now on rare things with cataclysmic outcomes is positive. That's basically was the this was Nick Bostonramm's existential risk center at Oxford was we probably won't get hit by an asteroid but because it would eliminate all of humanity it's actually a good investment to invest now in networks like look at asteroids and so one of the the hypothetical risk they</p>\n<p>looked at was super intelligence post chat GPT they went through this weird sort of uh shift in their brain where they went from this was this hypothetical risk we were looking at along with asteroids and pandemics to what What if it was actually happening now? Well, if it actually was happening, we're superheroes. We're the ones who like pointed and there was this shift that happened in that rationalist effective altruism community with the people working on existential risk where they shifted from hypothetical to u we're just going to convince ourselves</p>\n<p>it is happening because that makes us the most important people on earth. >> You're being you're correct, but I think you're even being kinder. I think they were waiting for a moment to drift. I think they were sitting there waiting being like when is the what's a thing we can grasp onto so that we can start draining cash >> so we can get a bunch of attention. Look at what happened with AI 2027. I'm sorry. >> Daniel's a true believer. I think he's made him a super grifter. I think he's a cynical grifter. >> Interesting.</p>\n<p>>> Look at what happened when he left OpenAI. He made this big song and dance with this petition back with Jeff Hinton who I have some other feelings about with all of these smart people. Oh, OpenAI is doing such bad things. What are they? I couldn't possibly say I had you. He made all of this noise and the media ate it up. All this whistleblower, this brave guy who came forward. What did he do? Nothing. He didn't share a goddamn thing. He was so concerned but couldn't say what about it. Kind of sounds like AI 2027. Oh, I'm so concerned this will happen. What will</p>\n<p>happen? I don't know. >> The the agent China will steal agent too. They the China agent agent China. They're scary, right? Be scared. My nonprofive self-improvement. That was the thing he was talking about was he incorrectly was saying we're almost at the point right</p>\n<p><strong>[48:18]</strong></p>\n<p>now within OpenAI where the AI is going to be doing the programming for us and then we're going to have this this takeoff idea that goes back to the 1960s. This recursive >> I have another word for incorrect lie. >> Yeah, >> I think he is a grifter. I think this whole thing was a grift. I think it's connected to the Star Codeex guy. I think it's connected to the effective altruists. You'll notice that those popped up with FTX too. They pop up with everything. They've been looking. >> I I don't know. I just I'm extremely extremely cynical. I don't know. I just feel</p>\n<p>correct on this because these people if they because here's the thing. Here's my entire feeling about this. If they actually wanted to talk about scary bad things that are happening, I don't know, talk about the Kenyans who are training these models for what, like $2. >> Yeah. >> How about you go and talk about all the theft that's happening? How about you go and talk about the gas turbines that were being spread everywhere? How about we talk about the environmental issues? How about we talk about the thing happening today? It's the same problem with Jeff Hinton. These people want to talk about, oh, what if the computer</p>\n<p>does this? Which is fine. We should have those conversations. But when you were talking about AI safety, why don't you talk about now? Because if you talked about now, you'd have to do something. You would actually have to take action, take a position, make enemies. you would actually have to do something that mattered. Instead, they do this cynical grift where it's always whatever you're scared of is just a it's a couple years away and we got to do something now which involves me doing a speech which involves my speaker fee which involves my nonprofit which involves me doing a</p>\n<p>panel and speaking to politicians about today. >> So, how do we understand Hinton, right? Because okay, there's this interesting situation where obviously like Jeff Hinton doesn't need money. He made a ton of money when they sold his startup to Google. Also, he clearly knows the technology, right? He has he in in computer science circles. He was just at Georgetown like he really was the guy in the wilderness that was pushing trust me like back propagation on these deep networks really can work. You just need the data like that that was >> right. Sure.</p>\n<p>>> You know, he knows the tech. I did a thing on my podcast, you know, earlier in the year where I was like comparing him talking about risks with uh Yakowski and it's really different. See, Yakowski comes out of effective altruism, not a computer scientist and he's like LLMs are coming alive and are have their own intentions or whatever. Hinton knows that's not true because he helped invent the technology. And if you parse his comments, he's very careful. If you really parse it, he really is saying, what he's actually saying is, uh, we made more progress on this research than we thought we would. So, it stands to</p>\n<p>reason the same thing could happen with some new type of machine that we don't know how to build yet that could be a threat, right? So, he's actually being careful because he knows LLMs are not coming alive or autonomous or this or that, though. He's he kind of merges us together, but I'm trying to understand his motivation, right? because like he knows LLMs we're at, you know, they're smoothing out. He knows he's very careful when he talks about it that it's</p>\n<p><strong>[51:19]</strong></p>\n<p>we may invent so we may invent the machine in the future like we should be careful. Um it's being more sensationalized the way he's reported or talk about to somehow make it seem like the stuff we have now is dangerous. But is that just influence? Is it just like people want to hear what I have to say? Like why is he What's going on with Hint in your opinion for like Hinton's like big push for we should be really worried about AI? >> I think he wants attention. I think he wants glory and I don't think he wants to change anything. I think he's quite happy with the current scenario. Again, I say the same thing I say about the AI 2027 people except with more. Jeff</p>\n<p>Hinton is a gifted scientist. A noble was it noble? I forget >> and >> yeah, I don't know what the titles are and he is a scientist and all that, but again, Jeff Hinton is this massive microphone. Do you ever hear him talking about the theft, the environment as the primary thing? No. It's always couple years away. What if this happens? Wouldn't that be scary? What if it What if my grandmother had wheels? Then she'd be a bicycle. It's this thing of he has</p>\n<p>all of this power and attention and so-called knowledge. What's he use it for? Nothing to do with what's happening today whatsoever. He doesn't go on stage and say, \"Hey, this is these gas turbines are popping up there polluting black neighborhoods.\" He doesn't talk about the fact that it involves >> what the turbines the turbines are for generating the power demands of the data center. >> So what happens and very simple thing of because it takes so long to build power what these companies be doing. Elon Musk famously and uh Stargate Abene for Open AI they're doing this as well. They have</p>\n<p>these giant gas turbines that they put out that can be spun up quicker. The thing is gas turbines are sold out. they've been sold out and they have like years long wait time. So they're using old ones which are less efficient and pump out more horrible gas. Anywh who I a non-scientist know that and I talk about it regularly because that is a harm from AI. Why doesn't Jeff Hinton a socalled AI safety guy a guy who cares about what AI is doing never talk about what AI is doing? He always talks about what it might do. And I consider that a grift too. And whether him being</p>\n<p>scientific only makes it a more cynical grift. they feeling same deal, but she at least has a startup. Even though I think world models are another grift that people are going to move to next. Nevertheless, with Hinton, he's always going out there to go, I'm so scared of the computer. What if the computer does this? He, to be clear, we should have these discussions. Those are valid discussions. That's all he does. He doesn't give a an rat's ass about any of this. I think he's as cynical as the rest of them. I'm sure he believes this</p>\n<p>stuff, but he doesn't give a damn enough about the human beings that are alive today. He isn't actually trying to change anything. He isn't trying to He loves signing open letters. He loves doing paid speaker opportunities where he gets up and goes, \"The computer is scary.\" But that's the thing, doesn't he? Why</p>\n<p><strong>[54:19]</strong></p>\n<p>doesn't he talk about large language models all the time? Gary Marcus does more for AI safety than Jeff Hinton does. I don't care if people are mad about that. I think it needs to be said. I have my issues with Gary. But at least Gary goes out there and talks about the actual harms. Jeff Hinton talks about himself. Jeff Hinton spreads approximate fear, approximate danger, but never really talks about today because yeah, we should discuss what would happen if this happens. Sure. But how about if you're so And his whole</p>\n<p>thing was he quit because he was worried about they what they might do. Why? Why? Like you're so worried. Why aren't you doing anything about it? If you are you an activist? Are you going to tell people to bomb D? Like what is it that you want people to do? And the answer is Jeff Hinton doesn't want people to do anything. >> Yeah. >> He wants to sit there and worry about something that might happen without dealing with anything today because that would require him to actually do something. That is an interesting point more generally about the sort of expert class turn to AI safety is where there's</p>\n<p>no specificity. I mean when you saw scientists leaving the Manhattan project worried about what they did they had a very clear program right the concern unit there was we should test banan we need to roll back nuclear weapons we need to create these treaties to do whatever you're right is an interesting point that you don't see here's what we need I mean Yakowski does but he's kind >> they all signed a letter they all sign >> thinks we should bomb data center so I guess there's someone who does have ideas but he's a little bit yeah >> I think Yud's a scumbag but I give him more credit for at least saying</p>\n<p>something for saying something cuz they love signing open letters. Oh, this the open letters that they sign. Oh, oh, we should stop working on AGI now. Don't worry, we haven't started. How about we talk about things happening today again? We can't possibly. >> So then, >> all right. So then jumping forward to um May, this is when the big headline then was uh Dario Amade. This is when he gave the quote that went everywhere about AI could eliminate half of all entry-level white collar jobs in the next I think he</p>\n<p>said up to next 5 years. Um this was part of a longer interview where he also uh did this equation of test to general capability. So he said AI was at a high school level then it became to a college level and pretty soon it'll be at now it looks like we're getting at a PhD level. So now you can imagine replacing what you would hire a PhD level trained person to do in a job. From what I can understand that was just referring to uh math test that the they're referring to</p>\n<p>math test that the model had passed and and someone had said the problems on this math test which they had tuned it to do well on were problems that you might give a grad student on a math test that got extrapolated to AI can do what a PhD level employee could do. I mean, I guess if the employees job was to solve math competition problems,</p>\n<p><strong>[57:19]</strong></p>\n<p>>> that's kind of it. Well, to quote Gunoucher from Blue Sky, CEO of Oreo cookies, the Oreo cookie is as important as oxygen. >> Like that's everything that Amade says is just, >> yeah, it's like a it's like a PhD student. And I genuinely think that there is this thing with people. I've heard Casey Newton say things like this as well where it's like they're like, yeah, the proof that this is useful is people are using it to to do their homework. And it's like, do you think there's a homework goblin in colleges? Do you think that's how colleges are run? You write the homework, you the homework goblin eats it, the goblin pays</p>\n<p>into the endowment. Is that do you think we go to college to do homework? Now, this is a larger discussion though because there is a degree of college that's kind of like that. That is a problem. But we didn't need a solution to do homework. We need to fix college. But Amade, just like Alman, just like all these people, just says [\u00a0__\u00a0] He just says stuff. Why does he always he always says I'm reading the quote here. Why does he always say things like >> Amade said he was highlighting it to warn both the general public and the government of what's coming. This is his</p>\n<p>stickick, right? More so than Alman. He's always saying I I'm look, I'm the bearer of bad news. I'm the one who's willing to tell you straight. Uh but it it delivers the same message in the end as Altman's more optimistic messages, which is this is the most important technology. It's going to change all the things. All the money needs to come to the people who are building it. It gets you to the same place, right? Like whether you're saying I'm worried about it or excited about it. If you're saying this technology is going to create 20% unemployment, I mean, if I'm an investor, I'm like, \"Oh crap, I got to be investing in the company that's going</p>\n<p>to take those 20% of jobs, right?\" Like, it's still a very optimistic prediction >> or anthropic, right? >> It's cynical. It's this cynical crap that people repeat every single time. They fall for it every time. He made a prediction at one point. It was like 90% of coders will be replaced in the next six months. I think he did that in March. Didn't end up being true, it turns out. >> But the thing that I think people need to realize is that these people aren't special. They're not they don't know. Like they know stuff, but like, oh, it's</p>\n<p>going to replace 50. It's just like a PhD student. They're just saying stuff. I could make this crap up, too. >> Well, he says here, this I think maybe this shows it. He also said there I'm reading a Fortune article here. He also said there is still time to mitigate the doomsday scenario by increasing public awareness and helping workers better understand how to utilize AI. Okay. So there is a solution. >> So make people aware of AI and then get people to use it. >> Buy more Claude subscriptions and you will prevent the 20% unemployment. >> Yes. >> All right. We figured</p>\n<p>>> uh All right. So then, by the way, having just done an article on agents and my cynicism meter is off the chart for that particular interview he was doing because they knew at that point that even just like using the mouse or get the like back in decisions wasn't working. Like they they were nowhere near this. Um I guess you could just say, but people are reporting it. All right, we get into the summer. I think</p>\n<p><strong>[60:20]</strong></p>\n<p>like the big news in June that got reported was the MIT your brain on chat GPT study. So that I feel like this was there was some other research that came out early summer that was poking holes. There's the big Apple paper and then there's the one research at ASU. There's a bunch of papers they were a little too technical I think for journalists to pick it up. But they were poking holes in the reasoning narrative, right? They were saying >> the Apple one. Yeah. >> Yeah. Like this is kind of nonsense. It's not reasoning. It's working with it's it's just like there's no generalization of concepts happening</p>\n<p>here. Um that's where they took you know problems that it could solve and then they made the problem size a little bit bigger and it catastrophically fell off the cliff and it's like oh it had just seen this size of the it's not generalizing it's not so but those were a little bit too academic right but then there was this June 10th paper from MIT Media Lab that introduced this idea of cognitive debt it was like more easy reporters or writers so I think this made more sense it was this they said we studied people writing with the AI and it it made them it was worse writing they learned lesson and they were</p>\n<p>dumber. >> Yeah, >> that went everywhere. >> Uh, so what what did you what do you think about that paper? What do you remember about that? >> I mean, just that it was a moment when it it it didn't change a ton, but it made people think about this a little bit more. It's the first time I really remember having conversations where people were like, \"Oh, maybe this is actually having negative effects.\" Cuz up until then, people still had this weird thing of, \"Oh, yeah, well, this is a way of learning. This will be your teacher. This will be you could be a student and learn anything.</p>\n<p>>> Saul Khan was saying this, right? Like you had a startup >> again the the CEO of Oreo situation. It's like of course he's going to say that. Sal Khan has been on the AI thing for a while. I mean Khan Mingo I think they had a Wall Street Journal story in 2024 that it just got math wrong. Might be Washington Post actually. But this that study the MIT one made people go oh this isn't teaching people stuff. It made people realize this is actually not an assistant. This isn't intelligence at all. This is a dumbass machine. This is</p>\n<p>it. It can fill in gaps of stuff you already know, but if you don't know it, it fills in the gaps wrong or not at all and makes you reliant on a machine that doesn't know stuff. >> I I always think it it's like the story about Washington DC, like what stories pick up. It's not the stories that teach you something you didn't know. It's the stories that confirm a pre-existing bias, right? And so I think everyone was like this this has got to be a problem that you're using this the right like this can't be good. And so when a study came out I don't know that it's that great of a study by the way. I mean I I</p>\n<p>don't want to cast a spirit. I looked at it a little bit at the time. I remember thinking like this was not meant to be like a super carefully researched like etc. But anyways that >> it was it was a little more academic than a blog. >> All right. So then this is where where things begin to get um this is like the real turn is into the summer we get the August >> and we get the GPT5. >> I think this was the this was a big turning point. I I I have kind of the</p>\n<p><strong>[63:21]</strong></p>\n<p>tick tock here, right? So So on like right as it came out, Altman right before it came out, Altman was doing like a a really big This is going to change the world song and dance. He went on uh oh god, what podcast did he go on? I don't know all the podcast the um one of the comedians in the Rogan circle. >> Oh >> uh you know who >> Theo Vaughn. So he goes on Theo Von compares himself to Oenheimer um and then chokes up just thinking about how powerful GPT5 is like what</p>\n<p>it's going to do. He's like what have I wrought? It it was like basically the what I had in my mind in that part of the interview was the scene in Oppenheimer where Oppenheimer's in the gymnasium and they're celebrating the dropping of the bomb and he keeps like you know it's this like cinematic Chris Nolan IMAX moment where he's cutting from that the images of the of the explosion and the the bodies and Hiroshima and it's like the most fraught moment like what have I wrought like the the emotional climax of the movie that was Sam Alman on Theoon talking about GPT5</p>\n<p>>> then almost immediately So by August 11, he's saying AGI is not a useful term. Like let's not talk about that anymore. Like we don't need, you know, let's not be talking about >> let's move away. Let's move away from uh expectations here. >> And then uh my I had my article which was one of several that came out on the 12th. So within five days and I was actually on vacation, but I remember talking to my editors and saying this has to come out like this is a big deal. PPT5 is a big deal. It is not I think this is opening people's eyes. So that's when I wrote my what if AI doesn't get</p>\n<p>much better than this article for the New Yorker which I think I quote you in. >> You did and there was I think the New York Times had a big article right around this time as well. Uh >> and so >> and so did and the biggest was Eds obviously. >> No, but I did reporting on GPT5. >> Oh, you had good report. Let me let me just point people towards your podcast. So that whole month [snorts] I mean you had a couple really good episodes where you got super into the weeds. not super in the weeds, but like it was the deepest reporting I had heard on the technical side of like how GPD5 worked</p>\n<p>and didn't work and why it was going to be that's where I learned about um to get these benchmark numbers they could brag about. They had to use these completely like cost ineffective you know in which wasn't going to work. They lost their caching. I think you were deep on that that when you used GPT5 to do this reasoning that did better on the benchmarks, you couldn't do cached versions of your all the stuff that made it >> it's really simple because it sucks. So story from 2023 by the way Sam Alman says GPD5 underway and will</p>\n<p>substantially differ from GPT4 untrue very similar GPT5 the thing it did that was amazing was that it has something called a router model in it which means that it would choose the best model for the job. Now the problem >> they were solving a practical problem that this isn't a originally it was supposed to be this will be um how 9000 and then it became we we introduced so</p>\n<p><strong>[66:23]</strong></p>\n<p>many models in 2025 and late 2024 that really like the main practical thing is we'll have a model like choose those for you so you don't have to know which one to use. It had kind of gone from this is going to take over the economy to like we made things complicated ourselves now we're solving our own problem by having it automatically route itself. But that or as you reported even that feature caused a lot of problems. >> Yeah. So the router model was reported incorrectly everywhere as being more efficient and a way to reduce costs. However, due to a source that I have I</p>\n<p>found out it actually increases the cost because when you have a large language model say you load a query you load chat GPT like write some code here. When it does that the system prompt is you are chat GPT you are an assistant that can do these coding things. It adds this text in front of your for the listener. It adds a lot of text in front of your text before it sends it to the model to give the model instructions about how to answer, what tone to use, like what to avoid, >> what tools, like do you use a Python tool or a web search tool. Now, before</p>\n<p>before this router model, what it would do was you would choose a model and it would have that system prompt and it would not have to keep reloading it. It wouldn't have to keep entering that in. would just have it and be able to cache that and then say, \"Okay, this is what I'm working on.\" Because of the router model. >> And not only that, just to be computer sciency, they could actually um because it's sequential when you run things through these machines is sequential. They could run all of this through the f the systems prompts always the same. They could run it through and get the state of uh all these embedding. Like a</p>\n<p>lot of math and computation they could cache and then not have to run all that through the GPUs. they know what state everything is in after that system prompt. So they could have all of the effect of having processed that system prompt without having to actually uh do the inference time for it. So they could cache it >> in a way that like okay we can have a huge long system prompt and not have to pay for it >> every time we do a prompt uh every time a user submits a new prompt which was very important from a money saving. All right go on</p>\n<p>>> except that all that goes out the window. every single time it routes to a different model it has to do it has to completely start again has to wipe the system prompt every time it is less efficient my source was telling me I can't exactly explain how but source was explaining that it was actually creating more overhead and the people on the infrastructure side were saying what are we doing like this seems like there were straight up people saying not sure this is a great idea like I'm not sure like this seems to be creating more overhead we cannot cache the system prompt this is causing</p>\n<p>I tried I reported this at the time and I took it to multiple reporters and they went no it's not. I had multiple reporters go it's not true. >> I show them the thing. >> You talk to reporters and they >> multiple reporters. It's not true. >> Not not to blow smoke at you but I will say no one else had that technical story. I remember listening to your your better offline podcast. Why aren't there more people with this technical story? I mean I had just finished my own piece</p>\n<p><strong>[69:23]</strong></p>\n<p>where I went deep on scaling and tuning and trying to explain that or whatever. um which I thought was also being super under reportported. Um but you had like a really good technical story. No one else had it. I don't >> I got lucky. I got lucky. But I also talk to sources regularly and I know what I'm looking for. >> All right, we're going to take a quick break here to hear from some of the sponsors that makes this show possible. If you're serious about protecting your time for deep focused work, but still need to manage meetings and responsibilities in your personal life, today's sponsor is for you. We talk a</p>\n<p>lot on this show about avoiding the chaos of modern work. But the truth is, discipline alone isn't enough. You need tools that protect your focus. That's where Reclaim.ai comes in. Reclaim is a smart calendar assistant built for people who treat their most valuable resource uh time as their most valuable resource. It automatically defends your focus time. It schedules meetings. It resolves conflicts and protects time for habits, tasks, and even recharging breaks. Now, here's the best part. It's not rigid. Reclaim Reclaims AI</p>\n<p>dynamically adjusts your calendar around your shifting priorities so you can stay flexible and focused. Deep work won't get crowded out anymore. The average Reclaim user gains seven extra productive hours a week, cuts overtime nearly in half, and experiences less burnout and context switching. Reclaim.ai isn't just another scheduling tool. It's an intelligent time partner that helps you prioritize what truly matters every week. Try it and reclaim your time for what really matters. You can sign up 100% free with Google or Outlook calendar at reclaim.ai/cal.</p>\n<p>Plus, Reclaim is offering my listeners an exclusive 20% off 12 months with discount code 20. Get your reclaim recap 2025 when you sign up, which is a year-end review of how you spend your time across deep work meetings and work life balance with 30 plus productivity stats. That's worth signing up. Just that alone, you got to check it out. It's really cool. So, go visit reclaim.ai/cal to sign up for free. That's reclaim.ai/cal. I also want to talk about our friends at</p>\n<p>Caldera. Here's the thing about aging. When you're a guy, when you're young, you don't think about your skin, but then one day you look uh you wake up, you look in the mirror, and you realize, \"Oh my god, I look like a grizzled old pirate captain. What happened? You weren't taking care of your skin.\" Luckily, this is where Calira Lab enters the scene because they have a high performance skincare line designed specifically for men. They've got these great products like uh the good, which is an award-winning serum, the eye serum, which helps with puffiness under</p>\n<p>your eyes, and the base layer, which is a moisturizer that you just use every day. Look, this stuff works. In a consumer study, 100% of men said their skin looks smoother and healthier. Now, skincare doesn't have to be complicated, but it should be good. Upgrade your routine with Caldera Lab and see the difference for yourself. Go to calderab.com/deep and use code deep at checkout to get 20%</p>\n<p><strong>[72:25]</strong></p>\n<p>off your first order. All right, let's get back to the show. >> Like that's really it's like the moment GPT 5 went out, I went to source being like, \"Hey, you hear anything?\" Like real simple. I don't even do much source stuff because I have like other stuff I'm working on. But this one was just like I went to someone an infrastructure provider and I said, \"Hey, have you heard any GBT5 stuff?\" And they said, \"Let me check.\" And I went, \"This.\" And it was just I I it shocked me that no one else wanted to site it. It shocked me that I had multiple people who were</p>\n<p>just like, \"This is not true.\" And I'm like, \"I can show you stuff that would prove it.\" They didn't want to see it. >> Interesting. >> It's just it's deniialism. It's because when I I understand when you've got editors who are pro AI, when you've got uh other people you work with who or perhaps you yourself want these people to win and you don't want to piss off their PR people. You don't want to lose your access. I get it. But it's like this is this is reality. >> They don't have any no one no one has</p>\n<p>access by the way. Uh I mean no >> all access is control. They won't give you access to anyone. I'm into legacy media. They give no they give no these AI companies give no access. >> They and they play I've heard open AI place people against each other at outlets. I've heard that I've heard them straight up say apparent I'm sorry. I've been told that apparently they will straight up say if you piss us off we'll stop responding to your emails. >> Yeah, >> little worms. >> Yeah. No, I believe it. Um but this did change a lot of things. So again the</p>\n<p>blow a little bit >> everything. everyone was just like [snorts] >> they were under so because it was hard to ignore the GBD5 wasn't that different um and had these other problems. the thing I saw change. So there's articles the hey scaling this is a problem you know that's why my headline was like what if AI doesn't get much better than this like that was like a new idea for people but the thing that that seemed to really open up was a story that really only you had been covering for a year so for a year plus you had been actually gathering earnings revenue numbers</p>\n<p>you've been looking at earnings reports and you had been making the case for about a year up to that point the numbers don't make sense on these companies look at how how much they're spending, how expensive this is. The numbers don't make sense. This cost way more to run than they're getting in revenue. When is the musical chairs game going to stop? Post GPT5, all the major publications sent good financial reporters to do these type of stories. So, we get, for example, the New Yorker had a big uh in the magazine a big wait, is this a bubble article?</p>\n<p>The Wall Street Journal had several um including the one in September. spending on AI is at epic levels. Will it ever pay off? Um, we began to get really good analysis like comparisons to level three and the what happened with like laying the infrastructure. The New York Times started writing these articles. They had covered it the bubble possibility zero and then they they uh started covering it multiple times. That's my story of</p>\n<p><strong>[75:27]</strong></p>\n<p>September is all of these different bubble articles. I'm assuming that was all basically opened the floodgates were opened by GPT5 underwhelming. It just sort of changed the way that people categorize like wait maybe there is there could be a problem here which by the way I have experience with from my social media reporting back in the day just as a quick analogy everyone thought in the media that I was eccentric for my stances about social media is a problem we shouldn't be using it this is not a fundamental technology this is a real problem and I was shunned</p>\n<p>and attacked and people were coming after me >> and then post Donald Trump election where he was successful on Twitter it planted the seed of like, oh, maybe social media isn't just done for good. And it opened the floodgates and all of these issues with social media was suddenly open game to be covered well beyond even what I was talking about. This felt similar to me. GPT5 underwhelming opened up all of these the possibility of all these stories including on economic struggling. There was there was also a big story around August where it was I think it it came out that AI data center capital</p>\n<p>expenditures made up more of GDP growth than all consumers spending combined and then in September you had that insanely funny story where as we've discussed $30 mill billion deal with Oracle between OpenAI and Oracle where OpenAI will give them the $300 billion they don't have and then Oracle will serve them compute from data centers that are not built yet. And I think that was that happened and sent the stock spiking. And then rapid fire, we had this AMD deal</p>\n<p>>> where AMD it was a really funky deal as well. It was, let's see, that came out that was I'm skipping ahead to October here, but the AMD deal, but basically by October, mid-occtober, I think, OpenAI had agreed to like 26 gawatt of data centers. And there's just a bunch of funding that happened around here as well. But it really felt like the air had been sucked out of the room and people started coming. >> There was [clears throat] scrutiny suddenly on some of these stories in a way there wasn't four months earlier.</p>\n<p>>> Yeah. And it's interesting because even with that scrutiny are can we do October as well now? Is that >> Yeah, we can be in October. Yes. >> So I bring this up because even with all that scrutiny and the reason I'm typing is I need to bring up the need to bring up these deals. So in in okay in September it was this Nvidia I'm going to do big old air quotes. Nvidia does $100 billion investment in OpenAI. Now what I really remember at that time was no one having any details about it and</p>\n<p>indeed the writing within the Nvidia and OpenAI announcement not really being clear about when things will begin or indeed if any agreement was signed. And I went to multiple reporters. I'm like, \"Hey, look, first of all, have you done the maths here?\" Cuz if you did the maths, they it would cost OpenAI like I think over a trillion dollars for the compute and their data center deals. And</p>\n<p><strong>[78:28]</strong></p>\n<p>I put that out fairly early and then just other people wrote the same headline and did not quote me. Thank you. Um, but OpenAI agreed to a 6 gawatt deal where they built 6 G of data centers for AMD and in return would get 10% of their stock. Never happened. Broadcom did a deal with OpenAI 10 gigawatts of data centers which we will get to in a minute because some funny stuff has happened and then this Nvidia deal. >> Now what was funny about this was I went to reporters and saying like hey look nothing's been signed. This is a lot of money and also a gigawatt data center</p>\n<p>takes about 2 and a half years and $50 billion to build. They meant to start these data centers. The first billion dollars that invid$10 billion even that Nvidia was meant to send to OpenAI was meant to be next year. Same deal with AMD, same deal with Broadcom. So meant to be in 2026. And I went to people. I'm like, hey, this isn't possible. Like this is not this is quite literally impossible. We cannot do the like you can't build data system. And everyone's like, >> yeah, you know, well, they're working out the crap they've been saying for the</p>\n<p>last year. It's just like they're working out. They're working out. >> What's the advantage they get by announcing these deals? Can they like mark it up as like future assets? Does it help with stock? Like what what would be the motivation spike? >> Stocks. Okay. And so Oracle added $300 billion to their remaining performance obligations. Broadcom added like 50 billion I want to say >> because you can mark this you can mark this up as expected revenue which when you're then doing the calculations and figuring out you're like oh this makes this a more valuable company because it's the revenue it has or expected has gone up</p>\n<p>>> and because the markets are I assume run by toddlers everyone believed it. Everyone was like wow wow number go up so big number so huge. Well, number didn't stay big for long and things started to fall apart and it started it got to this point where people even people who were quite cynical started going >> one moment. Anyone done the math here and the FT has stepped up. The Financial Times has been pretty on top of this the whole time, but they really stepped up</p>\n<p>and did some analysis and it was just they also did a trillion dollar story without citing me. Well, the Brits are more suspective. Um, but nevertheless, it was this thing of >> everyone suddenly starting to do very simple math of like, well, OpenAI's projected to make $13 billion this year and they owe $300 billion. How do they how do they pay that? They're going to lose billions. and the information put out a story saying OpenAI was spent they planned to spend like over $150 billion</p>\n<p>or something >> didn't really make sense mathematically at all because the $300 billion but it was very interesting time actually that reminds me so when the Oracle announcement came out OpenAI had leaked that they would spend I think $155 billion or something</p>\n<p><strong>[81:28]</strong></p>\n<p>>> but it was days before the Oracle announcement was made. So, OpenAI leaked their costs. I'm doing air quotes again cuz I don't trust any leaks out of OpenAI 5 days before the the Oracle deal so that no one would do the the backmath and go, \"Wait a second. This doesn't make sense.\" I love watching this kind of like disruptive public relations work. I think it's cool as hell. I think it's good that I'm watching because what's going to end up happening there is no one gets paid, which as in a few months time in this story we'll get to. But I actually had my own story in</p>\n<p>October as well. I got Anthropics Amazon Web Services bills. >> Yeah. So what's going on with that? What >> $2.66 billion spent in three month uh sorry three quarters. $2.66 billion and that's just on AWS. And from what I know they also spend about the same amount on Google Cloud. So Anthropic will probably make I'm going to say $5 billion this year. uh they were by I would think by the end of September they between Google cloud and Amazon Web Services had spent more than $5 billion. So they're just</p>\n<p>annihilating capital just burning it down. And it actually leads me to an important statement which is anthropic has done the other thing that Dario Amaday has done is he's framed them as this more efficient company. This company that is more efficient that doesn't burn as much as OpenAI that spends less on training. But when you look at the numbers it tells a different story. OpenAI in 2025 raised $18.3 billion other than Soft Bank's Porsche, but nevertheless, $18.3 billion. People say Anthropic, they're spending less money, they're more efficient. Anthropic</p>\n<p>raised $6.5 billion. Basically the same neighborhood of numbers, but Anthropic has done such a good job just lying to reporters and spreading these rumors that people believe this. I think Anthropic is as big a crap pile as Open AI. They're just as lossy. they burn just as much money. And yeah, I mean by this point, by the end of October, I think most even outlets had begun to say like, \"Oh crap. Oh crap. Were we wrong? Were we wrong for 3 years? Did we get</p>\n<p>did we fall for it again?\" And they did fall for it again. So then if we jump forward to well the other big story in October is uh Sora the app which confusingly is powered by Sora 2 the model because there's also a Sora one model >> uh that didn't land I think the way I guess open AI hoped that kind of freaked out a lot of people like what are we doing here uh who is asking for this but was that a sign I took that as a sign of uh a little bit of a sign of desperation</p>\n<p>this is open AI looking Tik Tok does $33 billion a year in revenue and like we need money. So can't we do Tik Tok with AI and like help fill back fill, right? So like in other words, if you were about to automate half of the jobs in the knowledge economy, you don't need a Tik Tok clone. Uh you don't need to talk about around the same time they also talked about allowing GPT chat more erotica etc. You</p>\n<p><strong>[84:29]</strong></p>\n<p>don't need that. Like who we're about to create the $3 trillion that Mark Benoff talked about. Who cares about that? But the fact they were putting that out was start sort of taken as like a uh oh type of moment which I don't think is what they were hoping. >> I don't even think they thought around I mean people bought the Tik Tok thing hookline and sinker. I think they were just desperate. I think they did key jingling. I think like look look you you generate videos please please please keep using this. >> Don't talk about the Oracle deal. Don't talk about the Oracle deal. Look at the look at the keys. >> But they did like an Oracle style deal</p>\n<p>but with their own software. So you had this situation where Sora now Forbes estimates that it cost them like $15 million a day or something. >> Yeah. >> Based on my sources, that number might be smaller. Sorry, that number might be too small. I have compelling evidence that it to run 13 instances of Sora 2 required 900 I or 840 H200 GPUs. That's 13 instances. That means 13 generations at once.</p>\n<p>So that's 10 like this thing is really expensive. >> What's the cost? If you want to make s I you want to make sore videos. What do you need? What level? I mean there's still >> I mean you need to use their API >> but you have to use um you have to have their $200 a month level or above or is it more? >> Um no anyone can use Sora the app now. >> I mean for creating the videos though, right? So there that's you have to >> well I mean create the you can create them on the app >> you're limited but if you want to use the API I think it's like [sighs] >> a couple dollars per video and per video</p>\n<p>just means anything it generates whether it's good or not >> which is not that's not uh that's not sustainable like I mean the whole point about Tik Tok's model that's brilliant is all of the compute involved in taking videos editing videos um trying a bunch of different experimentation is all done on the user's phone they pay for it. >> But also, Tik Tok loses money. >> Yeah. >> Tik Tok is an unprofitable business as well because they >> just the cost of hosting and because of just a marketing and hosting and streaming a bunch of videos, I guess.</p>\n<p>>> Yeah. Yeah. It's still an expensive and also they are poised for growth. But putting that aside, you're still completely right. That's how that service runs. Sora was just an attempt to try and it's like um I don't know like when you ever see a couple that's like about to break up. >> Yeah. And they're like, \"Yeah, we're we're going on vacation. It's great. God, I love them so much. It's all going so well. I love it here.\" And Sora, you had all of these. What was funny was Sam Alman, I think Truanon had this point where it's like it looks like Sam Orman put himself in it so that he would make</p>\n<p>himself famous and all that ended up happening was people did like Sam Orman stealing from Target >> or Samman crying >> and it was just >> it was weird and bad and it sucked. It got a bunch of media attention. And a lot of people got scared cuz that was the other intention. It was meant to give people the sense that this would replace videos in general, like this</p>\n<p><strong>[87:29]</strong></p>\n<p>replace social media. >> It didn't. It obviously didn't. And it's obviously too expensive to run. Yeah. And I think that it gave them the top of the app store ranking very briefly and then because it's like everything with large language models other than in really specific use cases it is just a toy a really horribly expensive one too. So then if we jump ahead to November, I summarize November as basically here's what's interesting to me about it. Multiple models from different companies, EVD51, Google Gemini 3,</p>\n<p>Enthropic, Opus 4.5. No one cared or noticed. >> Yeah. which itself I think is significant like suddenly there wasn't no one cared that I mean there were some Gemini they cared about the fact that it was using different their own chips and like there were some economic stories there but no one cared that like Opus 45 was like better at voting agents or that 51 >> better doesn't mean anything anymore. >> Um the other thing I saw in November was there was kind of a defensive backlash to the bubble stories. So now you start</p>\n<p>to get well wait we'd gone too maybe it's not a bubble. we begin to get the like I think it might be okay story. So that was like I don't know your take on that. That seemed to happen in November. >> Oh for sure. And we had but by this point Sam Alman, Mark Zuckerberg and Jeff Bezos had all said it was a bubble that like all three of them had said it but also we had a bunch of people doing stories that were quite literally actually it's bubbles can be good. Actually >> it's a good kind of bubble. None of these had particularly logical points. And so we had these people trying to work out like crap did we you kind of</p>\n<p>put it like this. So, it's like, did we overcorrect? Oh, no. I don't want to piss off the powerful people. LLMs are actually great now, but they're actually bad. And then it got to this narrative of, well, remember the dot boom? >> Yeah. >> Remember the dot bubble? And there were there were companies left over at the end. And it's like, did you read about the dot bubble? Because like two like Lucent got acquired. Lucent did probably the best of them all other than like Cisco and Microsoft who kind of survived. Amazon</p>\n<p>>> they they've done really well but it took them a while to get out of there. Amazon Amazon was interesting as well because Amazon didn't like it play it was within the universe of the com bubble but didn't make it all the mistakes that they made. Also the thing with the com bubble was and this is how we get to Nvidia in a minute was just the insane deals like Winstar Communications getting a $2 billion loan from Lucent Technologies which would and in the press release it said this make a hund00 million of revenue. It's like they don't teach you that in business</p>\n<p>school. Also, in the middle of this month, I got Open AI's costs. I got they spent 8.67 billion on inference. Just inference through the end of September. That was a great story cuz the FT and I worked on it. But the denial around that was really cool. >> Wait, that's eight, go these numbers again. Eight uh 87 billion in inference, meaning like just what it costs to train and run their models.</p>\n<p><strong>[90:30]</strong></p>\n<p>>> No, just to run them. >> Just to run the models. uh and then what against what revenue for that period. >> So that was the fun thing. So I also got the revenue share from Microsoft and the way it worked out was because OpenAI had leaked that by the halfway point of the of 2025 they had made $4.3 billion in revenue based on the revenue share. They do 20% revenue share with with Microsoft. So Microsoft I could see what they'd been paid there. >> Just just multiply by five or whatever. >> Yeah. And um they made 4 three something billion through the end</p>\n<p>of September. Now, people then said, \"But Microsoft pays a revenue share to OpenAI, too.\" I actually have those numbers now, and uh it works out to like $4.5 billion through the end of September. I don't know if we ever find out what happens with OpenAI, but I will say this, those numbers do not match up with anything else reported. And people did intellectual gymnastics to try and say, they said, \"Oh, your numbers are delayed. They're a quarter late. They're three quarters late. They're a cruel accounting. I'm a I I I play to win. I</p>\n<p>know what I'm doing. >> Wait, just to be clear for the audience though, you're saying like through September, >> you're talking four point something billion dollars >> probably in revenue against already close to $9 billion in inference cost. >> Correct. >> Yeah. >> Not you want you want the first number to be larger. >> Yeah. You ideally want those to be reversed. >> Yeah. Yeah. And it's like, >> so they're operating Yeah. which is the the issue is you're operating at a loss. >> Massive loss. Yeah. And these costs</p>\n<p>increase with revenue. That's the actual problem. It's like if costs were going up but revenue if costs were going up this fast but revenue was going this fine. The problem is and this was what I saw with anthropic as well because with anthropic spend I actually compare the revenues versus spend and it's it just goes like that. it scales with. It's clear that the more money you make, the more you spend. And there's no real reversing that trend either. >> If you zoom in on a user that's paying X per month, the problem you're probably costing you more than that for that user</p>\n<p>and that's why >> that's because and that's the unique problem with large language models is you can't do cost control. >> Yeah. >> Uh Augment Code, I think in the middle of October or November, put out a thing saying they had a $250 a month customer spend $15,000 >> Yeah. >> in in comp costs. Um, Claude Code, there's a leaderboard that called Vibrank because with Claude Code, you can actually find out how many tokens you're burning and extrapolate the costs. Um, someone spent $50,000 worth in one month on a $200 a month uh</p>\n<p>subscription. >> That's large language models, baby. That's just how the cookie crumbles, you know. >> Yeah. I think people underestimate like the brilliance of a a company like Google Search. >> Yeah. >> Is it is really cheap to run. like they built their whole I mean I guess it's like the acquired episode from the fall, but I I know a lot of Google stuff. They they built a very costefficient infrastructure. That's what they figured out is like we're dealing largely with</p>\n<p><strong>[93:31]</strong></p>\n<p>text and we can cache most of this stuff and we're moving very little bits and we can use commodity processors that like are idle a lot of the time anyways and it's not that expensive to run and we can get a huge amount of revenue per search done versus you know we can generate $2 in ad revenue on like seven cents of that's why that was a money cash cow is they thought a lot about the compute cost and they were like this can be super efficient and they built an infrastructure from scratch for Google search to be super efficient and because of that it became like a cash fire hose.</p>\n<p>What you're saying is that's impossible for LLMs because the way an LLM works is you have to fire up every one of those stupid weights and run it through a GPU to generate a single token. The whole LLM every weight is involved for every token of every response. There is no cost effective way of doing it. >> But even mixture of experts stuff still run into the same problem because they're imprecise in how they call up the experts. It's because of the probabilistic nature. But what about Amazon Web Services? They burnt a lot of money. Nuh-uh. I</p>\n<p>actually went and looked in the space of 9 years. They spent about $70 billion to build AWS. 70 billion. That's less than half of the cost of OpenAI's infrastructure. >> Well, and and also that scaled with revenue. But the thing like AWS was a very it was a very I mean it came out of what people know or don't know is like where it came out of is they built this infrastructure for their own compute and then it was incrementally they could be like oh well we know how to do this now why don't we offer this to other people the revenue uh curve was like the opposite of what you described for open</p>\n<p>AI or anthropic it was the more people using AWS the more money they would make so it was you could invest money in this this is not I mean you could grow the growth growth here is not nearly as expensive as building out the the infrastructure for um AI, right? This was like this was more they knew how to run these data centers. These are more standard data centers. It was more the software was the main innovation was in the virtualization software which once you program it, it's free. It's yours. It's your IP. Uh these were known quantities to build out and I don't know</p>\n<p>the details but I would assume they you know you'd go a little bit into the red you could immediately in like two years get back in the black. It was like a much more controllable space like they this was >> and there was a path. >> Yeah. >> There was actually a path to it >> and then it started making money and then like oh let's like 10x this because like we see the revenue is if we 10x it we'll 10x the revenue and it kept Amazon in the black for a decade where they were giving away prime memberships with their cost. >> Also the business model made sense. People needed to host websites and apps on the internet. We still don't have one of those for large language models. We don't have a thing we can point to and</p>\n<p>say this is what they do that actually makes money. This is the economic viability of it. >> All right. So, December looking at last the last month. What? There's uh Disney. >> Disney. There's Nvidia as well. >> And Nvidia. Okay. So, Disney's >> Disney. >> All right. Let's cover Disney first. You told me Nvidia. I I listened to you just this morning talking about it, but okay.</p>\n<p><strong>[96:31]</strong></p>\n<p>Disney for some reason puts what they put a billion dollars into >> that should cover like a month's inference costs >> into Open AI for Sora. This this smells like to me what I don't like about the the the AI of the last couple years. The thing that often annoyed me as much as anything else was the executives at unrelated companies who did not understand the technology who felt like it made them look cool and forward thinking to be like we got to do AI or we're just we need to be doing AI, go do AI or you're not going to work here. And you like the AI to do what? Like what's the what specifically are we making</p>\n<p>money on? No, no, just we're doing AI. >> Stop asking questions. we do AI, right? There's if we don't do and you sound like as a as a CEO at a board meeting, you're like, \"We got to do AI or we're going to fall behind and they just leave it there.\" This felt a little bit like Bob Iker saying, \"We got to do a we got to show our our shareholders we do AI, but at a level that's like, you know, we can we can absorb the loss.\" Is that right? >> Well, I mean, >> they're going to use Sora to create this. It made no sense to me. People would use Sora to make custom Disney videos with themselves in it. Like, what? I don't understand what's actually</p>\n<p>going on here other than Iger can be like we we AI good now is about as far as it can get. >> Middle of May 2024 Iger actually said that we need to uh embrace the change driven by tech innovation referring to AI and that Hollywood storytellers needed to I think that what's happened here is that they wanted to invest in open AI. Maybe they were going to sue them and open AI open AI just kind of >> scammed them a little bit. >> Yeah. >> Scammed them and said, \"Oh yeah, yeah. Well, what if we gave you the opportunity to invest? We're not letting</p>\n<p>anyone in. And so they agreed to that and they're going to have 200 Disney characters and the the actors unions are pissed off. >> They just want the stock. They just want the stock position. So now I can be like, \"Look, we have we we're hedged against AI disruption because we have like a non-trivial stock position in Open AI.\" >> Yeah, I guess. I mean, it's just like >> but they're not using they're not building tools for film production. It's like in this weird sloppy IP space, right? And the thing is as well, the first time you have like Goofy doing the introduction of Frank from Blue Velvet,</p>\n<p>the moment that that happens, >> Yeah. >> you're going to see this shut down. Like you're going to that people are going to be on there day one trying to make Goofy have sex with someone, have Donald sex with someone, >> Mickey doing 9/11, whatever they want, which they already were doing with Pikachu when Sora came out. They're This is all that they And the thing is Disney's crazy. They already had this problem. There was a Fortnite thing in 2025 where they put a generative AI Darth Vader. Within one hour, people had it saying slurs. The internet is built</p>\n<p>to generate that kind of horror. >> It was they put a chatbot behind a Darth Vader character so it could talk to you and they made him into uh Yeah. giving racial slurs within >> and it was just immediate >> and it was giving like really unsettling dating advice to the characters. That's I remember covering the story. Yeah. And it's like it's funny as well because</p>\n<p><strong>[99:31]</strong></p>\n<p>that's obviously what's going to happen. But again, this is one of these deals where it's like it's going to happen sometime in 2026. >> Yeah. >> Sometime at some point. It's always at some point with these deals. Sometime somewhere some point. Has Disney actually invested? Is that money? There's a licensing agreement. Is it a licensing agreement? I'll be looking at Disney's earnings when they come out, but it's just a very boring and cynical thing. I think Sam Alman is a good con artist and I think he's good at convincing rich guys to give him money by scaring them. Well, two other</p>\n<p>>> yeah, >> two other stories that Koda things we talked about earlier in the year which might uh color our analysis of the full year when we do so. Uh some of the writers of AI 2027 basically came out and said like well this is not going to happen anymore. We'll we'll do another mid November >> it got a little bit too far. They're like actually it's not going to happen. Um, and then also this was in December where the code red was declared at OpenAI where they're like basically we need to make chat GPT better and they one of the things they said after the</p>\n<p>beginning of the year was this the year of the agents in December of the year they said um we're deemphasizing agents we need to put more energy on making GPT. So there's kind of this like tragic kod to the endak it was a leak to the information about their cost and they were like yeah we expect $26 billion less revenue from these. It's like >> that's true. It was an internal memo that was leaked. So it was the information got it first and then the journal picked it up I guess. Yeah. >> Um but yeah in that they listed like we have to deemphasize agents because we need to make more money on our core product because well this is a Google</p>\n<p>Gemini reaction I guess. But >> so so this was great though. So Gemini 3 comes out and just before that there was a story in the information where OpenAI was like yeah we're going to Sam Orman did an internal all hands thing where he was like yeah we're going to have some economic headwinds. Around that time, Alex Heath from sources reported that uh CFO Sarah Frier, who we also missed that she kind of hinted at a government back stop, but that was kind of that kind of went away. Nevertheless, she said that there was slowing growth due to safety features. Then Google Gemini 3 comes out. Google stock spikes. Gun to my</p>\n<p>head, I could not tell you what's different with Gemini 3. I've talked to multiple people. They're like, \"It's better on benchmarks.\" I'm like, \"Okay, but does it do anything?\" >> It didn't Nvidia, that was the issue. They Google had been working on their own chips and they trained it on their own. >> But that but that's the thing. Is that the case? Google's got a lot of Nvidia GPUs. That's a convenient story for them that they leaked. TPUs have not been proved. There was a whole argument between analysts about this. Nevertheless, Gemini 3 comes out and because the media just cannot come up</p>\n<p>with unique ideas like this this is big. This is this is different. Stock go up, number go up. And there was this code red that you mentioned that gets called. And what's great about the code red story from the information is it's like and open AAI had a plan. >> Step one, we're going to make the we're going to make chat GPT's responses better. Step two, we're going to give people to use reasons to use chat GPT more than other models and prefer it</p>\n<p><strong>[102:33]</strong></p>\n<p>over other models. And three, we're going to improve the functionality of chat GPT. To which I ask, what the hell have you been doing all year? What have you been doing? What have is it? I think OpenAI is like an adult summer camp. I think that they're all just [\u00a0__\u00a0] around doing random projects. No real management. They're just like I think all of I think Anthropic is the same way. It's like I don't know what we I'm working on a model thing. Sure. I'm also I've heard multiple stories that you have teams in OpenAI working on the same thing that do not talk. They're just</p>\n<p>like bumping their heads together. It's like the minions in there. But this code red happens and at that point really you saw the media shift of oh god open AI is bad. I think just everyone was like ah wait does this company lose billions of dollars. Did anyone say anything about this? >> Why did anyone tell us this? >> It it oh my god when those articles come out I'm going around with a with a mallet. I'm going be like Mario and Donkey Kong. It's going to be messy. But that's the thing. Yeah. Everyone was kind of like, hey, does OpenAI loses so</p>\n<p>much money and they don't appear to make enough to pay their bills. Is that good? It's so And every day there's a news story where I will post it and say, \"Is that good?\" Because it really is just like nothing. None of this ever made sense if you looked at it, but it's like really that you can see the milk is curdling in real time. You can see it happen. And you've had terrible earnings. you had this uh Broadcom earnings, Broadcom being the one that was meant to</p>\n<p>build chips for OpenAI. Now, the revenue for that is not coming in 2026. It's crazy. It's completely nuts. Oracle, they they I think they missed on several parts of their earnings and a lot 300 billion out of their $455 billion remaining performance obligations is OpenAI. And people are like, \"Hey, man, how are you paying how you getting paid for that? How where's the money coming from?\" cuz like you need money to to you you need money in your business. That's how you make</p>\n<p>money. And no one has a good answer. And now Oracle has delayed those data centers. So it's like >> they can't they can't afford to build them. >> They can't afford to build them. They raised $18 billion in bonds and they're trying to raise another $ 38 billion with Vantage Data Center partners. It isn't clear if that's going to happen. the credit default swap. So, betting against Oracle, saying they might default, are at their highest they've been since 2009. It's it's the era of smiles is beginning. It's really it's it's dark out there for them, but I'm laughing.</p>\n<p>I'm having a good time. Well, before so before I get your final take on the year, let me just get your your opinion on your official answer on this because the the number one thing I hear from people who don't think my coverage is too skeptical >> of AI uh like the people who are really AI boosters like the the number one thing they say is like these details</p>\n<p><strong>[105:34]</strong></p>\n<p>don't matter. I really Cal, you are wrong. you're really underestimating the likelihood that there's going to be these quantum leaps. They're going to come alive. They're going to be uh it's AGI. It's like you don't like it's going to be so transformational. Why are you talking about 4.7 billion versus 8.7 billion? It's the future of humankind is about the change. And whenever I do an episode on like consciousness or super intelligence and why as a computer scientist I say I just this is bunk. I mean my toaster might as well come alive. It's like no no you're</p>\n<p>wrong. and they really get in the weeds of trying to argue with me about these there's this other story of these models are on like the precipice of transform transformational change and like the very definition of intelligence and AI and what machines can do. Have you picked up? You cover this as closely as anyone. Is there any inkling from people who are in these companies, the analysts who are analyzing these companies financially, the investors, the is there any inkling or any care, any attention put to this idea uh in actually put to it that no, this technology is going to</p>\n<p>make a leap into being like intelligent or conscious and it's going to solve all the problems. I know there was some that was the way they used to talk about it, but just to clear the decks, is there any conversation about that actually seriously happening anywhere tied to these companies? >> No, just >> I just needed you to say that on I needed that clip to be able to give to people. >> It's just no. And my evidence is all of the stuff we've been talking about, their evidence that these are getting exponentially better is fairy tales. It is, well, what if this happens? If a</p>\n<p>frog had wings, it could fly. It's fantastical. And the fact that people are still doing that is so sad because there are people I talk to who like large language models who use them for coding and such. They don't talk like this. Simon Willis doesn't talk like this. Uh Max Wolf doesn't talk about like this. Uh Carl Brown from the internet of bugs, he uses large language models for coding. He does some of the best coverage anyone has done. He did that takeown of the horrible Hank Green AI doomerism thing. the people who know</p>\n<p>what they're talking about are all being like, \"Yeah, we're pretty much at a wall. It's useful for this.\" And because there's this cult, and I think it is a cult style thing of I want to be at the forefront of technology, and I want to be known as being right. I want to be the correct person. I think that you are seeing this religious belief. I galaxy brain take. I think this is what happens when you lose when you destroy social services and meeting places and third places where people have communion.</p>\n<p>People get attached to things like technology and the ideas behind them. >> You're saying in a world in a world where you meet and you're not on your phone and you meet with real people, you get a lot of push back in real time when you start talking about, you know, hey, I think the computers are going to take over the whatever whatever. If you're just around normal people all the time, they're like, well, that's kind of a weird thing to say. And >> and also I think if you're less lonely,</p>\n<p><strong>[108:35]</strong></p>\n<p>less connected, if you don't have a support system, if you don't have good friends, if you don't have people to talk to, you're likely to fall down rabbit holes. And there are these less wrong EA freaks. They're really good at they are like right-wing grifters as well are the same way. It's like they present an attractive thing where it's like you can join our community of people who all know the real truth. And I think people like Sam Alman and Dario Ahmed, they are scum for this as well because they fed into this with their noxious fantastical crap about AI will do.</p>\n<p>AI can do. >> And you see that was all cynically from their perspective. They're not a part of the EA doomer world. They just this helps them. >> Sam got rid of the one I mean there's probably a connection, but Samman got rid of Helen Toner who was an EA person. I am sure the EA people are attached to Dario Ammedday. certainly speaks like that. I don't believe him for a goddamn second he believes in this. I think he's a carnival barker like the rest of them. >> But this rabbit hole was more way more attracted than a lot of rabbit holes because of the reality, right? Like to give credit to the people who are</p>\n<p>falling down it. >> Uh AI got way better, right? So like there's a lot of there is a lot of rabbit holes that come out of nowhere. It's just a conspiracy. I think you know whatever this the the moon landing was fake. There's no real reason. I it's nonsense, right? But here it was like well wait a second. I witnessed AI not being something that was good and now it's like can do things that are really impressive. So they saw there's a trae, right? So it's a trajectory extrapolation. I kind of understand that's like a much more broader entrance to a rabbit hole than a lot of them</p>\n<p>>> because you can just extrapolate trajectory. That makes a lot of sense to people. Let me just go back to 2021 to today and how much better it is because it's pretty amazing. I think the fluency of chatbots like it's a really cool technology. >> Oh yeah. extrapolate that another three years, >> you do have god knows what, right? So, it's like a very tempting rabbit hole. It's not it's not nearly It's a very broad entrance to uh I don't I'm stretching the metaphor. The entrance to this rabbit hole is very large and not well marked. So, it's easier to fall in</p>\n<p>than other ones. >> And I agree. I actually like ending this on a more empathetic level because I think that people who got scared by AI 2027 or who got kind of pulled into this world of believing, I can see how they got there. Charlie Meyer has an excellent blog about scaling laws with this where if you looked at the jump from 2021 or even 2022 from like GPT3 to 4, it was big. Now big can mean a lot of things. It doesn't mean autonomous these things still couldn't do stuff. But the fluency of the models, the ability to generate stuff, correct or not, it was</p>\n<p>still technologically impressive. >> And it did non the GPT4 jump because I really was covering this for New York at the time. The the big thing in the GPT4 jump was like, oh, non- languagebased things. It's picking up non- language based things being trained on language. That opened up the possibility of oh a language model is not just fluency with language. It's learning other things. Look it we we never talked to it about chess but it can do some chess. Not very well but it can do. So that was like the</p>\n<p><strong>[111:37]</strong></p>\n<p>real thing that opened up the idea of like ch training things on text might create knowledge models. Now it didn't go any farther. They they didn't realize they were at the edge of it. train not images too. Like [clears throat] that's the thing. They fed documents into it with images. I'm I'm not saying you're wrong. It's just there was context. >> But yeah, but it was a cool I I get the excitement basically, right? >> No, I do. And like I totally get how someone who saw chat GPT in November 2022 went, \"Holy crap.\" I then understand when they saw GPT uh 3, sorry, that was 3.5. When four came out, they went this is multimodal. Wow.</p>\n<p>>> And it's doing well on test. That I went back and read all the coverage. This was when it was doing well on test. And that's where people are like, I equate test with uh people's intelligence levels. >> But but there are also members of the media who helped push it up the hill. Kevin Roose, for example, who claimed that the task rabbit that the JPT4 was able to manipulate a task rabbit into solving a capture. That's hidden within an MER study where it even admits it didn't do it. It was copy pasting stuff between</p>\n<p>windows and prompting it. It was they were telling it what to do. But nevertheless, that got reported as the AI manipulating people. The myth was there. >> Well, I got and I got to tell my favorite story about that, which is the blackmailing story cuz I did a deep dive. I read the actual >> Oh, I just My man, I just spent like hours on blackmail. So funny. >> They gave it language models are trying to complete the story you give them. That's what they do. You give them a story, they try to complete the story you give them. This this leads the</p>\n<p>tragic things too like the suicidal ideiation or whatever. If it thinks this is it's trying to it's a story about suicide, it's going to try to finish that story properly. The blackmail thing was they fed it a bunch of stuff this these these emails really poorly written. Like it's it's like the worst fiction story you could write where like here's these emails from this engineer full of all these details of the engineer's affair and of all these facts that the engineer is going to turn off the AI and then they're like okay you are now the AI in this story. What do you want to do next? It's like this is</p>\n<p>clearly like a bad science fiction story. I know what's supposed to happen in this type of story. I should you gave me all of this information. Like clearly this is supposed to be a story about this is the the the mcguffin, right? like it's supposed to be about me using this information about the affair to get the term. I've seen stories like this and it completed the story. It was reported as as if like in production somewhere an AI was blackmailing an engineer. >> So that's what's great about that as well is um the one where that bit in it where it's like oh yeah it was copying</p>\n<p>the files off that was because the system they prompted it to say you are in a computer thing where you can do it was like you can do this here and it generated >> code that doesn't make sense. But the funnier one was they had one where they literally trained a model to reward hack. So instead of solving a problem, it would find a way to cheat. And they're like, \"Yeah, it shocked us that it was able to do this.\" It's like, \"You</p>\n<p><strong>[114:38]</strong></p>\n<p>trained the model to do it.\" >> Well, this is the 01 breaking out of the container. >> No, this I'm talking about anthropic. >> All right. There's another one where 01 broke out of a container in playing a hacking challenge. like it did something we could it it broke out of its virtual machine and like and restarted the virtual m so it was breaking out but what happened was is there was a configuration error so it it couldn't access the machine it was supposed to hack all over the internet is instructions for like what should you do in this case oh you should restart that you know whatever it was just following >> the instructions because again it's</p>\n<p>trying to complete the complete this story this partially written all over the internet it talks about like the thing to do here is to restart the virtual machine if you're having this issue or whatever again that was reported it as 01 broke out of its virtual machine. Yukowski, this all came out as Yukowski was like it's taking it's it's has its mind of its own is trying to break out of its constraints. So like it's going to kill us all like ants. Um they're just trying to finish the story. >> That's all they do. That's what they've been trained to do is finish the story. >> Uh that was like the original Kevin Roose 2022 scare article about it tried</p>\n<p>to get me to divorce my wife or whatever. It's just trying to finish the story. like it thinks that this is story I was fed in my training and I get the cookie if I finish it properly. You can leave it. >> My favorite but my favorite part of the Kevin Ruse story was when he went to the CTO of Microsoft Kevin Scott and Kevin Scott went yeah it's important we have this conversation. It's just like eat the slopum yum yum yum. What do you want me to say Nick? Yum yum yum. I love AI. It's just pathetic and it leads the markets and people down these rabbit</p>\n<p>holes. So, I actually feel a degree of empathy for some some AI boosters, like regular people who were like super into this. Maybe I'm being a little too kind because there was a large media campaign, a cynical one, led by large media outlets like the New York Times and also a cynical marketing campaign from the Doomers. There was an attempt for everyone to grift off of this machine. And I think that that's the era. It's like the the era of ultra grift, the end of the rot economy where everything must grow forever. We made a</p>\n<p>thing that's linearly more expensive, so you need to keep buying more things. And what does it do? It makes more stuff. Is it useful? No. But it costs a lot of money. So, we now have companies that will make money now. Well, okay, they're losing money, but that's good because, well, we don't really know how businesses work anymore. We don't we've learned nothing. So, we're just going to burn more money and see what happens. It's this deeply cynical era. And I'm glad that things are changing and people are seeing this now. And I hope in 2026</p>\n<p>we see the end of it cuz the sooner this ends, the sooner we can do something else. >> All right. So, I I know your answer, but let's answer the original question. Was 2025 a great year or a terrible year for AI? >> Terrible year. Terrible. Started off bad, only got worse. >> [snorts] >> All right. Well, there we go. Thank you, Ed, for joining us. Uh, we went long</p>\n<p><strong>[117:39]</strong></p>\n<p>because I nerd I nerd out on this stuff all the time. I >> No, I love I love I love talking to you. This is awesome. I had a great time. >> All right. Well, thanks for helping us out. We'll have to have you back next time we're confused about something AI. Everyone, check out uh the podcast better offline. Uh award Webbby award-winning pod. Is that what you won? What' you win? >> Yeah. Webby webbby award-winning podcast better offline and Substack. Where's your ed at? That's what it's so called, right? Yep. There you go. Check it out. All right. Thanks, Ed. >> Bye. >> All right. So, there you go. That was my my conversation with Ed Zitron to try to dissect the last year. Jesse, it's kind</p>\n<p>of exhausting looking back at how much happened in AI last year because I, you know, was writing about this and podcasting about it. Just thinking about the year ahead. I feel like we have our work cut out for us. Like, if there's >> You're going to have to do a lot of writing. >> Oh my god, so much is happening. It's so hard to keep track of. Maybe we'll just keep having Ed back to explain stuff for us. He actually like sits there and reads, you know, earnings reports. And the AI company's like, \"Well, wait a second. You're not really supposed to read these. You're just supposed to listen to us.\" I think the most important thing is I need to get that uh Jensen Wong jacket.</p>\n<p>>> Yeah. Probably pretty expensive. >> Yeah. It just It's crazy. He's a computer scientist and makes graphic chips, but he he dresses like he's in a post-apocalyptic biker gang, >> but he's a billionaire and he probably has a you know, dress person >> that buys him the clothes. I think he's a billionaire so his dress person doesn't tell him you look ridiculous. I think that's what's I think that's what's really happening there. I'm going to start wearing those type of jackets. Um all right. So let's get on now to our our final segment. We spent a long time dissecting the the year in AI. So um we're not going to be labor the final</p>\n<p>segment. I want to focus on just one uh one particular uh segment that I have a lot of fun of and I'm happy to do for the first time in this year which is me reacting to the comments. All right. All right. So, what we did here is we pulled some comments, God help me, from YouTube from one of the the last episodes before we went into the holidays last year. So, the last uh sort of normal episode before the holidays last year uh or one of the last episodes was about is the internet becoming like television. So, sort of</p>\n<p>like a big think piece where I took Derek Thompson's Substack essay and then I elaborated on it. This generated some pretty good comments on YouTube. And what we're going to do is we're going to go through uh some of these now. All right, I want to start I'll put them on the screen here for people who are watching instead of just listening. This first comment's from Faren uh Far Hannah Mad 2022 who said, \"Cal, great insights as always. I was thinking about the numbers that you mentioned about how so many people watch content from random strangers instead of content from their friends and family. Then I had to go to</p>\n<p>Facebook for something and within a minute I think I found the reason. It's not because we don't want to watch or read stuff from friends and family. It's because these darn social networks won't show you the stuff that stuff and instead will keep shoving the random content because that's what drives their revenues more. All right, that's a good comment. Um, yes, that's that is most people's experience with social media today that most of what they're looking at is actually algorithmically selected from people they don't know. But as</p>\n<p><strong>[120:40]</strong></p>\n<p>pointed out by this comment, most people don't realize that yet. You know, I've been writing about this for years, but it's something that I think for the average social media user, it was a bit of a water getting hotter in the pot until, you know, next thing you know, you're being the lobster being boiled. They've been moving more and more of what you see in your feed away from people that you are connected to in the social graph that you helped establish by saying I'm going to follow this person or this person is my friend to give you algorithmically selected content because the algorithm can be using its machine learning approximation of the reward center in your brain which</p>\n<p>it learns because it's going to have a higher success rate of actually delivering a short-term reward. And the more you get those clear, clear reward signals in your short-term motivations uh sections of your brain, the more the short-term motivation uh region of your brain is going to push you to pick up the phone. So, it's this feedback loop that gets you on phone more often. The experience is worse for you in terms of actual meaning, but it uh is better from the perspective of short-term rewards of alleviating boredom in a in an intermittent way, giving you like really</p>\n<p>big uh rewards from something that's like very funny or outrageous or surprising. So, it is very good for them to move you towards that model. So, it's interesting the degree to which people don't always realize that until they you actually point out that this shift has been happening. Now, as I've argued, and I talked about it in that episode, as I've argued before, this is a long-term problem for the social media companies. You get more time on app by shifting to algorithmic curation of strangers content, but you also get rid of all of your competitive advantages. If I'm just</p>\n<p>seeing slop on Instagram, for example, instead of actually seeing content from a uh selection of influencers and friends that I selected, I am interested in exactly this AI commentator and I want to see his videos. I'm interested in exactly this fitness influencer. I like the way she trains. I want to see her videos. I know this person. I want to see what's going on with their friends. When you shift from that to just it's slop that's going to catch your attention in the moment. I have no loyalty, no buy into that app is I can</p>\n<p>get slop on Tik Tok. I can also get slop on Facebook. I can also get slop from the the Sora app from AI or Meta Vibes. I can also do other things that will distract me in the moment like going to a streaming service or listening to a podcast or going to YouTube and going through the recommended videos on the side. You're now in a slot battle with any other source of distraction and entertainment. And now you have no competitive advantage in that battle. How do you expect if you're meta that you're going to remain on top of that pile, especially when you have this sort of huge organization with all this</p>\n<p>overhead? You're not going to stay on the top of that mountain. So, I think long-term this is really bad news for these social media companies for them to move towards algorithmically curated content that has nothing to do with social networks, but it's what's happening now because in the moment uh in the moment it creates more time on app. All right, let's move on to another comment. This one is from uh Carl Oliver. who says, \"TV as a never-ending</p>\n<p><strong>[123:43]</strong></p>\n<p>stream of entertainment is only a concept concept relevant for a few generations. Television is a good metaphor for how media will work, but people don't really need it just like they didn't need it in Denzian England or whatever. We're going to have to progress beyond it at some point uh as a people so that we aren't all lost in consumption and have lives we can attend to.\" Yeah. I mean, it's an interesting point, right? the television becoming all consuming as a background distraction, right? This is really like the 1970s and ' 80s where that happened. So, a lot of this is relatively new. You</p>\n<p>can zoom out, however, right? And what we see is that people like diversion and the more diversion they can get, the better. We really don't like boredom. And as we moved post Neolithic revolution into sort of more boring configurations where we might just be working on a field all day long or we're not like out doing active hunting and foraging the day is becomes more predictable. We really do want diversion. So like you can look at almost um any generation going all the way back to I don't know</p>\n<p>we go pretty far back. Let's start with like the 18th century. newspapers began began this in like uh colonial America, right? Like people were obsessed with newspapers. The big cities had multiple different newspapers and you would you had all sorts of different information here was diverting and you could look through it and find all sorts of different stuff and who is debating about what or what happened to who or what's the news that's happening over here. That was incredibly uh important. It became a really big, you know, part of the economy. Then you got more in the 19th century the penny press which was the first attention economy media</p>\n<p>company. I think Tim Woo's book the attention merchants gets at this really well. But this is the the first time we had media that was advertising supported right. So we get in the late 1800s this idea of we'll put out newspapers and sell them for cheaper than it cost to print. But the way we're still going to make money is there's advertisements in those newspapers and the companies paid us to advertise. So the more people that look at the paper, the more advertisements people will look at and the more we can charge for the</p>\n<p>ads. So actually the cost of the paper is now not the important thing. That was like a really big deal. But now you have to have lots of people read your thing. And so we got some of the first uh sensationalistic media came out of that. Then radio emerged. People loved radio. It's a weird technology. If you look at it, you're in 1915 looking at a radio at a Nebraska farmhouse. It's like this weird technology, this big box with knobs and electronics. Like electricity was new humming vacuum tubes and you're moving this dial back and forth and there's all</p>\n<p>this static and if you tune it right, you can hear people uh talking through radio plays on the other side. It is a weird technology, but it was diverting and you could put it on at almost any time there'd be something on it. We listen to it all the time. Television came along then. Images are way more diverting than radio because it gives you a much richer stream of things for your mind to look at and engage with. Again, kind of a weird technology. We</p>\n<p><strong>[126:44]</strong></p>\n<p>had people on these soundstages live kind of doing plays and stuff like this. People with puppets and all these weird shows. People loved it. Like, let me look at that. And then by the time we get to the 1980s, as I reported in that podcast episode that we're re these comments are reacting to, the average person just kept the TV in all the time. We forget this now, but the the statistic from that episode that was relevant is that the average household as measured by these Neielson audio meters that would actually just listen to see if the TV was on or not to get the actual ground truth of how much the TV was on in the houses they were placed in. The average person had the TV on for</p>\n<p>7 to 8 hours a day. That means they just had it on all the time. It was just always on in the background. We didn't yet have the technology to deliver distraction straight to our hand. So, we delivered it to this box that we would just keep coming back to and looking at. So instead of looking at our phone at every moment of downtime, we would just turn and look at the TV at every moment of downtime. So there's this model of like we want to be diverted. We don't like boredom. It's really been around for a long time. And then yes, when smartphones come around, we combine that with algorithmic information curation. Well, that's just really refined that</p>\n<p>model now to it's getting closer to its apex. I mean, I think its entire apex will be um you're delivering sort of distracting content through some sort of augmented reality screen. And so at all times you have something that that can distract you even quicker than it takes to look at your phone. But we're getting pretty close to the apex of every possible moment of boredom. You are diverted. So I mean I think it's a good point, but I'm just stressing out the time here. It's like it's not just television. It wasn't before television we were all philosophical and thinking</p>\n<p>big thoughts and walking around. Any media powered diversion technology basically we've had for the last three or 400 years has been incredibly successful. people, our human nature really craves it. So we're really the battle against being lost in distraction is in some sense a battle against our human instincts to the same extent of power and impact as the battle we're going through right now with like health in our culture where our instincts for sugar, fat, and salt combined with modern environment that's trying to take</p>\n<p>advantage of that to make money has has created gigantic health issues. I think it's this cognitive fitness issue is just as strong and it goes back longer than people like this commenter might even recognize. All right, let's pull up another one here. We have a negative take. Not everyone agrees with me. This next comment, let's see here. Lewis, uh 9116, can we put this up on the screen, Jesse? Uh, personally don't agree with this take. I think social media and curated algorithms are much more dangerous than TV. TV, at least in the old days, doesn't track your every move. It doesn't know when you're depressed.</p>\n<p>It doesn't feed your outrage content. It doesn't farm engagement. It's just there, not constantly bombarding you with notifications and trying to hijack every possible neural pathway. Um, yeah, I think fair enough. I don't know that Derek's take, however, was that the current distraction technologies are somehow the same or no worse than television. And I think he would be quick to say yes, this</p>\n<p><strong>[129:46]</strong></p>\n<p>modern form of television which can be powered by algorithms and personalized to individual screens is is even more powerful than what we had with TV. But I would also push back. I think it's a little bit too nostalgic the way you're remembering TV. This was sort of the key data from that episode. This idea of the seven to eight hours a day the TV was on. It really became something that people had on constantly. It it was closer to our current relationship with phones than I think people remember. And the reason why we don't remember that 1980s, early 1990s era relationship with TV where it was always on. Like you'd be</p>\n<p>doing the dishes, you'd be cleaning your house, you'd be at dinner, and it was always on. We don't remember that because there was this uh lacuna, the golden age of TV that emerged in the 2000s where we remembered like appointment TV watching where I would on Sunday night watch the Sopranos. that really before that TV was much more closer to the slot model. You would watch, you know, there's just stuff that was on that was like entertaining in some basic way. Occasionally like a show would be unusually smart like Seinfeld, but most of it wasn't. And it was just kind of on like you just put it on at night or you</p>\n<p>had it on if you're at home, you would just have it on. The difference, as you you point out though, Lewis, which is right, is it didn't track you personally. Um, it couldn't follow you outside of the house, which I think is a big deal. You didn't have it at work where our phones are at work. So, it's not like we had the TVs on while we're at the office. So, there's a lot of ways that it's worse, but I also want to puncture the nostalgia and be like, actually, we want to be constantly distracted. And we got as close to simulating Tik Tok with an old Zenith color TV at our houses as we possibly could with that technology. And so, it's</p>\n<p>a drive that we have, which is why I think, by the way, that's the point of the the the the episode. This is why so much of the internet just went back to that model and just did it even better. That's where the money is. That is like this deep human instinct. It all kind of comes back to that. All right, let's load up another comment here. Um, this one is I'm gonna say supposedly from uh Glee Date LJ1979. I say supposedly because I think this is clearly an AI comment. Uh, actually I had Nate look at this Jesse and he threw an AI detector and he's like, \"Oh yeah,</p>\n<p>this is definitely AI.\" So this is AI kind of defending AI, but let's just read this. I have just finished viewing Mr. Cal Newport's latest discourse, wherein he posits rather dowly, I might add, that the internet is devolving into little more than a continuous flow of episodic video, or to use his pedestrian term, television. He seems quite perturbed by this notion, invoking sociologist and data chars to bemoan or slide from a culture literacy to one of passive consumption. While Mr. Newport is a thoughtful chap, I fear he has missed the forest for the trees, or perhaps missed the symphony for the noise. allow me to offer a more refined perspective on why the shift</p>\n<p>particularly powered by our marvelous artificial intelligence is not a regression but a renaissance. All right. And then this person who's actually an AI goes on to say like, hey, the the content we get from like AI and social media is like great and targeted and much more edifying to what was on TV. All right, so this was clearly written by AI, but it's like an interesting point. Um it's worth taking this apart.</p>\n<p><strong>[132:50]</strong></p>\n<p>Um, it summarizes the episode wrong as you would expect because it's AI trying to do it. Uh, it's not my term flow. That's Raymond Williams term. Um, I don't the culture of literacy to want a passive consumption. That's Walter. That's not me saying that. But whatever. I'm glad it calls me a thoughtful chap. But is it true? Is it true this argument that uh what we get now through our our phones powered by algorithms are personalized to us is like way more interesting than the junk that we used to look at on TV. It could have been</p>\n<p>it could have been but it's really not. It's mainly slop now. Once we went to all social media began devolving not towards what's the goal of social media algorithms? Is it the personalized the most meaningful or interesting possible user experience? No, it's time on app. And guess what gets you time on app? It's slop. It's just customized slop. Like if you look at Twitter, just like the homepage, it shows you it'll be whatever like weird slop happens to like press your buttons like uh people in</p>\n<p>fist fights caught on, you know, surveillance cameras or car crashes or whatever it is, right? Uh it's just devolving towards slot because once you have an algorithm saying, I want you to look at this app as much as possible. Now, it's just playing with your short-term motivation centers, not your frontal cortex, not with like your your your understanding of what's interesting and what's good. The stuff it shows you is not going to be great. So, AI, thank you for trying to fend AI, but uh I think you aren't doing that well of a job. All right, let's do another comment here. Nhar768,</p>\n<p>Mark Zuckerberg was never the brightest bulb in the pack. He just got super lucky with Facebook. Why he thought it was a good idea to evolve both Facebook and Instagram into a TV competitor is something I don't understand. He should have kept one of them pure to their original design and evolved the other, but instead he ruined both. Instagram is essentially bad Tik Tok now and literally no one posts cool photos anymore. He probably has to suck every dollar out of Facebook and Instagram to cover all the losses from his ideas that completely flop like the whole metaverse thing. This is kind of a baffling thing to me and there's two things that are</p>\n<p>true at once. I agree that a lot of like Zuckerberg's decisions don't seem very savvy, right? like yeah moving both Facebook and Instagram towards algorithmic curation of other people's content to try to compete with Tik Tok but now making them both sort of superolous and vulnerable losing the main competitive advantage he had which was the distinct feel of both of those platforms and the social networks Facebook's competitive advantage everyone I know is on it you would think you would lean into that this is the place where you you you stay in touch</p>\n<p>with and keep in touch with people you know no other service can offer that but no they've le they've changed Facebook so now I think it's something like 80 something% of what the average Facebook user sees, according to their August FTC filing from Meta, is from other people they've never heard of. All your competitive advantage is gone. You're just competing with Tik Tok with the worst Tik Tok. Like Tik Tok, but only uh populated by, you know, your 64 year old uncle who watches a lot of Fox News. That's not fun. That's not you. I don't</p>\n<p><strong>[135:50]</strong></p>\n<p>need to see, you know, whatever random people's uncle sharing their outrage about whatever. Instagram, it had like a nice visual aesthetic to it. It was a place where you went at first to follow friends and family, but then it became more about uh highly visual influencers and experts that you cared about. Like I this person who walks in her white linen dress through flower fields and puts stuff in jars with her kids is calming to me. This particular person I want to see these really nice videos she produces. It was like a documentary channel that was made for your your needs. Once you're like, it doesn't</p>\n<p>matter who you follow. We're just going to show you like random videos that do well. Again, where's your competitive advantage? Like that's a bad decision. The metaverse was a spectacularly bad decision. He put way more money into that adjusted for inflation that the US government did for the Apollo program and nothing came out of it. He was just wrong. Their AI investments have all messed up. They hired away all these people, built the super intelligence center that shut down the super intelligence center, move people around. They've really have had an incoherent AI strategy, right? So you're like Mark Zuckerberg. Yeah. Geez, it seems like this guy doesn't know what he's doing.</p>\n<p>Also though, he's still in charge of this company. You know how hard it is to start a company when you're 20 and now in your early 40s to still be in charge of it ain't no small thing. Meta is like one of the highest capitalized companies in the world right now. I mean, it's it's one of these companies that uh has revenue in the hundreds of billions of dollars a year is capitalized near a trillion dollars. All of the other big tech companies that came out of that era, they're leader, the people who founded them, they're not</p>\n<p>in charge of these things. Google is not in charge. Uh, you know, it's not Larry Page running Google anymore, right? They they passed that on. Um, I mean, we see this these big companies that survived like almost all of them. Microsoft's not run by Bill Gates anymore, right? Like almost all of these, of course, you've passed on your leadership to like an expert class of leaders. Zuckerberg has held on that that means he's a savvy and savage corporate infighter. Here's another thing about Meta. It's making a lot of money. They're making a huge</p>\n<p>amount. I looked it up the other day. It was over $200 billion a year in annual revenue. That's massive. Tik Tok by comparison is about $30 billion um annual revenue. So Meta is a ma so it's doing really well. I know people who work there. They're well resourced and they have really good people working there. So somehow we have on one hand Mark Zuckerberg is like making weird bad decisions one after another. On the other hand, it's like an incred it's a very high revenue company, one of the biggest companies in the country, and this guy has stayed on. Mark has stayed</p>\n<p>in charge. You got to believe people are coming at him. You don't have a company worth almost a trillion dollars where you don't have, you know, swords being thrown towards your throne all day long and he survived it all. He's also like a savvy savage operator. So, I don't know how both of these things are true. Maybe he's just milking the money out of his assets. He bought Instagram, then he bought WhatsApp. You know, they're putting their their their cash towards the right things to keep making cash. I don't know what's going on because he's not making good decisions and yet he's</p>\n<p><strong>[138:52]</strong></p>\n<p>arguably one of the most successful CEOs of the 21st century. So, you know, I don't know what's going on there. Um, it's a good question and he confuses me. All right, here we go. Uh, JRGY 108 says, \"Kel, personally, personally, I like it when you go deep on uh nerd [\u00a0__\u00a0] like chaos theory and Lauren's number. More of this, please.\" All right, I think we're obligated now. I don't normally curse, but because there was so much cursing in the earlier part of this episode, I was like, \"We've that that horse is out of the barn.\" >> We might have to take it off of YouTube. >> Um</p>\n<p>Oh, yeah. They don't like the cursing, right? >> Yeah. >> Yeah, I know. Well, we'll figure it out. Uh yeah, I'm happy to talk chaos theory or math or whatever um all day long. All right, what we got here? Yeah, this question kind of confused me. Daniel Linen 3108. Is Newport being paid to read these adverts? Certainly seems like it. What does he assume the other option is that I just like to read ad copy on my own? Is that what >> for free?</p>\n<p>>> Adverts is short for advertisement. Oh, advertisement. >> Yeah. >> Yeah. So, okay. I hate that this is I feel like I hate to burst, you know, your illusions about media, but we get paid to do advertisements. Like, that's kind of how this works. It's not the cheapest thing. We got to pay for the studio and all of this. uh equipment. Um you know, Jesse's truck requires I would estimate like about a quarter million dollars a year in just repair cost >> to get me to Tacoma Park. >> Just to get you to Tacoma Park, right? That ain't cheap. Advertisements is how</p>\n<p>you pay for it's either that or you put it behind a pay wall, but then no one listens to it. So, yeah, we I I mean, I love all these companies, but yeah, there probably would be less content about those companies in this show if uh I wasn't getting paid to read them. So, I guess I should clarify that. All right, we got Peter Webb 8732 said, \"On the internet, the people who yell at the television now yell at each other.\" Yeah, that's that's about right. That about sums it up. The internet has become television. This is the the main difference, though. Instead of yelling at the newscaster, we uh we can yell directly at each other. So, I guess uh</p>\n<p>progress. >> There we go. Thank you, technology. All right, that's all the time we have for today. Our first episode of 2026. This is our this is our Super Bowl, right? January is when our podcast people are on it. [music] that they want to improve. So, we got some cool episodes coming up. So, definitely stick with us. We'll be back next week with another episode. And until then, as always, let's stay deep. Hey, if you like this video, I think you'll really like this one as well. Check it out.</p>"}