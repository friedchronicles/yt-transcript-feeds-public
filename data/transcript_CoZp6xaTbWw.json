{"video_id": "CoZp6xaTbWw", "title": "[State of MechInterp] SAEs in Production, Circuit Tracing, AI4Science, \"Pragmatic\" Interp \u2014 Goodfire", "link": "https://www.youtube.com/watch?v=CoZp6xaTbWw", "published": "2025-12-31T19:50:01+00:00", "summary": "From PhD research on grounding and language models to shipping interpretability tools in production at *Goodfire,* *Jack Merullo* and *Mark Bissell* are building the infrastructure to crack open the black box\u2014making models not just powerful, but *understandable, steerable, and safe.* We caught up with them live at *NeurIPS* to dig into the state of mechanistic interpretability heading into 2026: why interpretability is no longer just a research curiosity but a *practical tool for deployment in high-stakes industries* (healthcare, finance, life sciences), how *Goodfire's platform* turns unsupervised feature discovery into real-world applications like *paint.goodfire.ai* (painting directly into Stable Diffusion's mental map) and *PII detection for Rakuten* (500x cheaper than GPT-5 as a judge, higher recall than LLM-based methods), why the *memorization vs. reasoning spectrum* matters more than binary \"memorized or not\" framing (factual recall sits somewhere between rote memorization and logical reasoning), how *cross-layer transcoders and circuit tracing* are scaling interpretability across every layer of a model (creating attribution graphs through interpretable features), why *Neil Nanda's pivot to pragmatic interpretability* isn't a retreat but a validation that interp is ready for real-world impact (managing by outcome, not just reverse-engineering from scratch), the *Pasteur's Quadrant* philosophy that drives Goodfire (bouncing between foundational research and applied use cases, like Pasteur pioneering germ theory _and_ vaccines), and their thesis that *interpretability unlocks latent capacity in models* that you simply can't access by treating them as black boxes\u2014whether that's finding novel biomarkers of disease in genomics models, scrubbing PII from customer chats, or giving creative tools direct access to a diffusion model's internal concept map.\nWe discuss:\n\n* Jack's path: *PhD student (2020\u20132025) working on grounding in language models \u2192 full pivot to interpretability research \u2192 Goodfire*\n* Mark's path: *Palantir healthcare engineer \u2192 Goodfire in March 2024,* focused on applied interpretability and platform engineering\n* What Goodfire does: *AI research company building a platform for interpreting models across modalities and domains,* focused on real-world deployment in high-stakes industries\n* *paint.goodfire.ai:* using interpretability to paint directly into Stable Diffusion's mental map\u2014unsupervised concept discovery (animals, backgrounds, scenes) lets you select and paint concepts on a 2D canvas, a totally new interface for a text-only model\n* Fact editing and the *ROME paper* (rank-one model editing): updating facts (\"capital of France is now Marseille\") is an interpretability problem, not yet deployed at scale\n* What's new in 2025: *interpretability showing up in model cards, evals, red teaming exercises* (Gemini, Claude), and real production deployments like Rakuten's PII detection\n* AI for science: *narrowly superhuman models in genomics, medical imaging, proteomics, materials science*\u2014interpretability unlocks novel biomarkers of disease and scientific discovery in domains humans can't natively understand (base pairs in, base pairs out)\n* *Cross-layer transcoders and circuit tracing:* scaling SAEs across every layer, tying features across layers, and creating attribution graphs to trace how models produce outputs through interpretable primitives\n* Why *Neel Nanda's pivot to pragmatic interpretability* isn't \"interpretability is dead\" but a validation that interp is ready for real-world impact\u2014managing by outcome, not just reverse-engineering models from scratch\n* *Pasteur's Quadrant:* bouncing between foundational research (Niels Bohr understanding the atom) and applied research (Edison inventing the light bulb), with Pasteur as the model for doing both (germ theory _and_ vaccines)\n\n\u2014\nGoodfire Team\n\n* Goodfire: https://goodfire.ai\n* Paint demo: https://paint.goodfire.ai\n* Careers: https://goodfire.ai/careers\n\n00:00:00 Introduction: GoodFire's Mission and the State of Mechanistic Interpretability\n00:00:56 From Grounding to Interpretability: Jack's PhD Journey\n00:03:04 Paint.GoodFire.ai: Interpretability for Creative Control\n00:05:30 Disentangling Memorization from Reasoning: The Spectrum of Model Capabilities\n00:06:40 Unlearning vs Suppression: The Challenge of Removing Information\n00:08:15 Real-World Deployments: PII Detection and Enterprise Applications\n00:10:20 AI for Science: Genomics, Proteomics, and Novel Biomarker Discovery\n00:11:20 Circuit Tracing and Cross-Layer Transcoders: Anthropic's Contribution\n00:15:44 The Neil Nanda Pivot: Pragmatic Interpretability and Managing by Outcome\n00:18:37 Pasteur's Quadrant: Balancing Discovery and Application at GoodFire", "transcript_html": "<p><strong>[00:00]</strong></p>\n<p>[music] Light space to fire up. [music] >> Okay, we are here live hive at Europe's with two good folks. We want to cover the state of Mechinf basically and you guys are very passionate. Mark, I had you on for AIE before. Welcome back. >> Thank you. >> Uh and Jack, you're new but uh also part of Goodfire. Yeah. >> Uh how how would you describe uh what do you guys do and your path into Mecher? Maybe Jack, you want to go first?</p>\n<p>>> So we do interpretability research primarily. Uh we're a company focused on like making models interpretable and and robust and safe. And I guess I would describe like the type of work we do as like the science of deep learning which is trying to make models not just black boxes like that we things that we can actually trust and like deploy in in high stakes uh industries. I guess my own path into interpretability um was I was a PhD student um from 2020 to 2025. I graduated in May. Uh and I started</p>\n<p>working on I was working on language models. I was working on grounding in language models which is basically the idea that you need more than text data to represent meaning in the world. And it was basically right away as I started like at some point, you know, GPD3 had come out and I had a moment of like, oh, this is actually like, you know, just really good at like understanding the world and like it was kind of a slow transition, but somewhere along the way in grad school, I switched to fully doing interpretability.</p>\n<p>>> Cool. Yeah. As like Jack mentioned, Goodfire um is an AI research company uh focused on building a platform for interpreting models of all kinds across lots of different modalities and domains. My path looks very different uh than Jack. So prior to Goodfire, I was at Palunteer as an engineer on our healthcare team and then joined Goodfire back in March. And I guess one cool thing about kind of the the state of interpretability as a field and also how Goodfire is set up um is there's a lot</p>\n<p>of foundational research still to be done that folks like Jack are working on and our team. But at GoodFire, I'm more focused on sort of like our applied real world use cases for for interp. building out a platform that can help in use cases like scientific discovery or for kind of like inference time monitoring of models um for deployment in enterprises uh and things like that. So I think there's a lot of exciting just totally new theoretical research to be</p>\n<p>done. But uh especially over the past year I think we're starting to see interpretability have practical use cases be actually deployed in like especially situations where models are being used for high stakes industries um and problems which is which is super exciting. >> Yeah. I I think a lot of people are ignorance of the fact that we're actually at this point where people</p>\n<p><strong>[03:01]</strong></p>\n<p>could actually apply these things for for real life use cases. I saw the platform directly. I you guys had like a launch party in your office for the diffusion thing. Maybe you want to recap what that was so that people can go play around with it again. >> Yeah. Yeah. So, this is uh kind of a a research preview that we put out. It's at paint.goodfire.ai. It's live. That was a use case of interpretability for um sort of like the creative domain. I think just giving one hint at like interpretability gives you a set of I think of it almost as like power user tools for accessing models and doing things with them that you</p>\n<p>might not have realized you could. Uh so that example is you take stable diffusion XL turbo. So a model that typically you use text input to prompt the model and you get an image out. But using interpretability techniques you can sort of like plug directly into the mind of the model and you get a 2D canvas where you can basically like paint directly into its mental map of the image. And so we used unsupervised techniques to basically figure out all the concepts that the model like has</p>\n<p>internally. So animals, backgrounds, um, scenes and stuff like that. And so you can select any of those concepts, which again we're found totally unsupervised, and you can just like paint a lion over here and then drag it and move it over here. It's just a totally new way. Like this is a model that only takes text prompts as input, but when you plug into the brain, you can do these cool things. >> Yeah. and then highlighting other work because I think people that one went relatively viral. Uh Jack, I don't know if you want to take a turn at like other highlights of your year in terms of stuff that you shared.</p>\n<p>>> You start with the memorization. >> Okay. Sure. So there's a lot of work and like going back years on like so models memorize a lot of their training data. That's a privacy concern, but it's also I think just scientifically under interesting to understand and there's sort of like an unclear picture of like how should we think about how it's represented like within a model when it's is is it like a a computer with like like a file system that we can kind of tap into where maybe files are redundantly stored like you know memorized training sequence are kind of spread throughout or should we think</p>\n<p>about like some other way and we kind of proposed a a kind of like different lens on it. I think a exciting question around the work was um whether we can like disentangle like core cognitive capacities in a language model from >> from knowledge right >> from knowledge >> I was thinking like GPQ QA0 >> and uh you know if 100 >> exactly >> right so this is a paper we put out recently in October um and uh we showed u there's this like nice spectrum of like capabilities in so there's another</p>\n<p>I think another new lens on on memorization which is that in language models it's not so black and white like what is memorized and not so like memorizing that the the capital of France is Paris is you know memorized in some sense but it's it's a lot different than just like memorizing like single page like license agreement document that shows up a thousand times in the pre-trending data or something like that</p>\n<p><strong>[06:01]</strong></p>\n<p>you can actually see like a the way that we like disentangle memorization you can kind of see this uh like gradient of memorization uh in between both mechanistically and and behaviorally with like logical reasoning tasks being quite distinct from wrote memorization and then like factual recall it's kind of somewhere in the middle. >> What's your uh follow on work after you've done this? >> So I think like understanding um like the reasoning capabilities is is very interesting. I think like understanding how postraining affects those capabilities in general is super</p>\n<p>interesting and I think we're going to keep pushing on that. >> Can you induce forgetting so activity? we show and so yeah so there's there's a lot of work on like unlearning uh I would I would describe it more as not unlearning but maybe suppression um I think there's like really like I guess guarantees that you've fully removed information from a model is is I don't think it's been convincingly showed anywhere yet and I I think that it's I think maybe some of the point that we want to make in our work is that when we look at memorization this way it makes a</p>\n<p>a point about how hard or or you know possibly like intractable that is >> I don't know if this is a related problem or it's too different but instead of unlearning but updating >> so I moved the capital of France to Marseilles >> but there's so much training data saying the capital of France is Paris >> I need like a date I need to be able to tell the model hey like this is now out of date but like short of tagging a date on everything I don't know when that makes sense you know that's like a weird</p>\n<p>>> yeah yeah a really big uh like paper from a couple years ago on this exact thing on fact editing was the the Rome paper. It's rank one model edit and um >> nice acronym. >> Yeah. Yeah. Here we go. Uh and they look at exactly like exactly what you're describing. Um it's a really nice nice paper. I don't think it's it's being used like in deployment anywhere. But I think that's like and it's it's really an interpretability paper. And um I think that's somewhere where like you know from a basic science perspective like interpretability has like so much</p>\n<p>to to offer is understanding how we can do something like that which is currently just very difficult. >> So call you over to market a little bit for industry stuff. What can people do at the end of 2025 that they could not do at the start of 2025? >> So I'll preface with saying we we still have a long way to go but but we are excited about seeing like the seeds of you know things actually being deployed in practice. I I would say that we're just continuing to make progress on understanding that models have a lot of latent capacity that you can't get at by</p>\n<p>treating them entirely as as black boxes. So, we're seeing interpretability show up in like model cards now for um eval people are running um for various like red teaming exercises and stuff. Yeah, I know. Uh there's some stuff in Gemini 3 um Claude I think Claude 4 and it's definitely >> Victor. >> Yeah, for sure. I mean, yeah, the interp team there is is phenomenal and I think</p>\n<p><strong>[09:03]</strong></p>\n<p>works across some of their other teams, but and then from Goodfire's perspective, you know, we so like one of our partners, Racketin, is deploying an interpretability based tool in production with one of their language agents. This is a really cool use case where if you what they needed to do was take chats between uh their customers and their agent, find instances where personally identifiable information is mentioned. So names, emails, phone numbers, things like that, and scrub it out. And interpretability turned out to be the the best way to do this at scale</p>\n<p>and in a cost-effective way. It was both more effective and cheaper than alternative techniques. >> And you do that by having like a PII feature. >> Uh yeah, exactly. So what you do is you the uh customer is talking with an agent, but then separately you're putting that transcript through what we call like a sidecar model. And you could, what's really interesting is if you ask that model, try to like use it as an LLM as a judge, it's not very good. But if you probe its mind and you sort of detect when the features related to personally identifiable information</p>\n<p>are firing, that gets you the highest recall of anything. It's it's the equivalent of using GPT5 as a judge, but it's, you know, like 500 times cheaper. So So they're they're deploying that. And then I you know am personally very excited about the use cases for scientific discovery. So some of our partners in the life sciences and in materials uh there there are these narrowly superhuman models for things like genomics, for medical imaging, for proteomics, for material science and</p>\n<p>those are um especially uninterpretable because they are working in domains that yeah we can't well they're they're huge and like we can't native I don't speak genome you know like it's literally base pairs in base pairs out but they're super human at tasks. that are very interesting. So, we have some some, you know, exciting early results in in finding novel biomarkers of disease with some of our partners that um we're excited to share soon. And yeah, I mean, I think AI for science is, you know,</p>\n<p>becoming a very hot topic and I think for good reason. >> Yeah, we're literally starting AI for science pond like we're spinning out a separate inspace AI for science bot. >> I love to hear that. Yeah. >> Yeah. Featuring other work uh done notable work this year. I feel like I have to mention in topics circuit tracing paper. I don't know if there's much discussion internally for you guys. Let's just let you riff like what do you think? >> Yeah, Jack I know has done a lot of circuit tracing. You want to >> Yeah, when that so when that came out we we did a so that was um back in like March. Yeah, March or April.</p>\n<p>>> If you could uh let's say people know about the SAPE work. Yeah. >> What is the difference? Because I I struggle summarizing it. My my summary you can correct me. It's like take an individ like have a full access to a model. take an individual layer and train a replacement circuit that simplifies what it uh does. >> Yeah. So, yeah, that's right. And a good way to maybe put it is that um you take a representation in the middle which is this like weird dense uninterpretable</p>\n<p><strong>[12:03]</strong></p>\n<p>like web of concepts and you know features in a representation and the the essay like decomposes it into like primitives like um concepts firing for like mentions of coffee shops or mentions of like uh New York City or something like that and those are those are much more interpretable and that it basically decompos it shatters the representation to like many many pieces. The uh big change with the the cross layer transcoder so the circuit tracing work was a to to really scale up the models to incorporate like every layer uh so like cross layer uh so the the model is called a cross layer transcoder</p>\n<p>so it's incorporating like it's um tying features across different layers and then the the tracing part is a method for creating like an it's called an attribution graph through those features which are interpretable to like describe how the model is like producing one output like through every layer and through like a bunch of token positions. I can talk a bit more about it. So, when that came out in like March or April, we um so still pretty early for us. There were like eight to 10 of us at the time, I think. So, we we tried to we like uh went had to replicate some of their</p>\n<p>findings. We're really curious about, you know, what would look like training one, like what it's like to use it and basic scientific questions as well, which is like could it rediscover um some like rich representations from like previously understood circuits. So we we put out a post I don't remember when exactly that went up but over I think over the summer like uh May or June on um our replication effort on that and like um how that works. Is there is there an obvious next step like what is what is this all leading up to? Because to me it's like basically just always</p>\n<p>scaling it up, always making it more unsupervised. Uh I don't know if there other trends that you can see like yeah these are the core principles that we're just exploring. That's like a good point to bring up interpretability as as an alignment science versus as like a science for understanding models like more broadly which is like if you >> you're one in the same kind of >> I think it depends on what your motivations are. Um so if you are focused on reading a model's mind um and understanding like what's going on internally to make sure it's not having bad thoughts or like it's not</p>\n<p>misaligned. It makes a >> bad thought. >> Yeah. [laughter] uh it makes a lot of sense to to like do exactly what they're doing. So I think and that I think they're just not they're I don't know exactly what they're up to but uh applying like these techniques to like read out what's going on inside these models mind. So it's like very nice detection like framework but um if we want like really robust control uh like really robust control of models like what they learn during training I think like that's where there's so much more work to be done. I think you know many</p>\n<p>different teams are all working on on these problems but in terms of next steps that's uh yeah like some other directions to go. >> Yeah. No, I think to to Jack's point I think the the circuit tracing work is super cool and I think it's um it's sort of one arrow in a in a larger quiver of techniques that are useful and you know what technique is useful depends on the task at hand that you're interested in.</p>\n<p><strong>[15:04]</strong></p>\n<p>So to Jack's point, like the techniques that are useful for alignment science and evaluations, you know, that's one use case, but there's a whole world of other things that you might wish to apply other techniques for. And maybe maybe you don't need circuits for things that can be uncovered using probes or using other techniques to understand how you know post-training changed a model. also this like model diffing use case and maybe the things that you want to use as like inference time sort of guarantees for for models look a little bit different. So I think it's super</p>\n<p>cool but yeah to your point kind of like there's there's a variety of um a variety of techniques depending on the the final use case that you're interested in. >> Last thing that a lot of people hear in New York I've had like two conversations about it already is what's happening with Neil Nanda. I don't like celebrity culture, but it's hard to avoid Neil's impact. And he basically said he's pivoting his team at Deep Mind uh to no longer focus on whatever and now it's like pragmatic interpretability. What's that conversation about? Like what what</p>\n<p>do insiders think? >> You know, I I read uh Neil's post and I think like I also share the the hope that of of a pragmatic future for for interpretability for sure. Um I think a lot of the >> Why didn't we think of that earlier? we should [laughter] medic like oh god >> I well I also think I also think a lot of the um the response to that I'm not sure that folks actually kind of read it all the way all the way through I think there's a lot of reading of that for some reason folks thinking oh like interpretability is dead or something if anything that says interpretability is</p>\n<p>alive and well like there are use cases for not black te blackbox techniques that that can be brought to bear in real world use cases so I you know I I think we're Um we we probably agree with Neil on that. And then um you know I think also good for like different companies to have different agendas that they're pursuing. Uh I think it's dangerous you know a field will stagnate if everyone sort of is just converging on the exact same same approaches of things. Not sure if you have other >> to the point about like is um</p>\n<p>interpretability like dead or something I think is like a a like gross like misattribution of like what that post is about. to melt your wine. >> Nobody's I'm not saying it. Nobody nobody I talked to was saying it. It was much more like the existing approaches for him were not scaling to some extent and he was the way I put it is kind of like it's like like well okay let's forget about complete understanding and let's manage by outcome and like let's measure ourselves by our ability to steer outcomes and forget if we know</p>\n<p>precisely what's going on. >> Yeah. Yeah. I totally So in terms of like should we just be focusing on like reverse engineering models like from the ground up? So that's maybe where the big change is. I think like we we probably agree that like interpretability should be useful and we're trying to get it like like used right now and we are getting it used right now. I think like maybe where like where we kind of align on this is like we're really focused on</p>\n<p><strong>[18:05]</strong></p>\n<p>like use case inspired like basic research. So yes, like we're very pragmatic, but there's also like a like a lot of like very deep foundational science to be done on understanding models, which is yes, driven by outcomes, but we also want to like develop like the understanding of of what goes on in models because if we're not doing that, then every other uh like lab producing models is just a blackbox AI lab. And you know, that's not how things should be done. or Jack snuck in a little um reference there that I think was was good. So there's this concept</p>\n<p>that we like to talk about internally called pastor's quadrant. So Louie pastor like >> is this like a statistical thing with like the the same distribution but four different that's an okay >> yeah no this is more like a conceptual kind of framework but the idea is that you can have so the two axes of the quadrant are I've heard it described as like discovery and invention or like pure research versus applied research like something in in that kind of domain but the idea is that you can have just pure basic research and so the classic</p>\n<p>example there is like Neil's bore like understanding the atom, understanding the electron just for sort of like theoretical physics uh interest and then you have the uh corner that is just like purely applied research purely you have an end in insight which is like Edisonian research. So Thomas Edison says I want to make a light bulb I don't really like I'll learn the chemistry that I need to but it's just this goal is is the goal and then there's Louis Pastor who like spans both of these. And so the idea is that it's not just this linear thing between basic research and</p>\n<p>applied research. You can kind of have a combination of those two. And so past your, you know, sort of pioneer of like germ theory, but also engineered some of the first vaccines. And so this idea that you should sort of be bouncing back and forth between kind of open-ended foundational research, but then also having a goal in sight and be able to really foot back and forth between the two. I think there's a lot of cases where you see this being a really productive way to make progress in a field. So, there's a company like hero mascot. >> I think it's Tom McGrath, one of the</p>\n<p>co-fire co-founders of Goodfire, who um he started the interp team uh in Deep Mind back in the day, and he uh he has a lot of these like great references from other domains of science that are just uh I think really good kind of grounding points for for how we operate. >> Amazing. Let's get a quick call to action in. Uh you guys are hiring. What are you hiring for? What's what's hard to find? >> Definitely actively hiring. Uh, I hope the AI engineer community um should should definitely go to um goodfire.ai, check out our careers page, actively</p>\n<p>hiring for researchers and uh engineers and >> and you hear nuts. >> Yeah. Yeah. Exactly. Exactly. There's a combination. Um uh so so yeah, we're you know we're doing foundational interpretability research, but we're building out a platform to apply this in real world use cases. You know, customers across life sciences, materials, government, um financial services. So if you're uh someone with an MLE background, no interp experience</p>\n<p><strong>[21:07]</strong></p>\n<p>required whatsoever. You can see we have different backgrounds. I'm kind of coming from industry from engineering. Jack's coming from a PhD. Other folks at Goodfire have backgrounds in like quant trading firms or frontier labs um all sorts of places. But if you like training big models, uh building agents, engineering systems, um we're hiring across uh a variety of roles and really looking to fill some of those engineering gaps. Exit. I think that's it. >> Yeah, that's what it Thanks for having us, son. >> Thanks for coming. >> Yeah. [music]</p>\n<p>[music]</p>"}