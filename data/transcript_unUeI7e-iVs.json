{"video_id": "unUeI7e-iVs", "title": "Captaining IMO Gold, Deep Think, On-Policy RL, Feeling the AGI in Singapore \u2014 Yi Tay", "link": "https://www.youtube.com/watch?v=unUeI7e-iVs", "published": "2026-01-23T15:36:37+00:00", "summary": "From shipping *Gemini Deep Think* and *IMO Gold* to launching the *Reasoning and AGI team in Singapore,* *Yi Tay* has spent the last 18 months living through the full arc of Google DeepMind's pivot from architecture research to RL-driven reasoning\u2014watching his team go from a dozen researchers to 300+, training models that solve International Math Olympiad problems in a live competition, and building the infrastructure to scale deep thinking across every domain, and driving Gemini to the top of the leaderboards across every category. Yi Returns to dig into the inside story of the IMO effort and more!\nWe discuss:\n\n* Yi's path: *Brain \u2192 Reka \u2192 Google DeepMind \u2192 Reasoning and AGI team Singapore,* leading model training for Gemini Deep Think and IMO Gold\n* The *IMO Gold story:* four co-captains (Yi in Singapore, Jonathan in London, Jordan in Mountain View, and Tong leading the overall effort), training the checkpoint in ~1 week, live competition in Australia with professors punching in problems as they came out, and the tension of not knowing if they'd hit Gold until the human scores came in (because the Gold threshold is a percentile, not a fixed number)\n* Why they *threw away AlphaProof:* \"If one model can't do it, can we get to AGI?\" The decision to abandon symbolic systems and bet on end-to-end Gemini with RL was bold and non-consensus\n* *On-policy vs. off-policy RL:* off-policy is imitation learning (copying someone else's trajectory), on-policy is the model generating its own outputs, getting rewarded, and training on its own experience\u2014\"humans learn by making mistakes, not by copying\"\n* Why *self-consistency and parallel thinking* are fundamental: sampling multiple times, majority voting, LM judges, and internal verification are all forms of self-consistency that unlock reasoning beyond single-shot inference\n* The *data efficiency frontier:* humans learn from 8 orders of magnitude less data than models, so where's the bug? Is it the architecture, the learning algorithm, backprop, off-policyness, or something else?\n* Three schools of thought on *world models:* (1) Genie/spatial intelligence (video-based world models), (2) Yann LeCun's JEPA + FAIR's code world models (modeling internal execution state), (3) the amorphous \"resolution of possible worlds\" paradigm (curve-fitting to find the world model that best explains the data)\n* Why *AI coding crossed the threshold:* Yi now runs a job, gets a bug, pastes it into Gemini, and relaunches without even reading the fix\u2014\"the model is better than me at this\"\n* The *Pok\u00e9mon benchmark:* can models complete Pok\u00e9dex by searching the web, synthesizing guides, and applying knowledge in a visual game state? \"Efficient search of novel idea space is interesting, but we're not even at the point where models can consistently apply knowledge they look up\"\n* *DSI and generative retrieval:* re-imagining search as predicting document identifiers with semantic tokens, now deployed at YouTube (symmetric IDs for RecSys) and Spotify\n* Why *RecSys and IR feel like a different universe:* \"modeling dynamics are strange, like gravity is different\u2014you hit the shuttlecock and hear glass shatter, cause and effect are too far apart\"\n* The *closed lab advantage is increasing:* the gap between frontier labs and open source is growing because ideas compound over time, and researchers keep finding new tricks that play well with everything built before\n* Why *ideas still matter:* \"the last five years weren't just blind scaling\u2014transformers, pre-training, RL, self-consistency, all had to play well together to get us here\"\n* *Gemini Singapore:* hiring for RL and reasoning researchers, looking for track record in RL or exceptional achievement in coding competitions, and building a small, talent-dense team close to the frontier\n\n\u2014\nYi Tay\n\n* Google DeepMind: https://deepmind.google\n* X: https://x.com/YiTayML\n\n00:00:00 Introduction: Returning to Google DeepMind and the Singapore AGI Team\n00:04:52 The Philosophy of On-Policy RL: Learning from Your Own Mistakes\n00:12:00 IMO Gold Medal: The Journey from AlphaProof to End-to-End Gemini\n00:21:33 Training IMO Cat: Four Captains Across Three Time Zones\n00:26:19 Pokemon and Long-Horizon Reasoning: Beyond Academic Benchmarks\n00:36:29 AI Coding Assistants: From Lazy to Actually Useful\n00:32:59 Reasoning, Chain of Thought, and Latent Thinking\n00:44:46 Is Attention All You Need? Architecture, Learning, and the Local Minima\n00:55:04 Data Efficiency and World Models: The Next Frontier\n01:08:12 DSI and Generative Retrieval: Reimagining Search with Semantic IDs\n01:17:59 Building GDM Singapore: Geography, Talent, and the Symposium\n01:24:18 Hiring Philosophy: High Stats, Research Taste, and Student Budgets\n01:28:49 Health, HRV, and Research Performance: The 23kg Journey", "transcript_html": "<p><strong>[00:00]</strong></p>\n<p>The thing that I find the most useful about like these models in general is like when I have this big spreadsheets of a lot of results and I understand plots of it. I think models can quite use a screenshot and make a plot of this. I hate making this mplot like stuff about it's so annoying. There were so many moments this year where AI suddenly cross that like that immersion thing. I think AI coding is one of them where we just discuss. I think like nano also got to the point where I usually like you make these images is just like for fun. They just troll your friend or something like that. But like actually really got so good.</p>\n<p>>> Welcome back. How are you? >> Yeah, I'm good. I'm good. Great to be back. It's been one one and a half years. >> Yeah, it's been one and a half. >> Feels like a long time. So last time we talked, you were at Rika. >> Yeah. And then you joined GDM again working for Quark again. >> Yeah. >> And more recently, you've started GDM Singapore. >> Yeah. >> Is it GDM Singapore? Gemini Singapore, dude. I don't know if you've named the team. >> Oh, I I I think we have a Germanite team in Singapore. Yeah, team in Singapore. It's called reasoning and AGI.</p>\n<p>>> Yeah, reasoning AGI. >> Is it important to have AGI in the name? >> It was like a vit thing that we put AGI in. Yeah, I think that like one reason why we work on these models is that we want to get to AGI and this was a V thing that we added AGI to the job posting. Yeah, there's no like formal name of the team yet, but it's basically the Gemini team Singapore. Yeah, I mean I think people are like trying to triangulate Amazon has an AGI team, you guys have an AGI team and then let's say Meta now has a super intelligence team.</p>\n<p>What are people signaling when they choose these names for their teams? Do they have oh we have a plan or is it just vibes? >> You try to fish some hot takes on the no >> you have officially AGI in your job title. >> No, it's not a thing name is is not Yeah. It's just you know we just want to signal the north style of we bringing these models to AGI. >> Yeah. Yeah. No, I wasn't really fishing politics. >> Okay. So, you rejoined GDM. >> Yeah. >> And I think last time we talked about I listened back to the whole thing. It was amazing episode last time you were talking about how it's like externally</p>\n<p>were in Brain and came out and now you're back in GDM. Yeah. I wonder what's your just your general reflections just plugging back into the Google infrastructure. >> Oh yeah. So I guess coming back it's very interesting because it felt and return to Google like everything including your LDAP your username is all the same. It's like you play Pokemon you you leave it aside and then you go back and you click continue save game. Yeah you save game and continue game. It's like that. Obviously the last 1.5 years while I was away many things have changed. Brain is now part of GDM and</p>\n<p>stuff. So I think that obviously a lot of things have changed but I think overall the coming back has been pretty seamless. Obviously I I love Google infrastructure and I think debuts are great and stuff like that. Yeah. And I'm very glad to be back to to Google infra. Yeah. >> And was the intention always that you were going to work on deep think? >> No, not really. I think I miss research a lot like doing like research not like</p>\n<p><strong>[03:01]</strong></p>\n<p>super fundamental research but like close to model research, right? But I really miss being at the frontier and trying to go beyond that, right? So I really missed that a lot and I think when I came back big thing wasn't a thing and I don't think there was any plans actually it was just like I'm just going to work on research and see what happens. Yeah I'm sure I guess there was some inclination that reasoning is the next frontier and that's like obviously the most rewarding research path especially this year. Yeah, I think reasoning these days reasoning and RL is like probably quite is RL reasoning</p>\n<p>comes I spend a lot of my past life I call it the past art working on like architectures and pre-training but I think now I more I like transition more into RL research. I'm not like old school RL but the games RL and old school RL and and to be honest I had almost no RL background coming back but I think like like RL is the main means of modeling these days and yeah so I think it was pretty easy to jump back in and I think a lot of fundamental skills in research is general purpose and universal and and it's it's quite easy</p>\n<p>to innovate even in a tool set that you're not super used to and yeah so I think RL is basically the main modeling tool set like we play around with yeah these superficially I see some you know in your UL2 and fan T5 work um some some overlap of like you know the the focus on objectives and the focus on the stuff that you're trying to incentivize so I would have maybe guessed there was more overlap than you are saying right now which is interesting</p>\n<p>>> but I know I understand it's very >> super shift is objective and they have some like overlap right yeah I think it's just mainly like the on policy and off policiness of designing in these things that change how like also the learning algorithm itself right >> let's just introduce this kind of terminology to people if they're not that familiar with the sort of RL policy I do think that a lot of people are like trying to understand what is working about this generation of RL research anyway so Jason had this interesting post which I think you were co-signing</p>\n<p>which is basically you always want to be on policy instead of mimicking other people's successful trajectories to your own actions and learn from the reward given by the environment basically correct your own path instead of trying to imitate other people's path. >> Yeah. Yeah. And first of all, he writes really well and I wish that more people wrote like him. But I don't know what's your reflection on that or your addition on top of that. >> Yeah. So I think like the biggest analogy of the own policy and off policy is basically all policy is basically like when you SFT something is odd policy. Basically you take some other model larger model stuff and then it's basically like this off somebody else's</p>\n<p>generated outputs trajectories and whatever. I think odd policy is mainly like the core idea of like modern LM RL where you like generate and then you reward the model based on its own generations and then the model trains on its own generations. Yeah. So it's more it's a bit like selfisolation to some extent. You the model generate it own output and then you reward it and then trains on it own output. So I think on policy is basically this idea of like</p>\n<p><strong>[06:02]</strong></p>\n<p>model training on its own outputs and letting the model like generate own trajectories and then let letting some reward verify it and then the model train it own outputs. I think this is more generalizable in general. I think there's still a lot of like science out still to be done about the gap between SFT and and RL itself. But I think basically on policy and off policy right and I think bring this analogy back to real like life. I mean we this on policiness is more like like humans we are more on policy because we go around the world we make mistakes and then we ah okay this is but like imitation learning is supposedly somebody else</p>\n<p>>> not first principle >> it just tells you what to do and they just copy. >> So I think yeah this philosophy bringing back philosophy to life is quite like powerful like when I like now I have a kid and everything like want my kid to try stuff and then you tell them like okay this is like where this went wrong where this went right and stuff rather than okay you just copy everything somebody else does. Yeah, there's a Montasuri schooling is mostly that right like very unstructured learning like you discover your own path and we just give you a safe environment to do it. >> Yeah. Yeah. Yeah. Yeah. What is the point in which you should transition</p>\n<p>from imitation to on policy? >> I do bounce back and forth >> with humans, right? Not models, right? I >> I would say in models it seems like there mostly has been a very concrete like first you imitate then that's pre-training and then you are at the end. >> Technically SF is still imitation. But I think for humans alo a little bit of this right because if you basically like like sports right when you play sports you start off by imitating like hardcore imitating but you cannot imitate forever because you need to like imitation I don't know whe this is good energy but watching a lot of tutorials and stuff is</p>\n<p>more like imitating you learn try to learn certain movements and stuff like that but then like on policy nurse is like going to the game itself and trying to get a reward signal from that right but so I think that humans do need some form of imitation learning but like I think everybody starts off by imitating thing but then again the human and model kind of is not it's just fun to have analogies but we shouldn't like take things like super literally and stuff like that >> I actually I'm a quite a serious taker of machine learning insights into human learning >> that's what we learn from models now</p>\n<p>>> yeah because I think like machine learning is the most scientific way we have ever studied learning just in general >> that's true that's where we have to invent curriculum from like scratch >> yeah that's and things like learning rate. If your learning rate is too high, learning rate is too low. Like wait, do humans even have a learning rate? So I do tell people to to keep an idea of their own learning rate and to be wary of it being too low. So for example, if you've been wrong once, you should ask where else have I been wrong? And typically</p>\n<p>usually, let's say learn, you know what I mean? People usually update slower than they should when they have been wrong. >> Where is it? Stubbornness. It could be stubbornness. Um I don't know is that is that the right word for it? It could be like they they're too Beijian when actually is like their prior assumptions are wrong and they need to completely throw out their previous assumptions because one counter example invalidates</p>\n<p><strong>[09:05]</strong></p>\n<p>all prior experience. Your entire world model is wrong. Throw it away. So Beijian actually wrong. Let's say you live for 10 years under some assumptions and you have one example that breaks your narrative. Okay? You shouldn't be like, \"Okay, now I have 2% update.\" No, actually it should be like, \"Oh, like something's really freaking changed. Everything I've assumed for the last 10 years is probably wrong. What else am I wrong in?\" And update 20%, update 50%, not 2%. You know what I mean? That's your that's a learning rate thing for me. So my my direct example is the whole getting into AI stuff. I was watching</p>\n<p>GANs for 10 years. >> Yeah. >> Has it been 10 years? Uh 2012, 2013. >> Time flies. Yeah. I was watching GANs and I was like okay this is cool it's getting more detail not that impressive then all of a sudden stable diffusion came out and you can run it on your laptop and that was my learning rate okay like [\u00a0__\u00a0] like my mental model of generative image images did not include this and so I was like okay like I am very wrong and I need to pivot everything and that's how I started later space >> so will this mean that like your your learning rate is high</p>\n<p>>> yes I yeah I will nudge it up I I schedule my learning phase because a role model has been violated Okay. I think it's a good it's a good strategy. I think also this brings a little bit to like when new paradigms happen like how fast people are to adopt it or like to invalidate their understanding of things. I think as scientists we definitely a lot of times we do have to keep as the few progress we do have to keep like invalidating our own world model. It could be like a certain ways the way to do like something all along and suddenly something comes along and invalidates it. Yeah.</p>\n<p>>> Yeah. You can be very proud of your priors until it's like becomes your prison. >> Yeah. Yeah, I know. Yeah, that's actually very dangerous. >> Yes, it is. >> Yeah. Okay, that was a bit of a tangent. I don't know how we got there. You did highlight Denny's LM reasoning lectures where he got traced the intellectual history of reasoning in LMS train of thought to to RLFT. And then the one part that I was going to prompt you a little bit was also self-consistency, right? Which I think people roughly know. I think is more crudely implemented with OpenAI than with you guys where it is straight up they have eight inferences and they judge or</p>\n<p>whatever. But I do think that also is relevant to on policy distillation where it's like literally you have eight different powers and they're all from the same model. So checking my intuition there. >> Basically the stuff that you're saying about why on policy is important and using let's say an external verifier to to improve your reasoning. You can also do that with parallel reasoning. >> Oh yeah. Yeah. I mean like when we train RM models they sample multiple times. So yeah to some extent there's some form of yeah self-conidence</p>\n<p>>> is that directly the let's talk right >> yeah self quency is a little bit more the more nuanced version of if you talk to Danny will tell is not majority voting for sure but it's more I agree >> yeah it's more nuanced version of that but I think parallel thinking definitely is related to self-consistency >> yeah yeah I think for those open also actually put out some interesting papers on majority voting versus other forms of like multiple output consensus basically</p>\n<p><strong>[12:06]</strong></p>\n<p>like you at the highest level is actual an actual LM judge that decides like this is actually a worthwhile trajectory that is more valid based on some internal consistency or just like inspecting the chain of thought which is very cool that we can train models to do that. >> Yeah, for sure. >> Yeah. >> Yeah. Self consistency is a big like a big fundamental idea. I mean chain of thought itself was alo a big idea and then self consistency was also like a big fundamental idea in in in uh in modern like literature. >> Yeah. Amazing. Okay. So let's bring it to I guess the one of the headlines of this podcast is going to be about diving</p>\n<p>into the IMO world. >> So this was around about May March March in July. July you guys announced Oh, this very nice photo here. This is the photo I was looking at. This is in London I believe where you had this room. >> Yeah, this room. >> Oh, you got to be at a like you got to be at a photo taking to to get the credit. >> That's [\u00a0__\u00a0] right? What the [\u00a0__\u00a0] >> No, no, I'm just kidding. >> The contributor list is bigger than this. >> Yeah. Yeah. But like they were like say like oh okay you should go to third >> to in order to get a literal gold medal. >> No no no no like like to get the the</p>\n<p>credit for being the IMO at first. So it's just a joke. It's just a joke. Yeah. >> But anyway, okay. Could you tell the story of studying this IMO thing? Apparently it was done in one week. >> So let me like be a bit more clarify a lot of things right. So the IMO effort has been like very longstanding. So Tang and basically and co has been working on this even last year, right? last year because like I was not very bad at Google at the time like they had the alpha geometry stuff and then they were like alpha proof and stuff. So it's it's a very long extending effort but I think this year was the we wanted to try to like use actually use Gemini as a end to</p>\n<p>end model basically no >> no no second system in text out model and even that was a not intuitive thing I covered the silver result from last year >> and I was like okay it's pretty close like it's one point off from silver just try be harder you'll get gold >> that decision to abandon it I think was pretty bold >> I don't know >> I personally was believe always believe in if we're not like in retrospect it's easy to say this but it's a bit like if the model can't get to IMO goal then can</p>\n<p>we get to AGI basically so it's basically at some point we have to use these models to to try this Olympic competitions and I think that one of the goals this year was like okay we're going to do an end to end like tech index model >> that's where like my involvement came in so basically I was not like involved in the IMO effort only do the model training part. So I have to say that that tongue did most of the IMO thing. I just trained the model with a bunch of other what does that work involve? What the surface? So so basically we we just</p>\n<p>prepared the model checkpoint for the actual IMO itself, right? So that that's also something that's easily overlooked about the IMO thing was that many times you you want to chase benchmarks or stuff like that is always like thing that you can kind of keep running and running and he climbing until you get there and then you but like the IMO was a live completion like some members of the team were in Australia for the thing and there was like this happening thing</p>\n<p><strong>[15:06]</strong></p>\n<p>was happening live was unfolding live. >> Oh it's a very alpha goal. You receive the thing you like punch it into your system and then like >> Yeah. Yeah. Yeah. So, so like some of the professors from Kung's team were like in when they went to the IMO itself and stuff like that the conference I don't even know where the IMO is a conference but it feels like there were people there like in Australia and then so it was a live thing and there were people who actually the job was to run inference on on on this IMO P1 the P6 that came out and they also came out on like different days so it's like</p>\n<p>different sets like one day one day two something like that so the fun part is that I knew nothing about IMO like at all. I'm not like a kid that took part in IMO like too down for that. You're a piano player >> and yeah, I have a piano but what I only knew was that okay we delivered the checkpoint and that checkpoint was used to do the IMO but then like there was somehow a week in London where everybody gathered that. So everybody was flying to London and then this photo was taken there and then you get to see how all the different parts like come together</p>\n<p>and like also being in the other rooms in the rooms with the other like co- captains and then it felt a little bit like a hackathon thing. So yeah, I think this was like the training process of this IMO model itself was like maybe a week or so. Not the actual like the whole like basically. >> Yeah. Yeah. I think that the question is I'm still not over the decision to throw away alpha proof. >> Okay. Okay. Yeah. Basically, I think it's very major and I understand that you have this goal of AGI obviously like at some point one model should do it to</p>\n<p>do all of it, right? But I think you pointed a gun at me and said in 2024, what do you need to do IMO and IOI and and CPC and all the other stuff that you guys did was you need an LM reasoning system that knows how to operate a computer and knows how to write lean and run lean verifier and all these. But basically you rot the lean verifier into the chain of thought. >> Is that okay? So basically like it is not obvious that you can do that.</p>\n<p>>> Okay. at all because I think >> so okay so I think what you mean is that like some in some way system encoded in of the model somehow >> yes >> yeah I mean it's just whether at the end of the day you just believe in like this like connection is one one model lots of parameter I mean there's also two use right there also two use which you know and stuff like that but I think to some extent the model we like I I think we should be able to get to a point where</p>\n<p>to like in the past when the LM first started the model could not even be a calculator. Now it can somewhat be a calculator. So technically like a tool like a calculator is somewhat encoded in the parameters of the model. So I think we we we will eventually get a point where whether there's things that cannot be expressed in the parameters of the model is like an open question. We we don't know where is that limit but I think we will keep pushing and pushing this limit. So whether like uh some something like a lean system or like</p>\n<p><strong>[18:08]</strong></p>\n<p>some other things to solve other like physics engine or something where it's still we still continue to push that that that boundary. Yeah. But I actually don't know like whether there were a lot of debates about symbolic system versus >> yes that's the word I was training. >> I actually don't really know whether there was like to me I was just like oh let's train a model and then some someone told me to train the model and I train the model. Basically there was like overarching like I people like the IMO effort that that decided this and I also think that because the basically these specialized systems are very like</p>\n<p>oneoff systems that are like you could create like a chemistry engineer create math engineer could create the thing right but at the end of the day you want one model for everything so yeah so I think this kind of fits that direction a little bit more where you have one model for and then this model was also like launched as Gemini dip thing as a general purpose Gemini dip thing so it's basically unchanged but with maybe some config toned down a bit. >> Yeah. Yeah. So the the the inference time config was like the one served to most people is different but and the full IMO like inference config was</p>\n<p>present like shipped to some mathematicians just because of the inference cost right but that was good enough to be a general purpose model. I think my take is that this intuition was what led to the trying to go towards one model instead of cuz this specialized systems there's no end right you can create many specialized systems yes the most I can see in the future is there'll be a model then that is there's something that really cannot be subsumed by a model then you just use a tool or something right but my prediction is that I think most things can be can be</p>\n<p>subsumed by the model I think yeah I mean researchers are quite good at he climbing Wait, >> history would say that you you have a lot of evidence backing you up. Is this the model output that this is it right? This is what >> Yeah, I think this is the model output. Yeah. >> What do you see when you look at this? You just see obviously it looks like a well-ritten problem. It looks like something a real human mathematician would do. I people did compare yours versus the openi one where openi is a lot more raw or have to clean up their versions. We don't have to talk about</p>\n<p>openi but I just I think what is interesting to you when you saw this kind of output. I want to give a little bit like a special disclaimer is that I know nothing about the app. Right. So I think the wonderful thing about this era of LM is that like you can be like a AI researcher engineer and you don't have any domain knowledge and you can still yeah source get a gold medal universal tool >> that you don't know anything about. I can't pass this at all like this is foreign to me. But like maybe a proof is a particular kind of chain of thought. >> But I would say that the other</p>\n<p>interesting thing that some of the some of your collaborators were talking about it was like oh this is the first example of reasoning in a non-verifiable domain which to me isn't proof by definition verifiable. I just want to give you things to riff on or debates that are might be worth digging into. >> So I think that's good. There's a lot of aside from proofs there's a lot of domains that are like nonverifiable and I think not not easy to verify. So it's</p>\n<p><strong>[21:09]</strong></p>\n<p>like when people mean non-verify, but it's like non-trivial to verify or like just not as not as easy as like the solution of like a math problem because pools are long form and it's also that's why it's non-trivial to to verify it to lin and then you do all kinds of things, right? So I think there's a lot of work to be done in like this non-verifiable domains. Yeah, I'm getting into this territory where I'm not sure what I can say, what I cannot say. Okay, so yeah, sure. Well said. I think another thing that is an open topic of debate was how much domain specific work or post</p>\n<p>training was done because you then went on to do the II and CBC stuff as well, right? The same model. >> I was not directly involved in the ICPC but I was related to some extent. That's all I can say. Yeah. >> Yeah. Any other interesting call outs maybe just on the team? you called out Jonathan as someone who is co- captain on on this effort and yeah basically how does the effort come together >> so I think there were four captains for the IMO two from London Jonathan was from Mountain View I was from Singapore</p>\n<p>so I think four of us basically trained this model together >> and I think one I also trying to see what T was saying but I think one one interesting thing was that like we all in different time zones and we all and there's something also very interesting about like passing on the job there's no like already like fixed workflow how to work together between captains. So it's more like oh I'm going to board the plane now. I'll be AFK for 12 hours. So someone take just babysitting the run. >> Sometimes there are bugs and stuff. The job comes down sometimes. So basically it's very ad hoc and it's very very it's</p>\n<p>really between the captains how we decide to like work together and yeah but I think it was a kind of interesting time also because we were all flying like I think the London folks were not having to fly but I had to fly and John had to fly and then like when you visit another country you have another like if you visit an office you have many meetings. So that was in and out meetings and it was a pretty like interesting and I also think that we nobody really knew whether we will get go at that time because IMO actually has hasn't happened. Yeah, it it was interesting, exciting and then I think</p>\n<p>like the whole process of this getting verified by the IMO committee and you know like you know that was like okay but we're not going there but I had to learn a lot about how the IMO works right apparently the goal score is not even it's not even a fixed number it's like a bell curve on right >> so it was like a time where you just look at the score you're like >> like I was even like looking at the watching the human part participants and then seeing like what the scores were because whether Gemini will get gold depends on like how the humans do. So you like looking at oh if a certain percentage you like what's the do we go</p>\n<p>>> yeah to some extent you don't have any control over that. So >> yeah but you just curious right because so I would say that is definitely more in like exciting like there's more adrenaline than like just running on a benchmark and getting a number was like a process that that that took some time. Yeah. Yeah. But I think overall if you have specific questions you can ask also but I think this whole thing has been a highlight for me. This IMO effort has</p>\n<p><strong>[24:10]</strong></p>\n<p>been >> Yeah. I I would say most people if you ask them maybe two years ago whether a model could get an IM model they would have said like impossible. >> Yeah. >> Then the silver helped right from last year but like the fact that you can throw that system completely away and then just take existing Gemini and scale up deep think and then just run it for IMO gold I think it's also like very non-conensus compared to last year. >> Yeah definitely to some extent I think researchers were also surprised. I I wouldn't say like surprise but like it was more like a pat of on the back kind</p>\n<p>of surprise but we actually made a lot of progress in we as in collectively the all the engineers and researchers working on Gemini it's a lot of progress being made you just look at how much we went in one year yeah and I also think that just 5 years ago right not two years like five you just you just imagine the outcome like you just look at the state of AI now like just generally the IMO and the ICBC go and like also even like things like nano banana if you just look at the AI progress now and five years ago I think people would think that we already reached like AGI >> some form of AI >> to some form of AGI we're just moving</p>\n<p>>> if you just traveled like you take this checkpoints and you traveled back into 5 years ago uh someone should make a drama about this but I think it's really quite impressive like how the field has moved so quickly >> yeah yeah >> the hard parts you would say were scaling inference >> in what like hard expensive >> hard as in maybe the most amount of brain power expended on the team I saw some comments where they were like actually the hardest This part was the inference optimization or like the very very long horizon inference that deep lang needed compared to normal Gemini</p>\n<p>stuff like that. >> I didn't work on the inference time scales. I yeah I wouldn't know here >> that is mostly that oh and then there was this the code name was apparently IMOAT which you named after your desk. >> Okay that's not really like it was in the so I think I tweeted about it at point at some point right bring up the tweet. So the IM cat was basically like okay it's not like a official code name or something. It's just like the name that the config of the job was like I cat that's that's the</p>\n<p>>> you just need some kind of name. >> Yeah. Yeah. I mean I just like you know I just I like cats and then Yeah. >> Yeah. That is mostly it on unless you want to bring out anything else. We have other sort of researchy topics, but beyond before I go into sort of researchy topics, I did want to maybe leave the floor to cover. What else should people know about the reasoning effort that's going on at GDM? >> Let let me think of where to start this. >> What do people need to know? >> Was really good. Yeah, that's what people need. >> Maybe an easy one to start with would be a lot of people were focusing on maybe</p>\n<p>academic benchmarks two years ago. Last year maybe LM Marina this year Pokemon is very interesting reasoning visual reasoning and just general long horizon agent planning yeah benchmark and I I don't know you seem to focus on it a lot and I think Gemini did it very well so obviously I think it's something that is easy to talk about >> I think I probably should there's actually nothing specifically done for</p>\n<p><strong>[27:11]</strong></p>\n<p>Pokemon >> of course >> yeah of course there's nothing specifically done for Pokemon and I think that I think Logan had this tweet recently about the recent Gemini on Pokemon Crystal >> and Pokemon Crystal is >> so much more efficient. >> Yeah, I think Pokemon is like so I used to play a lot of Pokemon and I'm a big Pokemon fan in general and I think it's a it's a great like you say it's a great long horizon benchmark and stuff like that and I think it's a good to check in once in a while on these benchmarks that like almost never get contaminated or like people actually like don't spend</p>\n<p>time to climb benchmarks. It's like kind of silly to like build like okay like what you working on some people oh I'm working on Amy I'm working on h I'm working on Pokemon maxing or something like that that's kind of like funny we did interview the clock Pokemon I think his name is David and it showed serious flaws in enthropics screen understanding vision capabilities couldn't literally couldn't tell I'm trying to like get past this wall but you keep just keep</p>\n<p>running to it doesn't know if the wall is And so that doesn't have any spatial reasoning at all. >> I mean that some of it could be like a harness like the harness thing or like also whether the model has access to like game state information or is it complete visual? >> Yeah. Cloud's implementation is very game state heavy. They dumped effectively like all the the memory of what's going on in the emulator. >> Yeah. Yeah. I see. Yeah. >> I think for I don't know whether I'm jumping off tangent or something like that. I think solving Pok\u00e9mon is going to be more of like how fast you solve</p>\n<p>it. And then like the thing that I have not really seen so far is like whether the model can complete the Pok\u00e9dex. >> Why is that? I know you need more challenging. >> No, complet is so hard. You need to plan. You need to like you need to search up like information like there's some things if you don't like go online and basically you need to like have a little bit of deep research in this. Yeah. There the model will just never know like that it needs to trade. Okay. If he's able to go online post on forums and then find someone to like hey co can I trade with you Pokemon to evolve? Some Pokemon needs to be traded to be evolved. Yes. Or me.</p>\n<p>>> Yeah. >> Yeah. But anyway, I I don't know. I have not seen the model be able to complete a book that completely hard actually for models. So I was I think that's actually an interesting like like an interesting one. Yeah. >> Yeah. I wonder what the real world analogy would be once let's if let's say we we have a model that is capable of doing that what can we make it do that we cannot do today? Oh, there's a lot of planning involved. Just real deep research >> like planning. There's also a lot of planning involved and it's more I think Pokemon the Pokemon game is very linear,</p>\n<p>right? The Pok\u00e9 Bax is involve a lot of like backtracking, research. >> Yeah. A lot of research and a lot of Yeah. So, it's a probably a different nature itself. >> Is that as interesting to you as for example a lot of other people in the AI for science world are trying to discover things that you cannot look up, >> right? >> No knowledge. >> Yeah. No knowledge. Because basically what you're saying is we're not even there yet. We're at the place where models cannot consistently apply</p>\n<p><strong>[30:13]</strong></p>\n<p>knowledge that they look up, right? Like you give Gemini access to a web search and you say, \"Okay, go try to collect all the Pokemon in a Pok\u00e9dex.\" You don't have high confidence that they'll do it. I don't know if someone's actually tried. Probably know, right? No, I I think the hard part is actually like trying to use the like synthesize the web knowledge and then apply in in in the game itself with all that visual state going on and stuff like that. It probably will be solved in one. It's not >> it is challenging. >> It's not like super interesting like you're basically just >> the task really is can you look up the guide to do it and then can you apply</p>\n<p>the guide? That's it. You know what's even more intelligent than that? creating the guide like being the first to figure out how to create the guide which is what >> oh yeah yeah but then when it comes to this is mostly does an exhaustive search thing the model to try and try like humans guide for that yeah >> okay so that's actually less interesting to you interesting >> okay actually to be okay when you think over like not super super interesting but it's okay it's just have not seen a model to try to do this so it's >> yeah I think like efficient search of a novel idea space is interesting obviously you can brute force anything</p>\n<p>but we're not talking about brute forcing we're talking about trying create an AI scientist >> but no knowledge is actually an interesting thing that that I think is going to be quite a big thing being able to generate no >> Google has done stuff there which I don't you're probably not that close to those teams that has done AI scientist work >> there's some things that have been it's like for example if you freeze the model weights at like 2015 the free time at 2015 >> and then you even with the current model let's say you have okay just assume there's no leaking of information somehow can if you ask the model what's</p>\n<p>the best ML okay not 2015 like 2012 or something just tell you like SVMs are like the best right this is the way that machine learning works in general right then the question is can you invent the transformer right it might not be able to like even today models they might not even be able to invent the transformer like if you freeze the time at a certain time and even you bring the tech I mean the model is a transformer so I just say there's no assuming there's no leakage so like totally possible like so so I think there's still a lot of open questions about like whether the model can really you know innovate and generate like really No.</p>\n<p>>> Yeah. >> Knowledge. Yeah. >> Yeah. One related question on that which I think is related to the Denny paper which is I think people have this sort of mythicism on what reasoning is and reasoning if you really demystify it a lot. It's whatever happens inside the chain of thought tags, >> right? And you post you you you're eliciting that reasoning behavior from some stuff that is already latent inside of the pre-trained corpus. is that that's one version of this</p>\n<p>interpretation. >> I think these days like reasoning itself is very vague and it's very open. So it's like most mostly different people will have different definition of what reasoning is right. So I agree that like this chain of thought is like basically when people think of reasoning they associate with chain of thought and obviously is what happens in in in the thinking and okay reasoning right but I think these days is more like I said in earlier part it's like reasoning RL is</p>\n<p><strong>[33:14]</strong></p>\n<p>almost like the like basically it's anything that is like post training to elicit capabilities basically RL and post training to exist capab capabilities so I think the actual like technical definition of reasoning is making models better with thinking and post training Okay. Yeah. So basically like RLing the model to think better, right? And thinking is more like thinking traces and thought trajectories and stuff like that, right? There's also this line of work for like latent thinking and and stuff like that like whether latent thinking and discrete token thinking is like going to be the same thing or like something like that</p>\n<p>is like a open question >> meaning adding extra tokens to cap that represent or >> there's all these I don't forot what's the name like this academy papers that do this loopy things or >> track tokens >> or like they basically instead of decoding discrete tokens you actually simulate this by doing this in latent space right so when you do chain of thought thinking and like reasoning you basically decode extra tokens hide it in the thinking tag >> and then you decode stuff but like lat is basically you just don't decode tokens you just don't bother buy it</p>\n<p>>> it might start speaking the native language of thinking is numbers not passing you through some filter of English >> and sometimes you might start thinking Chinese or something else >> yeah yeah generally I'm not I'm not really I don't really believe that model thoughts have to be the same with human thoughts I'm actually like generally in ML I'm more of the school of thought of let do whatever he wants in general >> there also discussion. There's a latent representation hypothesis paper that I think you're maybe sympathetic to if you haven't already read it. To me, sounds obvious, but basically image models will have the same idea what a laptop is</p>\n<p>versus a text model will have they converge on like the same latent. >> Yeah. And obviously, you can align them and you can do all those stuff with them. And so, it totally makes sense that their concept would be just a vector of numbers that represents laptop. That's the concept. Yeah. Yeah. And okay, maybe you have some numerical differences between one model's idea of what a laptop is versus another, but it mostly would be the same. Yeah. Very interesting. The question I was kind of leading into was that because there's now we are in this age where DLM text is in the corpus of like stuff that we</p>\n<p>train on where it's a little bit of a recursive loop, right? Like the reasoning tokens are out there now. And so pre-trained models themselves, pre-trained based models are also capable of reasoning and they're increasingly so as more and more reasoning text goes into the corpus. Isn't that interesting or is that worrying? >> Do you actually see much reasoning trace on the internet? >> So I've never seen those though. >> I would say that hug your face. Yeah, people are publishing that specifically</p>\n<p>as now as to whether or not people researchers are actually including that in their training corpuses. Who knows, right? Like that's their their choice. But I would say that percentage on common crawl that has coot tokens in there went from zero to 0.001%. >> And it will just go up over time because like people are publishing it. >> Yeah. But I think if the sources are</p>\n<p><strong>[36:14]</strong></p>\n<p>like quite clear, you can actually like filter away those because usually people put it on GitHub or put it on >> Do you want to filter it? Maybe you don't. >> There's a choice for the for the >> Yeah. quite quite literally the whole reason why I don't think we covered this in the previous part, but two years ago a lot of people were like, \"Oh, you just include more coding tokens in your pre-trained corpus. It will be >> Wait, but the coding tokens are different from like coding tokens. >> It generally is just outside of code for reasoning.\" >> Oh, that was like >> You don't believe that? >> No, no, no. eyes in light. I don't know if it's still true today, but let's see. >> Yeah, that was just our general coverage</p>\n<p>of reasoning. I would say that there's a lot of interesting work here and more to do. Maybe I'll cover one thing which I know that you have personal inputs on which is that you have started using AI coding. >> Oh yeah. Uh so I actually don't really use much AI coding in the past but I think we've reached a point where AI coding has started to become really useful like okay so before AI coding the most the thing that I find the most useful about like these models in general is like when I have this big spreadsheets of a lot of results and I</p>\n<p>just made plots of it. I think models can quite good screenshot and make a plot of this. I hate making this mattplot like stuff about it's so annoying. Okay, but that's basically like one thing that I can remember about like how I use AI in the past. But I think AI coding has started to become the point where I run a job, I get a bug. I almost don't look at the bug. I place it into like anti-gravity and like I told it that fix the bug for me and then I relaunch the job like beyond like VIP coding. It's more like VIP training VIP ML or something. I don't know. I</p>\n<p>would say it does pretty well most of the time and it's actually there's there are classes of problems that is just generally I know this is actually really good for and in fact maybe probably better than like I would have to spend like 20 minutes to find like figure out the issue and then fix the thing and then >> to be so yeah that's very interesting because I would say like level one vibe coding is you actually know what to do you're just too lazy yeah it's just ah just do it for me like I've done this a thousand times like just go fix it like I know exactly what to do here you're saying Is that kind of like the next</p>\n<p>level? Well, you actually don't even know. It's it's investigating it for you. As long as like the answer looks right, you just strip it. >> At the start, I was a bit like I did check it, look at the thing. And then at some point, I'm like, okay, maybe the model looks better than me. So, I'm just going to let it do it stuff and then I will relaunch the job based on the fix that the model gave me. And I think the models will just keep getting better and better. Yeah. >> So, yeah, it's something that that I also think that I recently there's anti-gravity. I think I think also because these tools were not like that in Google infrastructure is not that</p>\n<p>easy to you don't I'm not that familiar what is available outside and when I was at the start I didn't really I think the models were not like so good like one and a half years ago so it's also like a forcing function that like so people are like oh try gravity is a game changanger and stuff like okay so I just started using and yeah >> yeah you spent some time with verun recently what did what >> oh no I really say hi green okay</p>\n<p><strong>[39:15]</strong></p>\n<p>>> I guess you were telling me you're an AR researcher that doesn't was to use much AI and like now you actually like AI pill as a user. >> There were so many moments this year where AI suddenly crossed that like the emerging things. I think AI coding is one of them where we just discussed. I think like nano banana also got to the point where I usually like you make these images is just like for fun you just throw your friend or something like that but like nano actually really got so good that you can use it for >> that that you can use it for like basically yeah so it's getting really good and I think yeah this year the stuff like and even things like the past</p>\n<p>many this will like hallucinate things a lot but now I just trust it like automatically I think we just I people are just enjoying the utility by by yeah by by by by these models. So now I'm like I was always AI field like AI is a good thing like I don't see how anybody can disagree with that but >> yeah but you actually using it for things that you are high expertise in which is your own ML work. >> Yeah. Yeah. Yeah. >> And just just to you you do you have a special version of Gemini that you use internally that we don't have access to or it's like public Gemini or</p>\n<p>>> I do I think it's the public Gemini. Okay. I was just saying like it would be entirely reasonable to train Gemini internal for only your code base and your work. >> Oh, actually I'm not sure though. You see these things are just like rather for me. >> But obviously if it obviously improves improves your productivity by I don't know 10%. Yeah. Worth it, right? So I I think that's interesting and there's the interesting thing levels of how much do you trust it? How much of your jobs do you automate away and no longer need?</p>\n<p>There's also the question I guess about how people come up and train in a field if they you no longer need juniors because the Gemini is your junior ML ML researcher. So I think this all interesting questions. >> I want to say one quick thing first. Right. So I think that when it comes to like whether a model can be like a junior suite or like something like that right so I think if you think of it this way of if a job from a one one x suite one time suite can be replaced by a model itself but let's say you are a manager right the objective that the</p>\n<p>metric you track is like your time and then if you can have a model that saves you like the same amount of time as the work that your reports do but you don't actually like replace one person per se but you >> a little bit from everybody. Yeah, right. Then you can I definitely agree that okay like when you count the net time save there are times where the model can fix bugs that like would have cost me like one day right one day is huge and these things are definitely like if you I don't know whether anybody has done any like real like metric evaluation or these type of things but if you use time as a real metric and then not as in number of okay maybe</p>\n<p>three hours like kind of metric right but these things are not like like going to replace one person as as it is but more like a passive aura that buffs everybody the game terms, right? Yeah. >> I often think of myself as a bard cuz I tell stories and I plus everybody around me. You know, that's that's a that's an ideal situation for me in a DND group. >> Oh, okay. Yeah. I don't play DND, but Okay.</p>\n<p><strong>[42:15]</strong></p>\n<p>I said they're the kings of support hero. >> Support spot. Yes. Yeah. Okay. AI support I think is very encouraging. I think like where is it still not working for you that you've tried and you're like, \"Oh man, I expected it to be better.\" Oh, there are times when models get try to get lazy and try to fix something in a like they still rehab. They get lazy and then they try to like guess like me into thinking that like the bug is fixed. So there there are still classes of there are classes of problems that are like very easy for the model, very hard for humans. There are some things that are very easy for</p>\n<p>humans, very hard for model paradox and and stuff. So it's still very hard to characterize this these things into this proper quadrants and stuff like that. So I would say that the capabilities on models these days are good enough to be like really helpful but like it's still you is a bit like it still has some but yeah but I think this will like this I don't think there's anything that to to be done to specifically like focus fire these things is more like general capability improvements the model just get better over time and then these things will just like go away >> you say that okay so yes I think obviously in the grand scheme of things</p>\n<p>just trust the process keep scaling in every dimension and uh things will just fall away, things will emerge. But you've also said in the past, I can't remember the exact tweet where you were like each additional data set compounds over time. They're just small additions. And I would say that when you say things like focus fire on things that you would think humans, it's easy for humans, hard for machines. Those are easy wins where you can just add a data set that would focus fire on that. And >> isn't hill climbing just a sequence of doing that until you reach AGI? Okay. So</p>\n<p>I I get your point. I think that it's true that sometimes a lot of progress on the whole is just a series of small incremental changes that Yeah. that push. I think that's accurate. That's true. There's also it also feels that there's also a lot of like small like seemingly minor for the lack of better word like that push AI to the state where it is today. So I definitely agree. So nothing like against people who like focus fire, but it's just that when I mean that like it might be not easy to focus fire on things that are like not very easy to characterize. So</p>\n<p>it's just that like when he has something targeted, right? Okay, I want to improve this capability as some data. So I think like defining like if I was defining the problems and stuff like it's like characterizing it and if it can be characterized and then okay then fine. But I think it's like what I was trying to say with the coding is that these things are not even some of the class of problems are like I don't work on coding but like people maybe work on coding they know like they have like terminology for different types of failures but so maybe somewhere somebody is focusing fire on this then make the model better that's great for everybody.</p>\n<p>>> Yeah I mean that's why it takes thousand people to like get all these things together. >> AI is definitely like a big collective effort these days. It's a big >> it's really crazy. Okay. So I just wanted to broaden out to general things people are talking about in the community on research >> which again I know that you are very locked in so you don't necessarily have read all the papers or anything but we</p>\n<p><strong>[45:16]</strong></p>\n<p>can just riff on ideas. You can obviously ask me what I think as well. >> Yeah. >> Is attention all you need? So attention and transformer has been is like a core idea in the recent times like pre-training and scale is the thing that made attention and transformers like actually shine right because without I think the first transformer paper was just like a machine translation thing and then basically like GPT and bird were the ones that like actually showed the full like big potential of this idea. So in terms of is antia like really really all we need like probably</p>\n<p>no but I think it's like one of from the architectural point of view also may maybe no but it's not all you need is but you need it definitely >> what else are you thinking about on that same level are you talking about stuff what do you mean when you say it's not all you need >> definitely need like you definitely need like the skill pre-training you need or the tokens you need like RL >> I think when I say when people say I say it's attention you need is mostly view. >> Well, will transervers get us all the way to AGI? Right. I guess is the >> Oh, so so basically when you get to AGI</p>\n<p>is the >> will still be will it still be a gnome architecture or like meaningfully different? You'll be a transformer I think >> really like like you like people it depends on what you call it but I think unless the paradigm shifts completely which is I mean as a scientist you cannot like like completely like say no to like like that this will never happen but my feeling is that it's been like what like almost 10 years since transform >> 2017 years since the transform I think we have not replace self attention like</p>\n<p>it's some form of it like you could rename it you could name something else because sometimes you >> can do local globals. Yeah, it's still a transformer in the end and I think like that's not going like anyway unless the whole thing with like back prop like everything like goes like the whole thing just changes completely like then there's a different story that's a different conversation to have but if it's still within the same scope and mounts so I spent a lot of time thinking of about architectures and like whether there's alternate architectures and stuff I okay at the sequence processing</p>\n<p>level like >> there is the ultimate >> yeah there is sequence to sequence transformer >> it's probably the self attention is there this whole big era which I was also involved in this era where people try to like undermine the attention as much as possible like they try to remove it simplify it make it efficient like this whole like like efficient attention era at the end of the day the outcome was always like oh remove all attention but we have one layer of self attention there and it still works like that's at the end like of the always a story which even known a character he published some</p>\n<p>stuff about how he has some ratio or mixing of local and global attention Right. Like you basically still attention but modifying it quite a lot. >> I will consider local and global attention to be like still attention >> just like how much you're skipping. >> Yeah. Yeah. The the only question is that if the formulation changes too much your QV becomes like ABCD MG or something like that or some >> maybe I'll give you some motivating</p>\n<p><strong>[48:16]</strong></p>\n<p>constraints in order to do this. You guys are still charging 2x for over 180k token context for 240 something like that and the max his theoretical max is 2 million tokens right what if we need 200 million is there some point at which where even this concept of input token context is irrelevant because you are doing continual learning that kind of stuff where you're modeling it as okay the AGI will be achieved through a sequence to sequence transformation therefore And attention is the best</p>\n<p>sequence to sequence model or architecture. Therefore, attention is all you need. But I think other people are like sequence to sequence doesn't actually capture intelligence. >> But that's not really about sequence to sequence. That's more about like the whole gradient design and back prop thing, right? It's not the architecture itself. That's that that's a problem that we that is more of like the learning paradigm itself rather than the architecture itself. I think the architecture is just basically like the interface between the learning algorithm and the tokens. I think it's more about the learning algorithm itself and this</p>\n<p>like continue learning this like there's many ways to think about processing many like insanely large like context right like 200 million 1 billion tokens or something like that right like whether it's going to be like you have a new learning algorithm that every time you run inference you you learn on it right then you can technically have some kind of memory like human being is learning as I'm talking to you right so that that's also like one way the other way is like whether okay maybe somebody will say that okay the attention is like just too expensive for 200 million 1 billion context so we need new architecture or</p>\n<p>some people will say that oh we just improve the chips like accelerators so I think many ways to interpret it but I think it's like if it's about there's a lot of fundamental things that if it's about continual learning and stuff like there's a lot of fundamental things about the learning algorithm and and stuff as well yeah that will have to change I think like the learning paradigm and architecture and stuff like that goes like hand in hand and I think as the field progress and ideas just stack on top one another right so there's also this thing about a idea that was proposed has to be compatible with all the work that have been done</p>\n<p>before to to shine right it's a bit of variant of this hardware lottery like by by Sarah it's not the harry that I wrote about the tu failing but it's it's the original hardware lottery but it's more like a bit of lottery of like the things proposed have to play well with the things that were proposed before so it's a bit like going down this local minima to some extent so now we are like in this local minima of like transformers everything everything right Maybe it's not easy to like get totally out of this because also a lot of people's investation optimization have been have</p>\n<p>been done. So the things that play well needs to play well with the ideas >> before and the way I see it now is it's very difficult to like >> yeah come out of it. >> Okay. I'm not entirely convinced. I see what you're saying but let's call it gen AI [\u00a0__\u00a0] hate that term. It's still a very young field and so yes, there's been eight years of work on the transformer, but what's that in the</p>\n<p><strong>[51:17]</strong></p>\n<p>grand scheme of things? Maybe we're in a local minima and we got to notch ourselves out of it. I do want to leave that open-ended. I don't have an idea. I do think that people are in this call what I sus calling the age of research, right? Like where we're like, okay, we scaled up what we can scale up. There's we know what the next maybe one two orders of magnitude look like in scaling on every dimension that we know about but what is the next dimension to scale >> there's this like mis misunderstanding a little bit about like oh the last 5 years has just been like scaling things</p>\n<p>scale >> okay please tell me more yes you made that joke about now we scale re researcher salaries >> okay let's not go there but but I think that ideas like matter and I think that there have been a lot of good years in the last 5 years. It's just that maybe it's just not so it's not been like blindly like if you took a MLP right just like without self attention and you just okay I'm going to throw like hundred trillion dollars on this and scale up that thing and the thing >> it's never going to work. >> It's never going to work. >> Yeah. Yeah. So there's no like there's part of it there's also like I think the</p>\n<p>bitter lesson gets used too much in like too conveniently used around but actually there's also a little bit of uh not a bit there's also a sweet lesson where it's like ideas matter and I think even to till today right like people downplay ideas and stuff like that yeah do you think the rate of new without being specific about what ideas cuz obviously you can't share but like do you think the rate of ideas has increased or decreased because there's like kind of a law of diminishing returns are smaller >> I think the number idea is always proportional to the number of researchers working on a certain problem. So by definition by definition</p>\n<p>it should increase but I think the number of ideas that actually work is not decreasing compared to the last like if you're not in the era of diminishing returns yet. >> Well so I think ideas are still very important and there's still very good ideas that are game changers that are being invented. >> Yeah. Yeah. And I think I know the answer to this but is the closed lab advantage increasing versus open source or decreasing? like the Chinese labs they say keep publishing open source models and some of the American labs as well publish</p>\n<p>open source models would you say that the ideas that I see there Nvidia has Neotron open has GPOSS these are all basically checkpoints on what is publicly known about training models as of this year you know >> okay okay >> that it's declassified information cuz everyone okay yeah everyone does this >> I think that the gap is increasing. >> I don't think it's it's completely predictable from stuff that you've said before. >> I think the gap is definitely increasing. Yeah, >> I think that would make that justifies</p>\n<p>researchers otherwise what's the point of having researchers if not finding new tricks that compound over time. >> Yeah. Um >> Yeah. Yeah. But but definitely I think it's is increasing. Yeah. >> Okay. I'll do a side tangent. I don't know if you have any comments on this. So then this is very related to Nvidia's recent purchase of Grock which I don't know if you have views because you're very TPUcentric but are we memory or</p>\n<p><strong>[54:18]</strong></p>\n<p>compute bound and this is relevant to the transformers discussion of like >> in terms of what like serving >> exactly I think the the classic view is that we're comput because we just need more comput for pre-train and RL and then inference. >> Yeah. But actually the counter argument I would make against this is I actually have these charts of Moore's laws. I wish I could just pull it up easily. Mo's laws of the scaling of compute versus scaling of memory versus scaling of network and bandwidth. And compute has a much higher slope of</p>\n<p>scaling than the other two. Memory what the cheap memory like honestly I don't think about this this memory bound that much. So maybe it doesn't. Yeah. So I would disagree with it but I don't have high confidence in in >> and because you're mostly on the research side less on the inference side. >> Yeah. There's maybe the inference guys will be like oh >> yeah yeah yeah I don't think about I don't wake up when I think about serving. So yeah maybe I don't think about the inference that much. Yeah >> my my previous line of discussion here was like Nvidia is very forsighted by Milanox because it actually is the real</p>\n<p>bottleneck in scaling because it has the lowest mor law >> and then the second one now is memory >> which is very interesting. >> Okay. Okay. But honestly, I don't think about I don't think about this that that much. Yeah, I understand. Okay. Data efficiency. So, this is a joke. But implicit in this is that there's some kind of maximum data exposure, right? And and so I so previously I would say that a lot of the training paradigms is like one epoch is all you need. That's the nimi title of this idea. I would say that the real number maybe is between</p>\n<p>three to four epochs. And I do wonder what the theoretical limit of data efficiency of a model in terms of training and compression should be I don't know that means >> data efficiency and basically but you're asking the question in a way asking like how much repeats is tolerable is that >> one and tolerable is contingent on does it actually improve in meaningful it's not about like you actually want to do it for it for his own sake but I do think there's that and then there's also</p>\n<p>just the sheer amount amount of stuff that we can learn with limited data. So you say let's say say like we you're not compute bound you're not memory bound but let's say you are data bound right last time that we were on the podcast we talked about chinchilla versus inference optimal training but now actually I think a lot of people are even talking about like data optimal training like given limited data set how well can you learn from it I think that's an interesting research direction that not enough people are talking about maybe is something that is common place in the labs but it seems very clear that we are</p>\n<p>very unoptimized with regards to how much we we learn from our data. I'll just put it there. I think in general the like learning more like extracting more from every data point is definitely valuable but I think that also related to the fact that we're like running out of tokens in the world. So I don't work</p>\n<p><strong>[57:21]</strong></p>\n<p>on data for pre-training and I think things that I say would >> general state of industry not >> the general state right so I think that the I don't know even know whether data has like diverged like the way that these things are done it have diverged too much across this labs and no open >> there's a lot of cross crossollination for sure >> crossation okay yeah but I don't think about data that much like the pre20 data that much this >> yeah maybe earlier this the first half of this year I would have said that kind of pre-training is dead and that everyone's like just funneling all their</p>\n<p>work towards RL and you we had this like grog chart which is very interesting where we're sending the same amount of compute on as to you think it's a scout >> no I don't know I had no idea yeah >> I think that people are taking it seriously they are like yeah okay whatever especially in the agent labs like cognition they're taking the open- source models from from whoever and then adding let's call it pre-trained scale RL on top of it if they have that level of info which data which they do</p>\n<p>>> which is very interesting I would say yeah this data efficiency argument yeah I think it's to me it's also more trying to discover new paradigms of learning in order to get where we want to all go which is yeah >> and the existence proof is humans right your 2-year-old daughter can is much more capable than than an LLM in some things having seen way like eight orders of magnitude less data. That's very interesting. >> Yeah. Compare human learning, machine</p>\n<p>learning is definitely like like >> purely as an existence proof that we could probably do better. Three examples of dog. >> Yeah. >> Fourth example of unidentified animal I can probably tell as a dog as a human but machines classically you take 20. >> The efficiency of humans is definitely way higher than than models. Yeah. The only question is that where does this thing come from? Is it actually like putting more flocks on every token or like maybe it's like back to the question about whether the transformer is the optimum architecture? Maybe it's back prop is the maybe it's the off policiness. Maybe it's the</p>\n<p>>> Yeah. So what is the like where's the buck right where exactly >> uh but maybe it's a feature not I don't know but uh >> so okay we've identified probably it took me a while to get this across. So this is the kind of data efficiency I'm I'm talking about. I think it's emerging basically at the end of every year. I like try to take bets as to okay what will be the big themes for next year. >> Yeah, >> I think this is one of them that people are really trying to focus on because you're feeling this data crunch even though everyone's like still investing in data. I forgot to mention that I would say that I've been wrong on pre-training being dead. Yes, I've now</p>\n<p>met pre-train leads from both enthropic and ofi and I've seen the talk from the deep mind guy recently and so like everyone's investing in pre-rain still which is like >> nice to see >> nobody's preaching was dead >> I know no it's a it's a theory that we're trying to disprove or or prove anyway so I think okay let me wind back to my general idea right so yeah data</p>\n<p><strong>[60:22]</strong></p>\n<p>efficiency seems worthwhile you would treat it as like okay well show me where the bug is and I'll go fix it. >> We don't know where the bug is. We just have existence proof that it could be better. >> And then I think the final >> logical chain in this for me is that everyone is focusing on some idea world model as a version of this for more efficient learning which potentially might not take the form of a sequence to sequence transformer. I don't know how that works like definitely a little bit out of my depth here. To me, that is more efficient because every wall must</p>\n<p>be internally consistent. And if the next piece of evidence come in and invalidates those walls, then you no longer need to pursue those paths ever. And you can just narrow in on the wall that you've identified. And so to me, that is learning where you're learning to fit world models >> onto the actual. >> Yes. So yes, >> maybe you can treat the learning process as curfeitting. >> Yeah. So you're learning the world instead of learning the world model. Yeah. on learning the world model, right? Okay. By sampling multiple world models and then finding out which one</p>\n<p>fits the data the best. So I guess my query is this what people talk about is this if you I mean obviously feel free to attack it because I'm just spitballing but this is what I pick up from talking with multiple people about okay what are you talking about with world war models what are you talking about data efficiency and learning efficiency and like how do you j it all together in a cohesive sense of the future where we can what's the definition of world model at the start from start >> yeah there are three kinds >> okay okay go on yeah >> first kind is the vio kind >> v kind</p>\n<p>>> or the what's the other genie >> that that D man has which is the sort of video world model. You model everything with some kind of goss or whatever and you like you inhabit those that 3D space. >> Yeah. >> Second, let's call it the Yan Lun/ Meta school of thought which I don't know if you're that familiar with it. He has published the jetack architecture and then separately fair has also published the code word models where you're basically specifically for code very interesting you are executing code and modeling the internal state of the execution environment as you go line</p>\n<p>>> okay >> the the LM actually like learns to predict those things and actually it seems a lot more efficient at the scale that they've tested it out >> which is very cool >> which definition are you anchoring on >> the third one >> the code one the code world model >> that's the second one those two are bundled together >> the japer fans are probably hating me right now because I'm lumping all metals work under one school of thought. >> Yeah. >> But whatever. >> Okay. >> Okay. The first one is VO genie like super spatial intelligence those kind of video based world models. Second</p>\n<p>one is some execution or some sort of explicit modeling in the uh as you as you sort of run through the the corpus. >> Yeah. And then I think the third one is this amorphous thing which I think people are trying to get to where they are doing what I said about the resolution of possible worlds and you're curfeitting as you learn as you inference. >> Yeah. But what is the world model itself</p>\n<p><strong>[63:23]</strong></p>\n<p>itself is it like >> it is a mental model of where everything is and how you think the world works. How what I think you think that everything >> but in technically it's like >> it is something in the latent space. Okay. Okay. So you can for simplicity could be just be like a transformer model between train and >> yes. >> Yeah. So to me that is the most coherence thing to the current paradigm which is you could actually do this in current transformers. I think the way that you train it will probably have to be different. >> Okay. I see. I see. >> I I don't have any conclusion here. I'm</p>\n<p>just throwing it out as something where I know you're interested in this kind of stuff and I don't have that many knowledgeable people to talk to about it. >> No, I don't think about role models that that often. I think because world models are just not really well defined in the first place. But I think >> so don't say world models but the problems is learning efficiency and maybe I guess accuracy or like AGI capability that is not easily unlocked right now on our current path of scaling. >> Yeah. Yeah. So I I think I think when it comes to like like data efficiency I think it's more like uh I'm believer of</p>\n<p>finding ways to spend more flops per token, right? Because you actually basically if you are data bound you want higher data efficiency because you can learn more from every data point squeeze squeeze out more points right so things like that can extract more can use more flops on every token is definitely like a form of data efficiency then there's the learning algorithm right because I think there's this there's a different scaling law of like humans is this machines is this and like dogs are this cats are this different exponent chart >> right yeah there's this famous famous</p>\n<p>famous chart uh and point one and point two are just like not entirely like different things because it could be that better architecture is actually just spending more flops per token. So if you you come to a point where you are very data bound but not compute bound at all you just find algorithms that spend a lot of compute on every tok on every token. So I think the overarching point is just that okay that it's a learning algorithm thing for data efficiency and then if whether the correct way is actually does apply more flops per token does squeeze out more from every</p>\n<p>or data every data point also because humans actually don't like like they exposed less when you say less or more data is very ambiguous because they technically like on 247 and then you mostly visual have a lot of like different types of inputs right >> and whether they actually spend more flops on every thing they listen is also question because maybe they're just pay efficient just because I mean I somebody needs to count like how much flocks the brain used to process like how much maybe they're just spending more compute on every token and also maybe the learning algorithm is different so but I</p>\n<p>agree that data efficiency is very important given that that I think we're going to like there's limited amount of data in the world >> one more thing before we go into DSI you know how like we're talking about RL and like you're working on RL stuff why are people paying so much for RL environments >> so who is paying for our environments >> open AI and the topic at least I don't nobody said anything about the mind so a</p>\n<p><strong>[66:24]</strong></p>\n<p>lot of the model labs that are not you are well known for paying at least seven figures for external startups to create our environments for them to train in okay and I think the question is if you are your models are so good at coding why don't you do it yourself and so I think there's some amount of expertise that's being distilled from human experts into anal environment that you can and let your agents run wild in. But I'm curious if there's any other deeper insight than that because I'm not satisfied with my own explanation.</p>\n<p>>> Our environments that are like that have a lot of domain expertise are probably very valuable and actually I don't know specifically about what environments people are actually buying. But what what was the thing that you're not satisfied by like the >> it was so valuable and a lot of people are saying like look it's a next app inside of a docker container that logs stuff out when you send inputs in then you could probably do it yourself internally right why you pay so much for some startup that you don't know to do it for you</p>\n<p>>> actually I have no clue about what like why this is happening yeah I have no clue >> and a classic example would be like if you want to build a computer use agent for buying things on Uh, in e-commerce, you would want all environments that perfectly replicate maybe the top thousand e-commerce websites. >> Yeah. >> And then you just parallel vote out on all of them. Does that seem reasonable? >> I don't know. >> All right. Cool. DSI and LM Rexis. A big bet for me this year for my conference was we actually like started focusing on LM Rexis. The other</p>\n<p>>> actually what's the motivation behind starting LM Rexis? Yeah, >> I think Rexis is the king AI problem in consumer. It is the single most valuable thing. All your feeds, any even search is Rexis. >> Basically, search basically like it's a retrieval. >> It's the god problem, >> right? Cuz Rexis is ranking, but then also filtering, also personalization, also reindexing and and and like performance. It is the god problem and you get paid a lot for it. engineers are not that excited by it, which is very weird</p>\n<p>>> because they don't see a lot of them don't work on Rexus and they probably never will. >> Yeah. >> But they don't see the monetary value that can come out of a good Rex. The other two pieces of updates for me which I actually didn't even know that DSI like directly tied into this. >> Okay. >> Was one Twitter publicly adopted their feed algorithm as an LM Rex. >> LM are just used everywhere now. like whether it's actually like a >> like a big LM >> like whether it's like a generary retrieval type of models like it's like</p>\n<p>another question is it >> we don't know all we know is that they have said that they have swapped out their current Rexus for an LM based Rex that's all they >> okay >> but what has what is published is YouTube where they actually adopted semantic IDs for YouTube's Rex >> and YouTube is obviously a big deal >> is it like public information yes >> okay okay >> they came and did talk about it with >> us okay >> and then they published a V2 this year</p>\n<p><strong>[69:24]</strong></p>\n<p>as well Okay. More info. I just So, so basically the last time we you were on the podcast, we didn't talk about DSI that much, but you have actually some background in IR. You care about IR. No, I don't care about IR, but but I think DSI, okay. Like DSI or generative retrieval was like I think one of my favorite works in the old of like I have some IR background in like when I was doing a PhD, I did some Rexis work. I did some retrieval work with Rexis and stuff. So, I have some IR and Rexis background. So I think generative retrieval and generative Rexis is all</p>\n<p>very conflated. DSI started as a retrieval thing. So we did like natural questions like ranking of like doc documents and everything. It started off as I mean there actually we did an interview with Yanick like me and Don we did when the paper came out like long time ago. So at that time we wanted to like reimagine retrieval and search. So we wanted at that time LM we were still using T5 models at that time. It was like not we're not in the LM era yet. it was premium right it was like okay pre-training works kind of thing and then there were like some pre models around so we wanted to re reimagine</p>\n<p>retrieval right but retrieval raxis they all the same formulation ranking retrieval problem right and then that's where we started to imagine retrieval as one giant that that encodes everything in the memory we tried so many different like sematic ideas actually my collaborator Vin was the one that came up with ideic ideas that basically at the start of this whole gender ritual was actually basically literally just trying to give a document like identifier and that's predicting like raw brute force predicting like this it actually like it actually works because</p>\n<p>the models can memorize some something if you look at the literature from all the way to things like Dr. back this very of like model the words have no meaning they're just ID in in a vocab it's another number right and technically the models have enough capacity to predict but I think semantic ID was an idea that that basically you have some like sematic association and then you actually try to break down the search space hierarchically right so how this will evolve into Rexis was at the time after DSI came out right so at cheese group and Mahesh the guy who who</p>\n<p>they did some exploration of applying DS SI to to Rexis and that's how that generative generative Rexis recommend system paper came out. >> I didn't even know he was involved. >> That's crazy. >> That was like basically us like transferring this like basically okay DSI works. If we try to try it on on Rexis and then I think the recommended system people have a slightly different way of doing semantic ids but it's basically just because the the domain is slightly different but after that I</p>\n<p>think we were done with the invention part and with this one >> the rest is details >> the the rest are details so over time I also left Google and stuff like so over time this thing evolved a little bit everything I think I also saw like something like Spotify is also using something like YouTube Spotify like they they use this type of semantic ids they start of DSI like models I think from the research community point of view like the DSI world was the first one</p>\n<p><strong>[72:25]</strong></p>\n<p>that like decodes semantic tokens but then when we went to I don't know community is like strange in a way that like they will do things like oh this is gen retrieval it's not genies it's like they'll do this kind of like random things that is like a bit strange but yeah I really this was like the whole history of this gender retrieval apparently there's also a lot like of people working on I don't follow actually I don't follow Edis at all Now it's just not even in my mind like there was once I went to even in the Singapore office there Swiss actually like working on >> genative retrieval. They don't work on I don't know whe they're still working on</p>\n<p>it but I met a person that tried to explain genative retrieval to me. It was quite funny that you know I kind of like co-invented generative retrieval but yeah I think this is this whole IR thing has been it's just an interesting phase and I I think DSI is one of my more creative works that I've done that is not like really LM but it's like under the general principle of apply ML to everything if the googler is working on general retrieval would that be like AI overviews is that something similar >> I have no idea okay for the people listening I I did have a track there I think you just type in AI.engineer</p>\n<p>engineer and you'll get it where the Gemini guy was talking sorry the YouTube guy was talking about how they use Gemini for the Rexis I don't know what size of Gemini cuz they he didn't talk about it but this is public work now and basically every YouTube video uploaded gets encoded into some kind of code book and they they retrain this every on some kind of batch job uh yeah just interesting so yeah I I don't know if you even know what what Gemini is being used or</p>\n<p>>> I don't follow like this these days. I do think like in the sense of like for people who are not still not getting it, applying intelligence and the general intelligence of an LLM to the retrieval to the recommendation task means you can accommodate such weird recommendations like such weird queries as well that normally like no classical system can ever handle and I think like is also somewhat emergent in a sense that when you were using T5 you just couldn't actually add that much value on top of a</p>\n<p>normal BM25 retrieval technique. Would you say that's accurate? It is not just about paraphrasing. It is about understanding query intent. >> BM25 is a really strong baseline actually. B25 is a really strong baseline. Yeah. uh is like I I sorry I don't I don't know the the the comparative delta versus T5 for you guys versus BM25 but I don't expect it to be very high and I expected it to be a lot higher for for a true LM base depending obviously on the query set</p>\n<p>>> I didn't really think about it this way before but because I've done modeling in many different domains going like search and you know in the search committee there's also set of benchmarks and stuff like that for like that people who climb on there some like there's Amy of or like a of I don't know what is it called these days anymore but generally the modeling dynamics of IR task is very different from like task is very different from standard language task or</p>\n<p><strong>[75:26]</strong></p>\n<p>like vision task and something like with Q clan LM where you train models like the way that modeling things interact with this environment is very different. So I think that I honestly I hated working on like Rexis and Rush stuff. Okay, I'm just looking about old days when you work on like T5, you work on you change architecture, you try to improve perexity, you use super glue like this is olden days era on like even now when you train LMS you just do zero shot two shot stuff like that your things the you because as a researcher engineer you just interact with the environment a lot by this you just like</p>\n<p>okay RL by this environment but Rexis and IR has a very strange feeling to it strange feeling in the sense that it feels like your like whatever works it's like you are this in a world with the the gravity is different or like you are in a world where the modeling things that feel intuitive are not intuitive so it feels like a very strange space to so I wrote some papers back in my days on like Rexis and stuff like that every time I ran some modeling experiments for for frais and stuff like I didn't enjoy the it feels like the environment was</p>\n<p>rude it feels like the vibes are just like like >> what makes it rude >> it just feels transactional >> no not not not transactional like I I don't know how to describe it for example like if you play like sports like play tennis perimeter when you hit the ball you have a very nice feeling like hitting the sweet spot when you do modeling in traditional when you get the feedback back you feel like everything sounds right everything feels right everything right but Rexis and IR is like a problem where it's like you hit the shutter C and you hear the glass shadow like randomly you just feel this</p>\n<p>weird say that that >> cause and effect are too far apart >> like it just feels strange and then sometimes maybe the metric like I think races they use like all the NCG effects and then the BM25 strong and then you do this and then you get like worse at the like the the BF25 back in the day where you stack two LSDM to three LSDM you like whoa I see like it's just the game >> unrewarding area to work in it's just weird also the IR community and the retriever community is also like always behind the mainstream and then now it's just probably gotten even more worse</p>\n<p>because of stuff so okay I'm getting into hotic territory but it's just like certain conferences are just like behind new rips and ICML and stuff Yeah, some conferences are just like they're just like applying things that that this >> they're downstream of >> they're downstream. They're downstream. >> Okay. Okay. >> So, it always feels very uninspiring to to to work on on this. >> Yeah. Look, there's a reason that you left and >> but it was like a side quest like where I work as a side quest thing. Yeah. >> Uh yeah. Okay. I understand. I still think it's an important business problem</p>\n<p>even though maybe it's an unre unrewarding field. You kind of understand why because the academic benchmarks for those tasks are just so far detached from they're so far detached from what industry I didn't on any of these like in like the thing but just from academic point of view. >> Oh then all you need online right yeah test and oo okay >> that would have been a different experience. Yeah >> that is mostly our sort of topic research topics coverage and everything.</p>\n<p><strong>[78:27]</strong></p>\n<p>I think we're just going to end on a very simple one on GDM Singapore. you you organize a symposium here. We brought Jeff Dean Quark and all the others basically what's the general message or the impetus for starting GDM Singapore. >> So we will talk about the event first. So the event was mostly so co and I are going to start a team and then I think before I came back we discussed this for some time. Jeff was very supportive of this. He was in the region many times in Vietnam in Singapore last like about around the time where I was going to come back and I think that so this event</p>\n<p>itself was qu and Jeff was were visiting and we just inspire the community here. I think that it's also a bit more like a soft like setting the tone right for the start of the Gemini team in Singapore. And I think it's a very rare instance where you get somebody like Jeff and Qu who are the true pioneers of AI in the world to be in one room and then are you there as well and I think like many people told me true of AI but okay I was there to to tweet >> yeah yeah I think having them all in one</p>\n<p>room and then giving these talks like many people came out to me and say they were very inspired by their presence in the region. So starting a team and starting something is also very like there's also no one moment that is okay press the button it starts right it's like a process right so we hire people and then people join one by one and stuff something like that right so I think this event was more I would say like a to set the vibe I think it's possible for Singapore to be close to the frontier and I think that we having the true pioneers of AI here we want to</p>\n<p>give this basically more like an inspiring thing and also get left for co and Jeff to meet the people here and and so Jeff was here last year but Quark hasn't been here for some time and he's going to have team here. So it's like also nice to bring him around and meet the people here. >> Yeah. >> So it was a really like amazing event. We met along as well >> and >> who a lot of people don't know has a CS degree. It's like one of the few PMs with a CS degree. >> Yeah. Yeah. I would say that the context of the meeting was more like partially like also he wanted to learn more about</p>\n<p>the IMO stuff. >> Oh really? And then also about Jeff. >> Oh, cuz they invited you without knowing that these guys were coming or something like that. Right. >> It was a bit like some young Jeff and Quark and me where we went to visit the chat with atanga and we discussed a little bit on deep thing discussed a bit IMO and then I think the rest of it was more like Jeff and Liz and was talking more about like became less about AI and more about very macro >> economical political thing which was I</p>\n<p>was very out of element way. So I was just like I just >> you were in the suit. Yeah, I I was just talking about the dictating and the IMO and stuff like that, but he seemed generally quite surprised that AI has reached this point. So I think >> but it was a was an interesting uh >> I I would say for people like you have done something that is unique in Singapore's history so far like you you're establishing a frontier research lab in Singapore</p>\n<p><strong>[81:28]</strong></p>\n<p>>> which is an accomplishment. I think the other thing also that I guess I'm still trying to wrap my head around is does geography actually matter? Like you're all working on the team. You have your London people, you have your Mountain View people, and mostly you're just like collaborating with them anyway. You've collaborated with them your whole life. I don't don't even really know what countries mean anymore when it comes to research or just AI in general because this thing is just inherently international from the start. >> This is a very good like question also is related to I think about identity,</p>\n<p>right? Because I think you also moved from SF and Singapore quite a bit, right? I was in like model this like one two weeks ago and I'm here but like almost all my like if you just look at my aside from my family like everybody I talk to is like somehow in the Bay Area or like just because of work and everything I think the geography matters okay firstly the most like thing is like logistical is like probably time zone >> so you literally want the 24-hour coverage around the world there are advantage no what I'm saying</p>\n<p>is that the difference is possibly But like it's also more like like people define the location more than location define okay the people somehow okay the time zone we get to time zone a little bit like pros and cons. I think there's pros and cons right >> you bullish on on Asia Singapore people the talent pool >> I think we managed to find like crazy amazing people but I al also have to say that this type of things is more like talent attract talent. I think most of the time like people are very excited like the vibe I get is that people are</p>\n<p>very excited because it's like co team and my team and we're working on library core things related to to to AGI. So I feel like the talent we can get from the region is really good but it's only it's only because it's us we can unlock this talent otherwise might join some other place but it >> yes and move to to the US. Yeah. Yeah. Yeah. About the identity wise, I would say that I definitely agree with you that like why does it matter like that? I think that the advantages of Singapore</p>\n<p>itself or like just anywhere like it's also that you are like okay the world is very go so technically you can interact as much as you want. You can also go there but I think Singapore has this advantage where you can go close and you can go far. I think the Bay Area is like so much about I have friends in London and New York. They will just never move to the Bay Area. I'm not against Bayer. I think Bay Area is a great place, right? But it's just yeah everywhere, right? I think sometimes if you have some like mental space and energy to like to have some other culture and then</p>\n<p>like you know like London, Singapore, New York, they have their own culture, right? But the Bay Area culture is just like AI, right? Like you just go anywhere you just hear AI everywhere, right? Even the billboards and stuff like that. >> You think a bit much. Yeah. >> Although I did see some billboards down here also. >> Yeah. I was like what is this >> culture infecting Singapore? >> I do think that to some extent if you want to do research in you need a little</p>\n<p><strong>[84:30]</strong></p>\n<p>bit of peace and quiet somewhere right. So this island may be good for that but then you can you still like able to like be connected right? Okay. So I think that's mainly talent wise I think people are strong here. Uh yeah. So far enough away but you're still connected. You have strong talent. What are you hiring for? You're still hiring, right? >> We're hiring like like my team will work on like RL reasoning for Gemini and Gemini deep thing. I think we care more about like talent density now. So we're not like also like growing that big this</p>\n<p>small team first just because comput is probably like important and yeah so I think that's something that that we're hiring for now. Basically, I personally just there's a lot of DNS, but I think like generally there's a lot of people like who are very capable, but I think what I'm looking for mainly is either you have like a track record of RL research or like some even not necessarily RL but or or like some exceptional achievement in like coding competitions or like some exceptional</p>\n<p>achievement somewhere then that's like the kind of people that that we want. >> Yeah. because you don't strictly require I do remember something about your record days where you're like you like to train your own traders from scratch right so you don't >> I forgot if I said that or not but to some extent to to extend yeah I think we definitely be very happy with people that are like very high stats and just like even without much statistical knowledge >> no stats points in like just high tech points people like just raw IQ high tech</p>\n<p>talent people like I think all like strong engineering skills ML ML can be learned easily the knowledge can be learned easily. >> Yeah, I think maybe one version of this is can it be done on the student budget and where they can you do something interesting anymore on a student budget. I would say re relevant to the point where conferences are quieter these days. I did do an interview with one of the best paper winners where they worked on thousand layer neuronet network RL and and that was done on a student</p>\n<p>budget. They it was very cleanly executed pieces and paper and good findings. Look, I'm not sure if production models will ever go to a thousand layers, but they stretched it in an interesting direction and found some good recommendations and the guy immediately got hired by OpenAI. And I think that's encouraging for the grad students in the market who are like, okay, well, do I need to know somebody who works at these labs in order to get in? My uncle works there. I get the internship or whatever. No, actually, you can just do it on a student budget with with good adviserss. Oh, I actually think one thing interesting is that for most of the people that I actually went</p>\n<p>to recruit them like personally like right so it's you see their work and then you send them DMs right so I get a lot of value people no no no like like for hiring generally >> so generally I almost to your point it's like you almost like you can just do good work put it online and then somebody will contact you right it's actually super easy but super hard at the same time because >> no I can tell you like I talked to a few</p>\n<p><strong>[87:30]</strong></p>\n<p>of these grad students they don't know what good work means means right because they don't know there's so many things their professors have the agenda they're forcing on them which like may not be right cuz it's not like their professors know what to work on either so yeah they just need guidance they just need hey work on these like five things you show me interesting result in any of them >> okay so if somebody comes up with something and then does something that you feel that is very tasteful and aligns with what like researchers in the labs like like want and they come out with that independently you know that the function that produces the subject is good, right? Like if you just go and</p>\n<p>tell somebody to do this, like you you can you just get the signal that these guys can execute, right? So I think there's some value in people that Yeah. they demonstrate taste. Research taste. >> Yeah. Research taste. Yeah. It's very interesting. I feel like I could give people Yeah. I I do care about this. In some ways, the research directions work that I do is a little bit of that. like it's low accountability for me because obviously it's just thought experiments but I think for a lot of people it's like their career is bounded by can you demonstrate research taste with this like short three four years of that you</p>\n<p>have and just do it. >> Yeah. >> Yeah. I would say that this is is more there's so much competition just because of like the everybody wants to get into AI. It's just more of like how to like mostly it's more like how you going to prove yourself that you're you're right. >> Yeah. It must be hard these days to be a grad student trying to prove yourself. It's definitely harder but yeah. >> Yeah. Not your job. Okay, that was it. Do you have any other sort of rants or topics that you had queued up before we rap? >> I don't. Yeah, but it was great. It was I had a great time. It</p>\n<p>>> was fun chatting, man. >> Fun chatting. >> Yeah, even I even love Last time we were supposed to do last time we meet at the symposium. We were supposed to record. But even we just ended up hanging out and chatting. It's just nice to get to the brain dump of what's going on in your world cuz Yeah. working on really important stuff, man. >> Always great to chat with you. business. Yeah, >> good to chat. Parting words on the sort of weight loss and workout journey cuz that's also a big thing for you. >> I think being healthy is important to be to do good research, right? And I think I think I've been probably in one of I probably in a big physical health now.</p>\n<p>>> Yeah, you look great. >> Yeah. Thanks. And I think it's also impacted my work in a good way. >> You did the sort of kapati inspired like biohacking. I didn't go to extreme but I was like also quite data driven when I came come I will like have my own emails and I will track this then I I was I'm still supposed to make a blog post about this but I feel like I'm not like really at the endame yet so like when I get there I will but yeah just to just for people who don't know I like I think I lost 23 kilos</p>\n<p>>> this year actually across one year >> one and a half years uh so 23 >> yeah basically literally from the last podcast to now >> yes yeah There's an ablation study now 23 kilos. Yeah. And I think like my HRV heart rate variability has went up by two times and my rising heart rate has dropped by 30 beats per minute. >> 30 beats per minute. >> Like it was like 80 90 and now it's 160.</p>\n<p><strong>[90:32]</strong></p>\n<p>>> Oh yeah. 8090 is super high. >> Yeah. I was unhealthy. Yeah. Yeah. Yeah. >> Okay. >> Yeah. So I think like when it's hard like what do you have a thing that kept you going? You know a lot of people they maybe they their focus on AI and including myself, right? I do prioritize work. I enjoy work. I don't enjoy the fitness side, but I obviously it it feeds in to your intellectual work like the sort of log off and go for a walk, eat better, all that kind of stuff. Obviously, it feeds in, but like people seeing a positive example like you, they will get inspired to do the same thing.</p>\n<p>So, I think it is good to set yourself up as an example. >> I think definitely helps like when I do these things for my health, I just think that it's also part of work because it helps me to get better in my job. So, it's important as well. I think it's important as well. >> Yeah, I like the HRV off the bat. I have no idea what mine is, but yeah, there's a general question about what is productivity and how do you measure it? What what really matters? And it's still unclear to me, but I do think general energy level and hunger almost like you almost have to like experience physical</p>\n<p>hunger in order to have intellectual hunger. Yeah. I don't know if that's like a thing. >> No, when I'm hungry, I just think of food. I think to me it's like this destroying things but when you but but it's hard to do work when you're hungry. Yeah. >> Okay. Thank you so much. >> Yeah. Thanks. It's really great. Yeah. Have a great time.</p>"}