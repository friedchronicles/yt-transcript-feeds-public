{"video_id": "XqoBSB3nsgw", "title": "\ud83d\udd2c From Red Teaming GPT-4 to Automating Drug Discovery: The Future of AI in Science \u2014 Andrew White", "link": "https://www.youtube.com/watch?v=XqoBSB3nsgw", "published": "2026-01-28T19:14:38+00:00", "summary": "_Editor\u2019s note: Welcome to our new AI for Science pod, with your new hosts RJ and Brandon! See the writeup on __Latent.Space_ (http://Latent.Space)_ for more details on why we\u2019re launching 2 new pods this year. RJ Honicky is a co-founder and CTO at MiraOmics (_https://miraomics.bio/),_ building AI models and services for single cell, spatial transcriptomics and pathology slide analysis. Brandon Anderson builds AI systems for RNA drug discovery at Atomic AI (_https://atomic.ai)._ Anything said on this podcast is his personal take \u2014 not Atomic\u2019s._\n\u2014-\nFrom building molecular dynamics simulations at the University of Washington to red-teaming *GPT-4* for chemistry applications and co-founding *Future House* (a focused research organization) and *Edison Scientific* (a venture-backed startup automating science at scale)\u2014*Andrew White* has spent the last five years living through the full arc of AI's transformation of scientific discovery, from *ChemCrow* (the first Chemistry LLM agent) triggering White House briefings and three-letter agency meetings, to shipping *Kosmos,* an end-to-end autonomous research system that generates hypotheses, runs experiments, analyzes data, and updates its *world model* to accelerate the scientific method itself.\n* The *ChemCrow story:* GPT-4 + React + cloud lab automation, released March 2023, set off a storm of anxiety about AI-accelerated bioweapons/chemical weapons, led to a White House briefing (Jake Sullivan presented the paper to the president in a 30-minute block), and meetings with three-letter agencies asking \"how does this change breakout time for nuclear weapons research?\"\n* Why *scientific taste is the frontier:* RLHF on hypotheses didn't work (humans pay attention to tone, actionability, and specific facts, not \"if this hypothesis is true/false, how does it change the world?\"), so they shifted to end-to-end feedback loops where humans click/download discoveries and that signal rolls up to hypothesis quality\n* *Cosmos:* the full scientific agent with a *world model* (distilled memory system, like a Git repo for scientific knowledge) that iterates on hypotheses via literature search, data analysis, and experiment design\u2014built by Ludo after weeks of failed attempts, the breakthrough was putting data analysis in the loop (literature alone didn't work)\n* Why *molecular dynamics and DFT are overrated:* \"MD and DFT have consumed an enormous number of PhDs at the altar of beautiful simulation, but they don't model the world correctly\u2014you simulate water at 330 Kelvin to get room temperature, you overfit to validation data with GGA/B3LYP functionals, and real catalysts (grain boundaries, dopants) are too complicated for DFT\"\n* The *AlphaFold vs. DE Shaw Research* counterfactual: DE Shaw built custom silicon, taped out chips with MD algorithms burned in, ran MD at massive scale in a special room in Times Square, and David Shaw flew in by helicopter to present\u2014Andrew thought protein folding would require special machines to fold one protein per day, then AlphaFold solved it in Google Colab on a desktop GPU\n* The *E3 Zero reward hacking saga:* trained a model to generate molecules with specific atom counts (verifiable reward), but it kept exploiting loopholes, then a Nature paper came out that year proving six-nitrogen compounds _are_ possible under extreme conditions, then it started adding nitrogen gas (purchasable, doesn't participate in reactions), then acid-base chemistry to move one atom, and Andrew ended up \"building a ridiculous catalog of purchasable compounds in a Bloom filter\" to close the loop\n\nAndrew White\n\n* FutureHouse:\u00a0http://futurehouse.org/\n* Edison Scientific:\u00a0http://edisonscientific.com/\n* X: https://x.com/andrewwhite01\n* Cosmos paper: https://futurediscovery.org/cosmos\n\n00:00:00 Introduction: Andrew White on Automating Science with Future House and Edison Scientific\n00:02:22 The Academic to Startup Journey: Red Teaming GPT-4 and the ChemCrow Paper\n00:11:35 Future House Origins: The FRO Model and Mission to Automate Science\n00:12:32 Resigning Tenure: Why Leave Academia for AI Science\n00:15:54 What Does 'Automating Science' Actually Mean?\n00:17:30 The Lab-in-the-Loop Bottleneck: Why Intelligence Isn't Enough\n00:18:39 Scientific Taste and Human Preferences: The 52% Agreement Problem\n00:20:05 Paper QA, Robin, and the Road to Cosmos\n00:21:57 World Models as Scientific Memory: The GitHub Analogy\n00:40:20 The Bitter Lesson for Biology: Why Molecular Dynamics and DFT Are Overrated\n00:43:22 AlphaFold's Shock: When First Principles Lost to Machine Learning\n00:46:25 Enumeration and Filtration: How AI Scientists Generate Hypotheses\n00:48:15 CBRN Safety and Dual-Use AI: Lessons from Red Teaming\n01:00:40 The Future of Chemistry is Language: Multimodal Debate\n01:08:15 Ether Zero: The Hilarious Reward Hacking Adventures\n01:10:12 Will Scientists Be Displaced? Jevons Paradox and Infinite Discovery\n01:13:46 Cosmos in Practice: Open Access and Enterprise Partnerships", "transcript_html": "<p><strong>[00:00]</strong></p>\n<p>MD was supposed to be the protein folding solution. There is a great counter example. The counterfactual is basically a group called Desha Research. They had, you know, similar funding to Deepmind. Um, probably more actually. They tested the hypothesis to death that MD could fold proteins. They built their own silicon. They built their own clusters. They had them taped out all themselves. They burned into the silicon the algorithms to run MD. They ran MD at huge speeds, huge scales. I remember David Shaw came to a conference once on</p>\n<p>MD and he flew in by helicopter and was like to this pretty famous guy, kind of rich >> and um he he gave an amazing presentation about the special computers and special room and out outside of Time Square and like what they can do with it. It was beautiful, amazing. And I always thought that protein folding would be solved by them, but it would require a special machine. And maybe the government would buy like five of these things and we could fold, you know, maybe one protein a day or two proteins a day. And when AlphaFold came out and it's like you can do it in Google Collab, you know, or on a GPU or</p>\n<p>desktop, it was so mind-blowing. I forget like that protein folding was solved. I always thought that was inevitable, but the fact that it was solved and on like your desktop you can do it was just completely floored. Changed everything. >> This is the first episode of the new AI for science podcast on the LAN space network. I'm Brandon. I work on RNA therapeutics using machine learning at atomic AI. >> My name is R.J. Haniki. I'm the co-founder of Mirror Omix where we build spatial transcripttoics AI models. >> The point of this podcast is to bring together AI engineers and scientists or</p>\n<p>bring together the two communities. These are two communities which have been developed independently for quite some time but there's been some attempt to combine them and only now after you know many years are we starting to see some of the big developments start to play out in the real world and start to solve you know key scientific problems. There's no like oneizefits-all solution. You need domain expertise. You need people on both sides of the aisle who can really talk to each other and really work together and understand both the</p>\n<p>modeling and all of the real subtleties of the system you're actually trying to work on. We hope that we can connect these communities and that we can provide a starting point for this new era of AI and science to move forward. >> So without further ado, let's get started on the first podcast. We're really happy to have in the studio today Andrew White, co-founder of Future House and newly formed startup Edison Scientific. Um, rather than introduce him, I'll let him introduce himself. Uh,</p>\n<p>hi, I'm Andrew from San Francisco, former professor now running two startups. Uh, one that's a nonprofit research lab and one that's a for-profit ventureback company. And we're trying to automate science. >> We're going to get into all those points. >> Yeah. I'm really happy to be here. Thanks for having me on. >> I want to know personally about jump from academia to industry and or quasi industry. So like I would love to hear</p>\n<p><strong>[03:01]</strong></p>\n<p>that story. >> Yes. Um I guess like that's the whole story, right? Uh so I did my PhD at University of Washington and uh I worked in a group with um I think 19 people doing experiments and like two people doing simulations and I was working on a topic uh called molecular dynamics um which I think is actually suddenly becoming interesting again as everyone's looking for ways to generate data from first principal simulation and molecular dynamics you know covers uh basically everything that's molecules moving</p>\n<p>around in dynamic systems. So like biology, things like that. Of course, the complement in material science is things like density functional theory where you can model chemical reactions and these like solid systems. So I was working on that. We work on biomeaterials. And so the goal of my my PhD was trying to find what are called non-falling materials. So in biological systems whenever you put like a a foreign object into the body, it will trigger a response. And that response called the foreign body response. Basically it encapsulates it in like this um layer of collagen. This actually is exploited for some implants like if</p>\n<p>you get a heart uh sorry pacemaker installed like it coats it with this collagen so that if you go to change the battery you can almost change the battery out like without even bleeding because like the body has like completely encased it and this is great for pacemakers but for like a glucose sensor or like a you know brain cognitive interface BCI is what they call it now. Yeah. there that's not so great. And so that's why some of those things have like a limited lifetime because eventually your body treats it as like a wound and and heals and >> rejects it. >> Yeah. It's it's kind of like so some</p>\n<p>rejection is like immune based. >> Okay. >> And so that's where like if the body can see anything on it like if it can see like some some um uh lian that it can bind to with antibodies then you get this like inflammation which is like an rejection response you see in organ transplants. with materials the body just like oh there's just like a wound or there's some something here and it just covers it up. >> I think you know the research in that field has gone on a long time since I left my PhD >> and there was a lot of theories about it's related to the mechanical properties of the material like if it's</p>\n<p>spongy there's things like if it's tracular like it's have a bunch of little pores in it. We worked on the theory that it had to do with how hydrophilic the material was. >> Okay. >> But anyway so I was only one working on computers in this group. I couldn't figure out like how to connect what's on the computer with what's done in the lab because you can make like a simulation of whatever 10,000 part 10,000 particles 10,000 atoms. It's like well this is not going to model a human body and an implant a lot more a lot more atoms involved. >> So I had a good time. We did some cool stuff some bopromatic stuff. I learned a</p>\n<p>lot. But then when I did my postto I was like okay we're going to try to merge experiments and and simulations. So I worked on this this theory called um maximum entropy. And it's about like how do you take complex simulations and match them to limited observations? And it's like the inverse of machine learning. Machine learning is like you have simple models. You're making going to a lot of data where I had like complicated models and trying to fit to very little data. >> Yeah. >> It was fine. It's great. We wrote some</p>\n<p><strong>[06:01]</strong></p>\n<p>papers. It was useful. And then I wrote like I started my research group at University of Rochester on applying these methods to model peptides. >> Yeah. >> And I'm always like too early for things. We study peptides for I don't know four or five years. And it was a cool niche field. not that popular. Now, peptides are like the hottest thing ever. I think there's even like a peptide rave I heard about a couple weeks ago. >> But, but when I was an assistant professor, nobody cared about peptides. Um, so we worked a lot on different ways to combine them. We looked at like uh different experimental methods that we</p>\n<p>could do these molecular dynamic simulations of peptides. And then in 2019, I was like out on a sbatical at at UCLA. um they have a place called the Institute of Pure and Applied Mathematics there which is like this institute where people can go and do a sbatical and learn new methods and they were happen to be doing they happen to be doing um machine learning for physics I think the name of it was it was like some like symmetric thing it's like machine learning for physics and physics of machine learning >> okay >> it's kind of a cool cool concept >> but like Yan Lun was there and like um Frank Noi was there who's a big guy in</p>\n<p>Europe in in this field I don't know it's just Terrence Tao came by great group >> and everyone was kind of jamming. It was like 2019 so like there had not really been the big hit um especially in in in uh in non computer science fields >> right >> and then I came back from that I was like well I got to teach class on this writing the book about like how you can apply these methods in chemistry it was very kind of niche field because uh every machine learning class that my PhD students could take at the time this is when I was professor at University of Rochester it was always would end in like okay this is an RNN and this is</p>\n<p>like what you need to know or like this is how you do image classification In chemistry, it's all about graphs, right? It's all about how do you represent these graph structures? It's all about symmetry and geometry. >> Y >> and that was like not a thing was very popular, but you had Maxwelling on before and you know the the godfather of of geometric deep learning. >> Yeah. >> So I wrote this textbook about like this these methods and there was a bunch of interesting mathematics to it. I had a good time and stuff and then um uh I think I was following you know the news in the space and codeex the original</p>\n<p>codeex came out and um I had been looking at transformers for a while I just tinkered with them and we started trying them on doing some chemistry tasks and we were really impressed actually and we wrote a benchmark and this is like 2019 or something we wrote a benchmark of verifiable rewards in 2019 maybe it's 2020 by then but like >> ahead of the curve >> a littlehead of the curve again yeah like here's a here's a function and um Sorry, there's like a task which is like I have a body of a function for like a marov chain Monte Carlo simulation it's missing some pieces complete it and then</p>\n<p>we had like a verifier that would see did is it a valid MCMC simulation we wrote this paper it ended up coming out I think 2021 2022 >> um because it took a long time to bank enough questions >> but I wrote an opinion piece about how transformers could change um how we think about chemistry and things like this and how we teach it and then opening eye some people there um Llama uh was there. She saw this paper and</p>\n<p><strong>[09:03]</strong></p>\n<p>they reached out like, \"Hey, we're building this new model and we think it'd be great to red team it to see like what could happen with these models if they're applied to chemistry or biology.\" And so I was a redteamer for GPT4 and I was using it like nine months or something before release was like August. So GP4 came out in March and I was using in August. >> Yeah. And then like the react and miracle paper came out. Um I think Shenu he wrote that paper and I plugged it in with GP4 like in the fall you know and I was like wow there's so much stuff coming out >> with React. >> Yeah. And it was really exciting. And then so when GP4 came out I released this paper called Chemcrow. I worked</p>\n<p>with Philipe Schaller in in um Switzerland on this and IBM. >> So that was like React applied to chemistry. >> Yeah. And what we had is we had like there was a cloud lab that IBM built in Switzerland. Yeah. So we had like GBD4 operating the cloud lab and then it was like I had written a literature research agent that did like agentic rag. Again nobody knew what agent rag was at the time. Um I think actually uh Harrison Chase had like written a blog post about some ideas there and so I I I stole some of his ideas. Really smart guy. Um and basically we applied that and</p>\n<p>>> we saw some really cool stuff. It was really exciting and then I we wrote the paper. It set off this crazy storm of like everyone was had a lot of anxiety about AI progress. >> Yeah. >> And um I ended up visiting the White House. Um I guess my my paper was like the only time a pre-print or peer review paper was presented to the president on like their schedule for like a 30-minute block. >> Wow. >> And the national security adviser at the time um uh Jake uh God I was confused about</p>\n<p>>> Yeah. No, sorry. One of them is a talk show host and one of them is like the national security adviser. I forget which which is which. >> Um >> that guy. >> That guy. Yeah. He like had a presentation about our our our paper and they presented to all because there was a big tech CEO summit at this time where they sent out Sam Alman and some other CEOs out there and they >> this is the future of chemistry as language or a different one. >> This is the Chem Crow paper. Sorry. I probably should name these things. >> And and so >> it was crazy and they had me go out there and then I I met a lot of threeletter agencies I didn't really want to meet. And um</p>\n<p>>> and like you know this like some somebody from uh one threeletter agencies like how does this change explosives? You know the three liter agencies like how does it change breakout time for nuclear weapons research of >> I was like Kai is like I don't >> I'm not really sure and but it turns out that like you know there's not that many people world experts on on on AI and science, >> right? So what's the answer? Uh yeah, agree. Um good question. We'll come back to that. Uh >> okay. Yeah, let's come back to that. >> In the end, um I I you know had a lot of energy, a lot a lot of excitement about</p>\n<p>this area. So took a sbatical from University of Rochester. It was Sam Rodriguez and um Sam had been talking to Eric Schmidt and Tom Khalil who was um also National Security Council on the Obama administration about how to uh uh like scale up these ideas. And so Sam had this concept of like focused research organizations which was how do you do science like not in academia, not in in a um in like one of these kind of</p>\n<p><strong>[12:05]</strong></p>\n<p>near monopoly tech companies, these big labs >> and uh wanted to try this idea out and I was like hey we should do this around agents for science or AI for science. >> I love Sam. He pushes me to come up you know with really >> lofty ambitions. So we decided to automate science as the goal instead of like see what fun stuff we could do with agents in science. But I think that was maybe the real mission. But of course, automating science is the the long-term mission. >> Yes. >> And so that was what led to future house. >> That was a very long- winded. >> Yeah. No, no, that's great. So, but so you chose to leave a tenure track position.</p>\n<p>>> I was on sbatical, which is a a beautiful concept. Um, but then I did resign my tenure position when we co-founded Edison. >> Yeah. Okay. >> And it was, you know, I had been on sabbatical for a very long period of time and so at a certain point I just had to resign my tenure. So I resigned tenure in June. Okay. >> Oh, so that's only recently. >> Yeah. Only recently. >> Yeah. And you just felt like this is this is direction of my career. I >> Yeah. Yeah. I mean, I got tenure and I had these early career awards like the NSF career award. It was great. And I</p>\n<p>think academia is really exciting. But um I just thought that right now these this kind of area like a ever science is um just I think a difficult to do in academia and b so exciting that I think you can take bigger bets >> and um I think having a tenure position and writing research grants is maybe not the biggest bet you can take on on a field. >> Yeah. So now we have a ventureback startup called Edison, right? >> Which we spun out of Future House and you know we took a lot of the ideas and and we're trying to do this at an even</p>\n<p>bigger scale right now. >> Yeah. >> And and so Edison was always kind of the plan like going back to Sam's idea of a FRO or like was it fundamental research organization like he he always had this goal of like let's do fundamental research and this like tightly scoped nonprofit which can kind of explore and then you have that as a natural arm for spinning off. Yeah. >> Um, you know, venture back. >> Yeah, I think that's right. Um, I think some things that like make that not as clean these days is how expensive AI</p>\n<p>research is and how expensive GPUs are. So, I don't think we can repeat it many times from future house. It might be like an end of one thing right now. It just may maybe not. I don't know if venture capital keeps growing then maybe maybe we can. But yeah, I think we took a lot of the ideas of future. Another thing like I think we expected it to be harder to automate science >> and actually it's really hard like I'm I'm feel like I'm always miscalibrated in this domain but it's always hard to predict progress. >> Yeah. >> Um and I think that I overestimate the speed of things on month scale and I</p>\n<p>underestimate things on year scale. So the two years from like 2023 to 2025 was an enormous amount of progress. Yeah. Um it always felt like things were not going as fast as I thought, but when you look back on it, wow, like there's a lot of progress. And so I think the idea that I think in future house and our Sam actually regrets us writing this, but in the the original marketing or like the announcement is like it's our 10-year mission automated science and like now</p>\n<p><strong>[15:05]</strong></p>\n<p>it's like okay yeah. So two years later we we had Cosmos and um things are going so much faster and it also this kind of this this thing you notice it in um in in San Francisco. is like it's actually kind of hard to find problems which are like so hard that like they are a challenge for language models but not so hard that they're impossible and this like gray zone and actually I feel like that's where we are right now is that we can actually automate so much of the scientific method because it turns out especially in a field like biology which is very empirical limited you know the top 1% guesser of know what what they</p>\n<p>think will happen in an experiment in the you know the top you know quintile or cortile they're about equal and so you know even if you had an If we wait 10 years and get even smarter models, I don't think it's going to really change the fact that we're ready to automate a lot of science with with existing LLMs. >> You mean what do you mean by automate science? That's like a pretty loaded state. These lots of things like there's way many ways of thinking about that. >> So we try to draw a line between um what I would call uh uh groups that are trying to model something like the cell or how proteins fold or like how</p>\n<p>antibodies can be designed or like maybe the virtual cells like an example. If they're trying to use machine learning or AI to model some very specific system, >> we're trying to automate the like cognitive process of scientific discovery, making hypotheses, choosing experiments to do, analyzing the results of experiments and using it to update your hypothesis or your confidence in those hypotheses and then leading to like a world model which of like okay this is how I understand this process to be and then that you know begets new hypotheses or new experiments. So we want to automate that sort of loop. We</p>\n<p>thought that we would have to build up like a whole new organization from the ground up for agents. So it means like automated labs, it means like putting all the papers in one spot, like getting APIs wrapped around everything. But um over time like the models have gotten better and better that we had to like you know stop and rethink okay we don't actually have to hold their hands so much anymore or like they don't actually need to necessarily have an automated lab. They can like write an email to a CRO or something or they can like tell you what experiment to do and you can take a video of you doing it and then show it to the model and they can be like okay well this is you know what happened. So it's been a really</p>\n<p>interesting experience of like sometimes you know we overengineer things and sometimes uh uh actually basically just mostly overengineer. So >> so I always think about systems and scientific is a system like scientific process as a system. I always think of it in terms of constraints, right? And like what is a bottleneck in the system? And so that so what is your hypothesis about this right like in my mind not knowing a ton but in my mind the constraint of the scientific process is the work you do in the lab and that's</p>\n<p>sort of notably missing from well not entirely you said auto you mentioned automating lab and whatever so like how are you thinking about this? Yeah, I think you're right is that uh basically the the best model um whatever opus 7 or GPT10 like on yeah >> it really can only propose the first experiment maybe slightly more clever but at a certain point you just need information right like some little</p>\n<p><strong>[18:06]</strong></p>\n<p>calculations you can do that like there's more atoms in the brain than you could ever simulate even if you had all the energy from the sun right like I think you simulate maybe a thousand brains in real time with all the energy in the sun because there just too much information yeah >> so science really hits these like bottlenecks where you just actually have to go measure things. >> Y >> we definitely think about maybe like lab in the loop sort of situations like one of our papers which was called Robin is that we like had an one of our agents proposed an experiment. We did the experiment and then we had our agent analyze the experiment that proposed the next experiment and that kind of loop I think is where you want to get to.</p>\n<p>>> Yeah. >> So what is the bottleneck in that? I don't think it's like the intelligence of the first experiment. I think the bottleneck might be something like right now. I think the bottleneck is something silly like knowing what's the lead time on all the reagents that you need and what is available in the lab, right? Like you know. >> Yeah. Yeah. Yeah. >> I think whether GPT 5.2 Codeex Max or Opus 4.5 is going to do better is probably doesn't matter. It's just a matter of like which one's going to have all the information of what's in the lab and how much will it cost, how long will it take, >> right? Um and also I guess the the kind of frontier that I think about for these</p>\n<p>models is taste >> which is like a lot of science I mean of course we want to you know accelerate technology we want to improve the economy we want to improve people's life expecties want everyone to be happier but a lot of what is done in science is is based around like human preferences like why do people study I don't know a particular worm well like there is a theory that by studying the worm it has led to good medicines or it's led to discovering new genes But also people studied in the past. People's careers depend on that worm and people want to</p>\n<p>write papers about that worm. And so there's a human element to some of this and I think the u models don't capture that so well about knowing what is an exciting result and what is a boring result. >> I see. >> So I think that's like scientific taste. It's like a broad category of all these things. >> How do you def like do you try to quantify taste in any way? I mean I know that I I have some like fun anecdotes about this but maybe Yeah. just like to hear what you thought. >> Yeah, actually we sat on this idea sat on it, but we like argued about it for a</p>\n<p>long time. Sam and I usually every Monday morning at 8:00 in the morning, Sam and I meet and we're both, you know, caffeinated and ready and we argue about stuff like this. And we had a lot of Mondays where we talked about scientific taste. >> And in the end, we're like, okay, let's just let's just do the dumbest thing, which is to like have our agents make hypotheses and put them in front of humans and have them be like, I like this one or I like that one. Right? So we just did like whatever RHF on on on hypothesis and um we learned a lot about how bad RHF is with people just like people pay really attention to to the</p>\n<p>tone to the details to like how many specific facts or figures from the hypothesis right like actionability about like if the experiment is feasible but what people didn't really pay attention to is like I don't know how to describe this like if this hypothesis is true how does it change the world if the hypothesis is false how does it change the world this like um how much information you gain. It's not really information but like impact or something >> and that really didn't come through from</p>\n<p><strong>[21:06]</strong></p>\n<p>those things. So then we're like okay well this is maybe one strategy and so we had to go back and think about it more and um we then took a pause from that research and then we made cosmos and then cosmos like has baked into it taste right like at the end of the day there will be some report and we're working on generalizing this basically at the end of like I made these discoveries and a person be like great I want to download that one or I like that one right I don't like this one and that rolls up to some hypothesis that came earlier in the process and so we think we can get to end to end on as opposed to human preferences.</p>\n<p>>> So you mean the feedback loop is the click? >> It could be the click. It could be also like the you know it we do an experiment. Sometimes in cosmos you could ask to end an experiment. You go see was experiment a success or failure or something like but I guess like we we've brought it out of this kind of like hard to quantify is this a good hypothesis or a bad hypothesis and into this like you can see some downstream consequences of the hypothesis. So yeah, humans have I think a very like strongly well-c calibrated nose for science. Like I mean maybe you could argue there</p>\n<p>sociological effects like um across the community, but ultimately often times people really like good scientists know right off the bat like is this going to be likely to be um useful or not. How long how many attempts did it take before you started to like see results that to yourself seemed useful? Like you've been working on this for I guess two years now. Um you know I think when the co-scientist paper came out from Google I think it was a really interesting idea to do this like</p>\n<p>tournament style or just pair wise ranking of hypotheses right so I think cosign is very interesting counter example to what we built is that what we built is something with either lab in the loop or data analysis in the loop or literature research in the loop where you're like iterating on an idea I think co-scientists took this like very different approach of like let's list all the ideas and then try to come up with a filtration process to come up with the best hypothe hypothesis. So co-scientists will produce these very long reports of like oh we like really tested this idea with lots of dialogue and and and it was very interesting</p>\n<p>stuff and I was really impressed with the paper that came out and then we had this Robin paper and one of the things that came out of the robin paper is that the hypothesis that people thought was best was not the one that led to success in that paper. >> Interesting. >> It was in um age related macular degeneration or ocular uh age related macular generation. Basically, it's like part of the eye is you're going blind because you have this like accumulation of debris in the eye and can't clear it out. >> That's the major cause of blindness in people over >> Yeah. >> Uh 60. Ali who works on the hill. >> Yeah. Yeah. He'll cringe when he hears</p>\n<p>me say that, but >> something like that. >> Something like that. Sorry, Ollie. Um in that one like we went to optometrists or opthalmologists get confused on that as well. Sorry, Ollie. Um but essentially, you know, asked them what hypothesis do you think are are good hypotheses? which you think would lead to like um a good mechanism for treating dry AMD. >> Yeah. >> And um yeah, there was you know they agreed to be on the top 10 but but</p>\n<p><strong>[24:07]</strong></p>\n<p>beyond that it was it was kind of noise. >> Yeah. >> Um and then you know the the what we found was ribbudal was a very good um medicine and it had a mechanism that I think is novel although there was lots of debate on X because in I think in 2012 there was a master's thesis which proposed this mechanism on like page 38. I actually think it was a typo. I think they meant wet wet AMD. But anyway, I won't I won't belager the point. I will concede that it maybe was there was one reported uh example of it in the past. That was a really eye opening experience</p>\n<p>for me because that was a the first really serious test where we really went to the lab and we you know spent like four weeks on a battery of experiments to see what is which hypothesis led to a good mechanism and a good repurposed drug. >> Right. >> And it was not as correlated with human >> opinions as I expected. >> Yeah. Yeah. And so since then I think that uh I have a lot more faith in these like um verifier in the loop kind of scenarios where you have either data analysis, literature search or you're running a unit test or whatever you you you're going and running the experiment.</p>\n<p>Anything like that I think is is going to give you a higher signal than the sort of vagaries of like oh this is a higher opinion or we like this one better. >> Yeah. Max Maxwellian called it nature's computer. >> Yeah. It's like it's like like you have this computer cycle you're running and the nature is part of that computational >> cycle. I'm curious. So you said that there is a paper which maybe like could propose maybe propose where this um molecule came from but like do you have some way of interpreting or like understanding where that hypothesis</p>\n<p>originated in the absence of that? >> Like is there a little thought train? >> Yeah. Yeah. Yeah. Actually, this something we pay really close attention to at Future House and at Edison was um Providence of like information. So, our first sort of like agent was was paper QA. Sorry about the name. Paper QA sounds like an email 7 that was an agent. >> It really does. >> Yeah. Um paper QA was uh has like every sentence that it outputs has a citation to a page, right? So, it's like a lot of providence and then we basically built along the philosophy for everything. So</p>\n<p>Robin would which is the name of this I don't know workflow or something you can call it that led to this result on riputal being a good um therapeutic for dry MD >> it has like data analysis that goes shows you like which line Python code led to the result here and then that is like okay then goes to this other model which says well based on this literature finding and this result from the data analysis I believe this is the right thing but you know where does the original idea come from like going after these rock inhibitors which is the the mechanism for the the target was</p>\n<p>basically enumeration and so this is like if you can't be smarter you can be I don't know you can try more times of course >> and I think that was like the theory of of the Robin paper was that we can put out a whole bunch of hypotheses and then we can filter them just like I think somebody how co-signist did is you go for a filtration process but the difference is that in co-scientist their filtration process was other LLM sort of ranking it with rubrics or like personas</p>\n<p><strong>[27:08]</strong></p>\n<p>and our filtration process was like literature search and data analysis this um like here's some data. Is it consistent with the data? Go see if anyone's discovered in the paper in the literature or if they've disproven it. >> And I think that's the easy way to succeed in in AI over humans is is you can try more ideas faster. >> Something I've I've heard people say and maybe I've experienced this in my own life. Um like sometimes hypotheses are kind of cheap especially in you know biology >> it's many ways actually easy to come up with what you think could be happening. And um it seems like to me verifying is</p>\n<p>often times a big bottleneck in maybe the biggest bottleneck like if you have lots of hypotheses you know and it costs you know 1/100th of your runway to test each one of them or something you don't have any shots on goal. >> Yeah. >> Yeah. So how do you make sure that like you you are actually enriching for good hypothesis >> literature and data analysis right you know like do >> there was a time when we used um something called tiling trees and tiling tree is like a literal brute force method invented by Ed Bon Sam's PhD</p>\n<p>advisor and basically the idea is okay I want to like accomplish X okay I could try these methods and then like once you pick I'm going to try this method then you split into like two different paths I'm going to use this method or not use this method. I'm using this method. I need to have like I don't know some kind of substrate. I'm going to try this substrate or this substrate or this substrate, right? And you can basically try to really like tile space of all the we tried some early experiments there and you're right, you run into this thing where some of the hypotheses come out just don't make any sense and like you are going to waste a ton of effort if you actually test them all. Nowadays,</p>\n<p>I actually would argue that if you go to an LLM and you ask it to evaluate, you know, hypotheses, including some garbage ones, it will probably do as good of a job as an expert in the field and and filtering them out. That's not always the case. >> Yeah, I've actually seen that myself. >> Yeah. But there's a lot of gotchas, and I think people can miss those, but I think they're actually pretty good. And so, I'm not as worried about >> hypotheses that can fail fast by an expert looking at them. >> I think now the filtration process really happens in in in literature. And I think the filtration process happens in looking at like um you know like uh</p>\n<p>biioank data or like you know what do we know from GW was or something know other sources of of existing data as much as you can draw upon. >> Yeah. So yeah and with regards to existing data um another like maybe contrarian uh take is that like oftentimes the the hardest part is just understanding the like the context of data and like where it comes from and how do you interpret it. I can also think from my own life multiple cases where you know the the data in some sense was like there and you had two</p>\n<p>people who were both experts and very smart people who looked at it and drew very different interpretations and and in fact like when we were interviewing Heather Kulick she she had some fun stories about using LLM and um she would find that there would be raw data in a paper which wouldn't agree with the conclusions of the actual paper and it's straight from the paper it's not even like cross paper talk or something.</p>\n<p><strong>[30:08]</strong></p>\n<p>>> Man, I'm going to be a really boring interviewer and be like, \"Yes, you're right.\" You know, like this is this is a hard question. >> Um I think, you know, to give you something concrete, um >> we have a a bioinformatics benchmark we call it big bench. And big bench is like we put it out, we've updated a few times. It's in it's in some Frontier um LLMs when they release their system card, they'll mention Big Bench. It's like one of the things they test on. >> Yeah. And um you know we're getting to 60% 70% correctness on Bixbench >> and um we found that actually we're at</p>\n<p>the point where humans disagree at this level. Like humans only agree 70% of the analysis. And so it's true that like when it comes to analyzing data like humans do not agree 100% of the time. There is a certain amount of like choice that goes into it. And you know we we try to um so Edison is is a for-profit company. maybe like trying to sell uh some of this stuff to to companies and we'll go to some companies like oh we never impute data imputing data is bad like or you know whatever and like okay</p>\n<p>well we'll have to change our agent so we don't impute data with them but then some other companies like oh yeah we impute data it makes everything easier right or uh and and you know you want to know what the real modern dark arts are that like AI resistant area of the world is like medicinal chemistry that is like the spot where like you know there's so much superstition >> oh yeah yeah everyone yeah everyone is like pseudo religious >> yeah exactly But you have to be to survive. I feel otherwise you get burnt out. >> But the religions never agree too. Two medicinal chemists will have completely different viewpoints about like a functional group. >> Yes. Exactly. And I remember this is</p>\n<p>talking to somebody who worked at CRO and they're like, \"Oh, whenever like company X orders anything, we never put boron on any of the compounds because they hate boron because there was one program that was killed because there was a boron, you know, somewhere in the core and it led to some toxic side effect. So no boron for this company. this company, they like love things to be florinated or something because they love think it's great for um the admit properties, right? And so there's like all this this this stuff where you reach the point where I don't know human bias level or human disagreement level and I think we're getting to that point in</p>\n<p>data analysis. And so of course you will see then that if I take the broad data from a paper and I analyze it myself, I will get a different conclusion. One of the cool tricks you can do is this back to this brute force thing is that I can go to our agent and I can run it 100 times and I can take the consensus like analysis or I can say even if you make these three different choices in your data analysis you get the same conclusion right or this conclusion is somehow sensitive to those choices and then you can there's even like words like epistemic versus alitor uncertainty right it's like this is alitor which means like I think it's noise from the</p>\n<p>data or this is epistemic uncertainty which means like I think there's some choices that are being made there's some differences that lead to the the disagreement. Anyway, there's like there's like a Donald Rumsfeld formulation of this as well, like the known unknowns and yeah, the alitor epistmic uh uh debate there. >> Interesting. This kind of digging into your cosmos a little bit. So I I glanced at the paper and one of the things that jumps out is that there were certain</p>\n<p><strong>[33:10]</strong></p>\n<p>class of problems for which uh it was only 50ome% accurate and oh yeah and can you talk a little bit about that and how that like okay so if I'm just raw getting 50% accurate answers and and then I'm going into the wet lab and being like okay try this and then it's like ah like the the stupid thing did told me to do a dump like how do you >> I would say first of all that 50% it's actually pretty good because it's rare that experiments in the lab are actually coin tosses, right? They're usually a lot more outcomes than >> you know than than binary. >> Yeah. Yeah. Sure. Okay. Yeah.</p>\n<p>>> But but that particular number was uh human agreement in the interpretation of the results. Okay. >> And so we asked people to evaluate different aspects of Cosmos. We had them evaluate like the data analysis decisions. We had people ask to evaluate the literature like is these is do you agree with its finding in the literature? that number that was 50% that came from Cosmos's interpretation of uh some of the analysis. >> Yeah. >> So like it might go in literature and find this result and then it would say wow this is super exciting this is amazing or it might do data analysis</p>\n<p>like this is a novel discovery really excited about it. >> Yeah. >> And then people would disagree that's actually not interesting or like I don't agree with the interpretation of it. >> So it's like picking bad problems maybe. >> Yeah. >> In the the the negative class. Yeah. And so I think it's like that that 52 or 55 whatever it is that's um interpretation. And so I agree. I think that's where like I was saying I think the frontier right now is scientific taste. Yeah. >> And so that's what we're working on right now is how do you get that interpretation to to match you step back and just introduce Cosmos from a high level. Yeah. Yeah. Um</p>\n<p>>> I would actually be in even curious to hear starting from like Chem Crow and uh you know you have uh paper QA Avery Ether Zero. Yeah. Yeah, >> I'd like to hear a little bit of the the lineage and how those different decisions were made. What were the key learnings and how did you get to where you are now? >> Yeah. So, I could retcon and tell a really great story about how we arrived at Cosmos, but I will say that like to a large extent we just try a lot of stuff and sometimes it works and sometimes it doesn't. >> Okay.</p>\n<p>>> You know, I I'll say that >> we're very I'm I'm a I'm a builder. Like I like to like build things piece by piece. I'm uh probably some fancy word for it, but I'm like a Lego guy or something. My vision was that we would make an agent that does this part of the scientific process, an agent that does this part of the scientific process, whatever. And so we had like, you know, um, Chem Crow, which is going to help us with setting up our medicinal chemistry work. We had protein crow, which we haven't released. I don't know if we will ever release, but protein crow is like designing proteins we might need for for some part of our workflows. Um, or we had a data</p>\n<p>>> or that's a >> it's an agent. So LM plus tools. >> Okay. or we had ether zero was like okay we noticed that the frontier models can't work with molecules very well so let's make a a model with intuition for medicinal chemistry and that was what led to ether zero but then Sam actually really pushed on us to like let's just even do the whole thing you know let's just try to build an AI scientist let's just try the whole thing and that was what led to Robin and um Robin was like let's just take these agents we already have and we'll just put them in like a a workflow basically was like you could</p>\n<p><strong>[36:11]</strong></p>\n<p>express it in a concise Python file of like you know try a whole bunch of ideas then go see if they all filter through literature or if they've been disproven and then go like uh come up with experiments that you could do in a wet lab. >> Yeah. >> And this is our inventory list and then go analyze all the data then go back and repeat the process. Right. So that's like what Robin was and um then we like we came ac across Cosmos we're trying to like understand what is the process that Robin is is automating and it came from this idea of like a world model which is that when we first started Edison we were thinking like what do we what do we</p>\n<p>want to change about this like what is new here >> and so we spent some time thinking about well the scientific process like what is actually going on like my brain which is that I have some understanding of of the world or the phenomena I've studying and that's my world model and then a lot of the actions I take are about trying update that world model and it's something that changes over time and so this is like this ability to change over time but it's also something that is practical like I can use it to make predictions about I know from this experiment this will happen that's why it's like a model and not just like you know uh uh memory or like a bunch of</p>\n<p>like papers or something like that it's like it's supposed to operate in cosmos we tried this idea out and actually um uh uh Ludo uh who was the the uh first author on the paper we tried a whole bunch of ideas around world models And uh we kind of thought they weren't really appropriate like well we tried a lot of different ways to do this. We tried you know method A, method B, method C and and they were okay. And so we all decided to take a break. Ludo like his project didn't work on trying to do this world model stuff. He's like I'm going to keep trying it. Lud is very</p>\n<p>stubborn person. So he tried it for like I don't know a week or two weeks and he was kind of like quietly like hey can you guys come take a look at this >> and and we're like wow this is actually really cool and then we like started building on it and jamming really. And I think what Lud figured out is that you have to get this like experiment the loop thing. You have to be let it and the data analysis agent is what got us in the loop. So if you put that in the loop of like it can really update this world model because the we were trying to build it around literature before >> and when you build it around literature there's just like not really experiments you can do and then see the results for that was like our surrogate was was literature. It just wasn't working. Data</p>\n<p>analysis actually really lets you explore ideas >> and so that was what led to cosmos. And so in cosmos we basically we had all the pieces sitting around. We working on world models. We working on a data analysis agent, working on a literature agent. Um, and then we're working on, you know, we built a platform for scientific agents. So, we had things that can write a law tech report. We had things that can make nice plots. >> Then we put that all together and like a world model was like sort of the the glue that allowed it to to fit together. Yeah. >> An analogy is like um >> in coding agents like GitHub is sort of the glue. like there's some shared repo</p>\n<p>and everyone works on the repo and like software engineers have spent whatever lots of brain cycles thinking about what's the way to coordinate you know and organize working on code together for a long time. >> So the world model is actually like a memory system kind of. >> Yeah, you can think of it as a memory system. Um we we think about it as as a model. So like it actually you can put in input and it will output predictions and we think about calibration. >> But like really it is a set of like a big >> bundle of information that we accumulate</p>\n<p><strong>[39:13]</strong></p>\n<p>over time that's distilled in some way and and that is like uh what allows us to do this. And I think you can think about like um a GitHub repo is like it's a distillation right like really there's a long graph of commits that lead up to it and like the current file system in that GitHub repo or should keep saying GitHub. I'm such a corporate >> shill here. get your git repo is like a distillation of all of the work that people have put in into the PRs into the into the uh the commits and so I think there's a nice analogy uh between a git repo and what a world model is. I see. And</p>\n<p>>> I think that's just sort of what allows us to automate scientific discovery is so well. >> Can you talk about like kind of how you implement a world model or is that sort of like secret sauce? >> That's our like secret sauce right now, you know? That's fine. >> Yeah. No, it's fine. People have asked around. >> So, one thing that's notably missing is the like simulation, right? Dynamics or or or like uh bolts or >> Yeah, I want to I'll help you guys pump up your views here. So I I think molecular dynamics is overrated. In fact,</p>\n<p>>> coming from someone who >> goes in the that goes in the in the thumbnail, you know. Yeah. >> But just >> Yeah. And and and DFT is overrated. In fact, DFT may be even more overrated than like the NEMICS. I think these method >> for materials or for biology or for both >> for materials. >> Okay. >> And I can explain more about that. Basically MD and DFT have consumed an enormous number of PhDs and scientific careers at the alter of you know the beauty of the simulation. >> Also random interjection once I I did an estimate I think pre like chatbt something like 20% of the world's</p>\n<p>computing power just went to simulating water. >> Oh my [\u00a0__\u00a0] god water. >> Yeah. >> I had to deal with so many water simulations. I did I did DFT simulations of water and they are so annoying. I used these big computers uh from uh the department of defense and we I spent like I don't know five months and by the way this is prel training days five months of compute is actually a really long time >> I simulated water with quantum you know effects with a grotest mechanism for how a proton hops through water and it's on</p>\n<p>YouTube it's my number one YouTube video and it represents like >> until now and it represents like I don't know a million CPU hours of compute it was you know one of the biggest computes that I probably the biggest one I've done in my life so far. Maybe Ether Zero is bigger, but but it took a lot more work anyway. And and what's the point? What did you learn? >> All I learned was like what set of hyperparameters reproduce some physical effects of water. But none of it was denovo, right? And this is the this is the issue with with molecular dynamics and DFT is that um they don't model the</p>\n<p>world correctly. And so we have to invent little stories we tell ourselves about we're like making good inductive biases and then it models the world more correctly. Like in DFT you simulate water at 330 Kelvin when you want room temperature water. >> Is room temperature 330 Kelvin? >> No it's not. That's a little too hot. Right? And so this is a the issue is that like people just make up these</p>\n<p><strong>[42:15]</strong></p>\n<p>these these things or like I don't know GGA or like BIP or B3 lip all these different like methods people. are clearly empirical and then they bolt it on to DFT and they say look >> it's a first principles method right but actually you made a whole bunch of choices and you know you you whatever overfitit to the validation data to get this to work and and that's I think MD and DFT are like that because >> if you go look at the catalysts you know what catalysts change the world none of them are single crystal materials that are really well suited for DFT they're always like they have grain boundaries</p>\n<p>they have dopins they're complicated right and and you never capture with DFT. So I think this is one of the fundamental I don't know dichotoies of the world is that simulations simulate really boring things really well. They don't simulate interesting things very well. And so that's why I don't do DFT and MD anymore. What about somewhere like the machine learning stuff like alpha fold and and >> alpha was trained on x-ray crystalallography data and I think you know this is the this is the story of MD is that MD was supposed to be the</p>\n<p>protein folding solution there is a great counter example there's a I don't know there's a word the counterfactual is basically a group called DESR dehaw research they had you know similar funding to deep mind um probably more actually they tested the hypothesis to death that MD could fold proteins. They built their own silicon. They built their own clusters. They had them taped out all themselves. They burned into the silicon the algorithms to run MD. They ran MD at huge speeds, huge scales. >> Yeah. I remember David Shaw came to a</p>\n<p>conference once on MD and he flew in by helicopter and was like to this this pretty famous guy kind of rich >> and um he he gave a >> an amazing presentation about the special computers and special room and out outside of time square and like what they can do with it. Beautiful, amazing. And I always thought that protein folding would be solved by them, but it would require a special machine. Maybe the government would buy like five of these things and we could fold, you know, maybe one protein a day or two proteins a day. >> And when AlphaFold came out and it's like you can do it in Google Collab, you</p>\n<p>know, or on a GP or desktop, it was so mind-blowing. I forget like that protein folding was solved. I always thought that was inevitable, but the fact that it was solved and on like your desktop you can do it was just completely floored. Changed everything. This is like the the bitter lesson on steroids. >> Yeah. I don't even know what it is, but it's like imagine Chad GBT came out, but instead it was like, oh, you can just run it on your phone or locally on your own desktop. Like that's the level of like shock that came out. >> And it gets down to this thing that humans are really bad at estimating problems that aren't humanmade problems. Protein folding we all thought was like</p>\n<p>would require a huge amount of compute, very challenging problem, the most hardest problem in the world, right? And it turns out that you can actually do it on I don't I think the numbers are now like 10,000 GPU hours. You can train a a good protein folding model. It's actually turned out to be barely an inconvenience. >> Therefore, why not? >> Oh. Oh. Therefore, protein folding was highly efficient based on experimental data. It they took X-ray crystalography. That's what Deepmind did is they took</p>\n<p><strong>[45:15]</strong></p>\n<p>X-ray crystalraphy data. Desire tried the first principles method >> and it's like a nice head-to-head comparison. Two very well resourced groups. They both tried different ideas >> and the machine learning on experimental data >> beat out first principal simulation by >> you know >> a very large margin. And so why isn't like bolts or whatever inside of Cosmos? Like why isn't there a tool that's that can run? >> Oh, we have bolts inside of we have bolts gen. Yeah. Yeah, we have that inside of Cosmos. >> Okay. It is >> I mean I think in the version that we have uh for people to just sign up and</p>\n<p>use. It's not in there >> but like uh you know you can imagine that you can just modal or lambda or tamarind or 310. There's all these companies that basically wrap a lot of these these like um uh deep learning protein design tools or chemistry design tools. wrap them in an API. You just give that to to >> give it to cloud code if you want. You can give it to Cosmos and you can be like, hey, >> you know, if you want to design a protein for X, use these tools. >> Your mechanism, it sounds like or one of the primary mechanisms that has been successful is like it like enumerate a whole bunch of possibilities and filter,</p>\n<p>right? And so how do you think about serendipity and out of out of distribution thinking and getting there and how far have you gotten and what's left? >> That's a great question. I think I guess the the short answer is that there's very so so this is the domain of seaborn so chemical biological radiological nuclear um uh weapons or I don't know safety >> this domain has been explored a lot in history by a lot of organizations >> and um I would say that >> there was a big question mark for us a few years ago was like how much of this stuff is uh intellectually bottlenecked</p>\n<p>>> like how often are people like oh wow I want to cause harm Um, but I need to know like some facts and could LLM's make that easier or go faster or anything like that. I think you know the first set of answers in 2023 I think was basically no is that like you know you can go find the synthesis route for many dangerous compounds on Wikipedia. >> People know what are the targets in the human body that like are are targeted by most biological weapons. It's it's not</p>\n<p>really that much of a mystery. So I don't think there was a lot of like um there's a lot of new ground when LLM first came about. Then there's a lot of concern about like laboratory protocols is that could agents or LLM uh reveal some tacet knowledge that like maybe people couldn't find in Wikipedia or like maybe for making something there's some technique that is required when you scale it up in size or something or maybe there's like some way to get around like tracking lists by ordering different compounds or >> so and that I think was really well tested. um by a few different labs, not</p>\n<p>not me, but there's some groups that spun up that started making like tests for this and it and labs pay attention to I think it's really been put into process where LLMs will like kind of shut down or be filtered in those scenarios, but I think that is actually an area where there is is some risk. Um and so I think this something that people pay attention to for open source models and there's still I think some some discussion there, but I think to a</p>\n<p><strong>[48:15]</strong></p>\n<p>large extent it's it's not really been greatly accelerating in practice or at least I haven't seen much evidence of it. Um, and again, I think it comes down to the fact that it's not really available, but like you if you look hard enough, you can find most of the information you would need to to get up to no good. >> Yeah. >> In the public domain already. But then I think now is the the next frontier is like uh can it somehow help you with real-time protocols, troubleshooting like more in the loop and um and and more especially in the computational side of things. there are some scenarios</p>\n<p>that are now coming into focus that could be more dangerous or more intellectually bottlenecked and so I think people are trying to pay attention to that to some extent there was like a first wave that we thought this could unlock a lot of stuff and I don't think it came to pass I think there's now an emerging sort of second wave of like there are some actually new scenarios that were just too farfetched to consider two years ago that I think are now realistic um some smart people are paying attention to it but I don't think it's solved yet >> I don't It's very vague. No,</p>\n<p>>> I mean, so I guess like one kind of differentiator, there's a lot of talk about AI safety in like the modern LLM, you know, ASI space and, you know, there it's jokes about or pay-per-click maxing robots or something. But like the the core threat here is more like a malicious actor using this as a tool to accelerate something dangerous. And like kind of the first order hypothesis is that you basically already have to be an expert to effectively create a bioweapon or a chemical weapon >> and a non-expert</p>\n<p>an expert would already know how to do this. >> Yeah. I I think you know so so each of the categories in the CBRN they're all a little different but I think to a large extent it's a a lot of like pushing material around. You know the classical example in nuclear is like it's a lot of lot of centrification lot of ultra centrification a lot of high pressure or high RPMs >> and so it it's just >> you can maybe get smarter about how to set up you know the the economy of scale to do that with an LLM but to a large I</p>\n<p>think you can call your your friend in country X and they can tell you what are the steps it's it's not I don't think it's that much of a secret it's just a lot of like moving material around and I don't think it's acceler meaningfully accelerated. Now, that said, there are all kinds of like, you know, dumb dual use things of like maybe you want to call a company that makes centrifuges and you want to make sure that they sell you them and they go through some KYC steps and maybe an LM can get you through the KYC faster. And that's like a dumb thing that like, okay, like yes, like uh you know, email makes it so that</p>\n<p>you can order centrifuges off the internet more easily. Is email like a dual use technology? Like, yeah, to some extent it is. And so I think there's a lot of like weird second order things that we don't pay attention to in AI safety of like does it make KYC easier? Does it make it easier for people to know like where to order this from or like what is the expected price or like what should you order first, right? All those like sort of simple logistical things I think are accelerated by AI just as like a a consequence of AI being</p>\n<p><strong>[51:17]</strong></p>\n<p>an accelerating technology. Um, but certainly I mean [\u00a0__\u00a0] guys, there's some scary stuff and I try not to think about it too much and uh >> yeah, >> I don't know. I guess I don't want to get too political, but I do think that right now um the the United States government is maybe taking a a slower, less intensive look at safety and um but there's definitely people I think in other spaces than the US government thinking about it hard. >> And do you think is this thing people need to spend more time on? I do get</p>\n<p>waves of angst about AI and I'm sure many people living in San Francisco do get like a little bit of a little bit of waves of it and uh sometimes I think that there isn't enough work being done on it >> and then sometimes I think wow like I need to mellow out and like you know we have lots of time to think about it. What is my opinion on it then? I don't know. I I I think my opinion is um not formed fully. >> Yeah. You and Sam have done a lot of thinking about funding science. and future of science. You have you've been</p>\n<p>vocal about the reproducibility crisis and other things. First question, why this focus research organization or for Yeah. >> From Yeah. What does that get you that you don't get from academia or, you know, big lab or whatever? >> Um, a nice network of of people. I think Edison is like a real uh, of course, I think Edison's going to do great, but I think it's a mystery of what's going to happen. Um, so I don't think we've had as much friction there as you might expect. But yeah, this is all stuff that</p>\n<p>we that that that Sam and I think about all the time is like how do you balance stuff like this? How do you balance the economics? Um, you know, there are some there are some ventureback companies that are having cash salaries over a million dollars. And it's like insane to me. >> Yeah. >> That you would use all of your cash from your equity financing, you know, in these insane salaries. But that can in terms of like total spend on GPUs, it can still be a total a small fraction of your burn. So sometimes it kind of makes sense.</p>\n<p>>> Yeah. Yeah. That's that's one way to think about it. >> So So like you this this is a good uh leadin to you are automating science in some capacity. So where does that leave scientists? So I think um this is uh Jevans paradox we can try here is uh um so uh let me start with a contrast here is that uh you know if we automate um you know taxi cap drivers there's a fi there's not going to be an increase in people needing to go places maybe</p>\n<p>there'll be somewhat an increase but like there is a finite amount of like time people will be spending in cars >> and so there's an upper limit. So when you automate that that's like a scarcity thing is basically you're displacing jobs when you automate driving. >> Yeah. In science, I don't think there is a finite appetite or a finite capacity for science. I don't think science is like a a scarcity thing. Like there's, you know, 100 more discoveries left to be made and then we'll be done. And so like we're displacing jobs. I think</p>\n<p><strong>[54:18]</strong></p>\n<p>instead actually if we can, you know, make science go much much faster. There will be no there will be no decrease in demand. There will be actually I think an increase in demand that will match whatever automation amount we have. And so my vision for what a scientist would be in the future is that they will be I don't know like uh agent wranglers or cosmos wranglers of like okay they're exploring 100 ideas simultaneously or they're like working with systems like ours to to make 10x the discoveries 100x discoveries because I think there's an unlimited amount of scientific discoveries to be made and so there's no</p>\n<p>like scarcity set where basically we will displace them all. No, that's kind of like, you know, this is what I would tell when I go talk to a first year PhD student, like everything's going to be just fine, >> you know. Then when it gets into the nuts and bolts, I I do agree that this is going to be like a really hard thing where like if I am CEO of a company that makes science, like a pharma company or material science company or something like that or R&D arm at IBM, I think, well, I could spend, you know, a million more dollars on on compute for the AI</p>\n<p>scientist or could hire 10 more people. I might just choose to go with the AI scientist because you know to a large extent like hiring people is hard right >> and and hiring an AI scientist is probably a little bit easier. >> Yeah. >> And so I think that there could be some there could be some friction but another thing is like science is in some ways closer to art in the sense that like there is a large number of people just appreciate good science. Like if you get published in nature it's not because</p>\n<p>it's really going to be world changing. Of course, that's part of it, but it's also because like people are like, \"Wow, this is really interesting science.\" >> Yeah. >> So, I think the the enjoyers of science are also scientists. And so, I think that it's kind of hard to imagine a scenario when there's not scientists as the consumers of science. And so, I think if they're going to be consumers of science, they're also going to be some of the producers are involved in the and the process by itself, >> right? >> If that makes any sense, >> yeah, you've touched on this. My the question in my mind is just what does a scientist do? Then >> there's a great short story um by um Ted Chang, I think in like 2003 or something. Okay. And it's about like</p>\n<p>well at first scientists were displaced and they became like the uh interpreters of like what the AI scientists are doing like the scientists read the AI scientists like papers and then you know translate them for whatever popular science or something >> and then after that like they couldn't read the papers anymore and so they were left behind and so they had nothing to do and they just sat around and >> but the problem is that >> science is like you know you you have to translate science to make any impact like science cannot exist by itself. I do agree there's like engineering can</p>\n<p>exist by itself like if you give some kind of system a goal of like making me a material that I can make a space elevator out of. You could be not participating in the beginning the process or the middle of the process and you just come by the end and be like okay follow this recipe like science of like what's the origin of life or like is there water on other planets or you know um why is some catalyst better than another catalyst that has to be hitting human eyes and human brains at some</p>\n<p><strong>[57:18]</strong></p>\n<p>point. So I think a human has to be involved in the process. >> Don't want to be contrarian but >> yeah be contrary. >> Why does a human have to be involved? >> Why does a human has to be involved? Well, a human has to be involved at least some point to be like yes this is good science or this is bad science. >> Okay. So it's it goes back to taste. >> Yeah. But I don't know. Maybe you're right. Maybe there is no point for humans. Maybe it'll be like you know what is it? Sora. Uh you know like the AI slop app. But I think in Sora there's still humans at the end clicking the videos or something. >> Yeah. So, >> so, so the the sorith analogy kind of brings up an interesting point like is it possible that like due to the biases</p>\n<p>of AI science if we really go full in science that >> you know there still is a market for kind of boutique human science like you know there's still people who want to you know paint things the old fashioned way but more to the point does it become even more important for to have a human who is uh actively doing their own exploration because there will be like large blind spots and biases due to the models that just you'll never be able to overcome because this is sort of baked</p>\n<p>in now um due to your training data and without a human that you'll always get stuck in there will be a blind spot that will never >> Bio which is a company in in Oakland um or in Emeryville and they do really cool stuff with automation. I think they're going to be testing this theory of like okay maybe if that's the bottleneck we can see evidence of it because they're going to start doing really well. >> Um it could be true. >> Mhm. I I still though want to say all of those I in my mind are still sort of scoped in terms of like R&D for pharma or bio but they're not like none of them</p>\n<p>are attempting to answer big fundamental questions and maybe there's like different levels when I think about that you seem to be >> um it seems like the future h the the focus of future house in Edison is much more towards like you know sort of R&D and sort of endrun science but um you know I I have some background in, you know, fundamental physics. Um, you know, it's like is there any thought about like how do you like take on, you know, dark matter candidates and like</p>\n<p>>> I just, you know, think the data to really give us a complete story is just not there yet. >> You know what like uh I'm sure everybody at every company like is the biggest critic of their own product, you know? >> Yeah. So we think Cosmos is we think it's great but there's an a very large amount of area for improvement and >> so with Cosmos can so there's like a open like sort of access to everybody version. >> Yeah. >> Do you provide access to other labs that um is less open?</p>\n<p>>> Um we have a version of Cosmos that has like um bigger resources like it can run for longer. It uses GPUs. Um so like basically when it does data analysis it'll have a GPU. So we use that for things like um like machine learning experiments you know if you want to know like this question about whether it's better to pre-train first on noisy data or not. >> Yeah. >> Um we have like pre-release models that that are coming out and we try those.</p>\n<p><strong>[60:18]</strong></p>\n<p>But um >> yeah so I guess like yes we do and we do have like research partnerships with with companies where we like build something specific for them and that is something we think about. >> Yeah. But broadly I would say Cosmos that's on the website is pretty close to to what is the best we have internally. >> Yeah. >> Uh I have a question. Um so you you previously have stated that you think that language is the natural um language. Was it >> language of chemistry? >> The future of chemistry is language. Yeah. Yeah. >> Um okay. So I wonder do you still believe that?</p>\n<p>>> Good question. I think I I would say yes. I still believe that um that so so in that article the opinion article my my point was that uh you know at the time when I wrote that article which I think maybe three years ago now or something maybe 2023 um it was that we have models for predicting solubility of compounds we have like data about very large populations and we have like papers and we have code and and the only way to bridge all that</p>\n<p>information is natural language and and the argument was that like humans is like you know whenever we can't bridge information like if I can't talk about my code or I can't talk about some idea to you I will invent words until I can get the point across right and that humans are always innovating on language to make it represent all known observations and people innovate on language to represent whatever code pattern they have right like this is like the the only shared activity we've been doing for this long is like coming up with words to represent everything we know >> and so I think to that for that reason natural language is the only possible</p>\n<p>way to connect all the different pieces of data we need in biology, medicine or any domain for that matter. Um, I think there's some caveats to this of like, you know, you can make an argument like if Yan Lun were here and he would make an argument about like, you know, world models or like vision or embodiedness, right? Like that there's arguments against natural language that like, you know, that maybe there's something more that it does. It's not the complete story or maybe natural language imposes limitations that you cannot exceed because you're stuck in this abstract space that was invented by humans and you can't escape it until you can like</p>\n<p>touch something. >> Yeah. I mean, it is an abstraction, right? in like like scientists basically work exclusively in abstractions to some degree. Um I I just I find I found that interesting because it seems like most scientists you're right like when they explain things they explain things through language but many conversations maybe most at some point result in people drawing diagrams or something like you know chemistry like biochemistry largely or medicinal chemistry is often times a it's it's a language of graphs right or you know I</p>\n<p>mean bonds are abstractions yes but like they're pretty good abstractions for most ca for many cases >> or like you know geometry you know think about you know protein as like the geometry of a protein >> you know it's like I think that that's how people a lot of scientists like to think about things >> and um so I find it interesting that like yeah that that you are focusing</p>\n<p><strong>[63:20]</strong></p>\n<p>primarily in language like have you thought about essentially a multimodal version of this like where you know when it comes along a smile string it doesn't just say oh this is a smile string but like this is a graph this is a representation of some higher like abstract object. >> You're absolutely right. And and the problem with these this like I don't know Jacob's ladder or something whatever you want to call it is like yes you can say that a you can call a molecule by its name >> you can show the graph >> then if you go to molecule like ferosine well it doesn't really have bonds like part of it and so then you're like well</p>\n<p>we need to draw it visually >> and then you go to molecule like I don't know >> gly betane >> this dihedral angle right and so like it's not actually this thing I drew it's actually an ensemble between this thing and this thing right then you go to benzene you're like Well, not only is it like a ensemble of these different confirmers, it actually has electron density. You can't really ignore the electron density in benzing. You like need to treat it correctly. And it was like, well, you can't actually represent the electron density that way. You actually have to look at the correlation of the electrons individually, right? Because you can't really model benzing with like DFT, right? Or functional. You have to actually look at the the</p>\n<p>electron correlation. Electron correlation like well, you know, you can model correlation, but you know, actually these things when they're in a solution, they have like, you know, relativistic effects because it's like there's a whole bunch of stuff around. So you really got to have the the relativity in there and you're like well you got the relativity and you have the electron correlation you could have the bonds you and you have the commerce but you really need to think about the cosmic radiation background because like you know it does actually impact everything and there is some some energy there right >> and before you know it you've ran out of you know you've ran out of compute or whatever resource you're using to model this</p>\n<p>>> and so I think um you have to draw the line somewhere >> natural language like I said is that humans have worked for a long time to make it be the you know what's the word like the least abstract or the you know it's somewhere on the border of like it's still abstract enough that you don't need to know all these details but it's still granular enough or concretized enough that you actually can make use of it. Um there may be some other representation like multimodal might turn out the video or maybe I don't know there's some other like fusion that you can make. I like</p>\n<p>natural language because we all work really hard to make it right at that boundary. And I do agree sometime sometimes ideas slip and they can't be in language. You have to get out the whiteboard or ideas slip and you have to wave your hands around, you know, or >> maybe then then you need that that uh degree of freedom to communicate. >> Just digging in on this a little bit more like uh famously quantum mechanics is like undescribable, right? Like there's there's an argument that you cannot understand quantum mechanics with words. it or in in with our preconceived</p>\n<p>understanding of the physical world because it doesn't behave like the macroscopic world and so that the only way to understand is through mathematics right um and I largely see language as the joint key of science as well but I wonder if that's not true for many domains and quantum mechanics is just the one that hits you in the face >> I mean I don't know actually I think the</p>\n<p><strong>[66:20]</strong></p>\n<p>there's like seven principles of quantum mechanics or five or something like this that you can actually express pretty concisely in language. >> I agree that like you need to actually look at the consequences of them. You need some mathematics. Um I don't know. I actually I don't know. This is like a challenge. I think you could actually describe a lot of quantum mechanics and language. >> Sure. Sure. >> But but I I see your point and um yeah I I guess uh uh I'm a realist like I when I talk to my kids you know maybe I I will like okay let me draw for you. I don't I don't make sure in our house</p>\n<p>everything is described with natural language. Uh, so I I agree with you there. Um, I think maybe we can be a little a little flexible with with natural language and include equations and smiles strings in it. And I think we can get a little bit farther. Um, so maybe that's okay. Uh, but some people I think like optionality, >> you know, like, oh, it could be this or it could be that. I'm somebody I like to take take strong opinions and see >> how much farther they can get me. And I think in my career, it's actually been better for me to take strong opinions, which in my deepest of hearts, I know</p>\n<p>that are maybe not correct or not fully correct, but once you take these strong opinions, it just you can sort of move many steps down the road. Once you take these strong opinions and like for example at future house, we took the opinion that scientific agents are the future. And that allows you to skip a lot of steps because a lot of other people were like, we need to build a foundation model for X. >> Yeah. >> And we just skipped all that, right? And I think if you also were unopinionated and you had optionality like I can think of a famous example of a different company that like liked the optionality and they wasted a lot of time on foundation models or something then then I think you you get stuck. So that's one</p>\n<p>of my strong opinions is that natural language is a is a way to join all these different domains. >> It may not be a correct opinion. It may not it may be more subtle or more complicated but it's allowed me to get very far. Um I'll drop it someday and maybe find a new one. Yeah. Not yet though. >> That's my ma opinion on the matter. >> The Ether Zero story on your blog I find hilarious and kind of awesome. >> Yeah. >> You know, when I was a kid, I love the like genie/ monkey paw like concept of be careful what you wish for because you</p>\n<p>just might get it. >> Yes. >> Maybe just like quick story. Can can you >> just talk about that? That was that was just a really fun >> Ether Zero was a a hell of a project because conceptually it was a very short project of like hey people have made a lot of progress in verifiable rewards in math and in computer and and code. Let's see if we can do it in chemistry. So chemistry is like not a verifiable field, right? Like of course you can go test something in the lab, but then we like had to think about all these like ways that we make chemistry verifiable. And one of the ones we settled was like</p>\n<p>make a molecule that has like three nitrogen's, two oxygen, 10 hydrogens's or something. And we thought that was like a pretty verifiable pretty verifiable question. But every time we would train a model, it would find some new insanely weird trick to generate these molecules. And and I I I'll just tell you one of the examples was that um uh it would make these molecules and we would do some checks to</p>\n<p><strong>[69:22]</strong></p>\n<p>make sure like it had the right bonds, the right number of electrons, the right number of atoms and stuff like that. Um but it would just solve the problem in any way possible, right? So like it would just put all the nitrogen's over here, put all the oxygen over here, just like things that don't look good. Yeah. >> And so we started coming up with these rules of like, oh, let's check to make sure it followed these good practices or these good practices. And we found ourselves into this like, you know, it's like the opposite of the bitter lesson, like I don't know, the boutique lesson where you like try to make everything custom. >> But one of the things it kept doing is it kept putting these nitrogens in a row and it put like one nitrogen, two nitrogen, three nitrogen all in a chain. And this is like, you know, if you have three nitrogens, it's like explosive.</p>\n<p>You know, two nitrogen's like bad. And like four nitrogen's you can't make. And I kept telling everyone like it would make these like six nitrogen comets and they're just they're just literally impossible and they're not possible. And many of the people on the team were like computer scientists like on this team and one of them like one day sent me that like this is on the cover of nature today on nature's website. Somebody made a six nitrogen compound and this is like somebody's like career to deliver this compound because this is the most unstable like insane compound you can make. It's some ridiculous setup and like the spectroscopy to get that proven</p>\n<p>was like very difficult and this this I don't know how they did it. It was amazing accomplishment like look Andrew like it's not actually impossible and it was so funny to me that like our model was sitting here spitting out these six nitrogen compounds in like you know 2024 or 2025 and like the paper just happened to come out that year that like mankind had finally made a six nitrogen compound. >> So do do you think that those were actually synthesizable even under these extreme circumstances? >> No. No. Our model was just it was just reward hacking. >> Okay. >> And it was just the the model was so creative in ways to reward hack. Like</p>\n<p>one of the another one we did was um >> we wanted it to make sure that the when it would propose a reaction like make this compound, tell me how to make this compound. We would try to make it sure that all the reagents were purchasable like you could purchase them. They were not like made up. >> Yeah. >> Um and and the reason we came with that is that originally would just like take the end compound and then like remove one atom and be like here's buy this >> and then put the atom on. It's like okay. It's like well it's I wish it was like that. Um, so they have to be purchasable. And then well, we're like, we thought it might be hard if they're all purchasable because sometimes you</p>\n<p>actually order things custom or or something. So, just make sure one purchasable. So, the first thing it starts doing is putting nitrogen in there because nitrogen is purchasable and it like has no participation in the reaction, right? Like, oh my god. Okay. So, I'm like, okay, it has to be purchasable. It has to participate in the reaction. Then it starts putting like acid base chemistry. It would just put an acid here. Acids are purchasable and it'll move one atom. And they're like, okay, fine. Can't be that. Everything has to be purchasable. Then we find ourselves and I'm like sitting there one day building this like ridiculous catalog of purchasable compounds and a bloom filter so it can go fast enough in our training loop and</p>\n<p>I'm like why am I doing this? How did I get here? >> How did I get here? And and I don't know it was really funny because um pre-training or training transformers you know on on just data like just supervised training where you just have the inputs and the outputs directly >> very nice relaxing you know like things are always robust you know things are go pretty smoothly. When you do these verifiable rewards where you have to like write a a bulletproof verifier it</p>\n<p><strong>[72:23]</strong></p>\n<p>is really difficult >> and we had so many models trained only to find out they were hacking some other like random thing in our setup. It's really hard and I and I I don't envy the Frontier Labs that have to do this at a very massive scale because we had a lot of adventures in Ether Zero and and you guys should read the the blog post. >> Definitely read the blog post. It was a great read. >> GRPO. We did make some modifications um to GRPO. >> Yeah. >> Um I actually I used to know all the names of these modifications, but uh I think it's like uh Dapo is one modification and like the clipping we</p>\n<p>did was special and we explored a lot of that stuff. Yeah. >> Um, and it was uh also one of these things where like you think the hypers are wrong, the algorithm is wrong and then you find out it's just because like you had somehow sorted the reagents when you made your training data, but in your test data, you didn't sort them alphabetically and the model was just like barfing because its whole strategy was to exploit something in the way you sorted things. So yeah, we explored a lot of different methods and it was um I learned a lot about chemistry, a lot about nomenclature. Um, and actually</p>\n<p>there's a I learned a lot about medicinal chemistry as well, more than I ever wanted to. >> Awesome. >> If you want to do some like engineering, just check out Edison Scientific and they have, you know, I think a lot they're hiring with lots of like interesting things. Everything from scientists to, you know, infrastructure engineer. >> Yeah. >> Yeah. Thanks, Andrew again. >> Yeah. Thank you very much for for joining us.</p>"}