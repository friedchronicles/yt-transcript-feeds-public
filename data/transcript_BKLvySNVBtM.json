{"video_id": "BKLvySNVBtM", "title": "Brex\u2019s AI Hail Mary \u2014 With CTO James Reggio (acquired for $5B by Capital One!)", "link": "https://www.youtube.com/watch?v=BKLvySNVBtM", "published": "2026-01-17T01:38:03+00:00", "summary": "From building internal AI labs to becoming CTO of Brex, James Reggio has helped lead one of the most disciplined AI transformations inside a real financial institution where compliance, auditability, and customer trust actually matter.\nWe sat down with Reggio to unpack Brex\u2019s three-pillar AI strategy (corporate, operational, and product AI) [https://www.brex.com/journal/brex-ai-native-operations], how SOP-driven agents beat overengineered RL in ops, why Brex lets employees \u201cbuild their own AI stack\u201d instead of picking winners [https://www.conductorone.com/customers/brex/], and how a small, founder-heavy AI team is shipping production agents to 40,000+ companies. Reggio also goes deep on Brex\u2019s multi-agent \u201cnetwork\u201d architecture, evals for multi-turn systems, agentic coding\u2019s second-order effects on codebase understanding, and why the future of finance software looks less like dashboards and more like executive assistants coordinating specialist agents behind the scenes.\nWe discuss:\n\n* Brex\u2019s three-pillar AI strategy: corporate AI for 10x employee workflows, operational AI for cost and compliance leverage, and product AI that lets customers justify Brex as part of _their_ AI strategy to the board\n* Why SOP-driven agents beat overengineered RL in finance ops, and how breaking work into auditable, repeatable steps unlocked faster automation in KYC, underwriting, fraud, and disputes\n* Building an internal AI platform early: LLM gateways, prompt/version management, evals, cost observability, and why platform work quietly became the force multiplier behind everything else\n* Multi-agent \u201cnetworks\u201d vs single-agent tools: why Brex\u2019s EA-style assistant coordinates specialist agents (policy, travel, reimbursements) through multi-turn conversations instead of one-shot tool calls\n* The audit agent pattern: separating detection, judgment, and follow-up into different agents to reduce false negatives without overwhelming finance teams\n* Centralized AI teams without resentment: how Brex avoided \u201cAI envy\u201d by tying work to business impact and letting anyone transfer in if they cared deeply enough\n* Letting employees build their own AI stack: ChatGPT vs Claude vs Gemini, Cursor vs Windsurf, and why Brex refuses to pick winners in fast-moving tool races\n* Measuring adoption without vanity metrics: why \u201c% of code written by AI\u201d is the wrong KPI and what second-order effects (slop, drift, code ownership) actually matter\n* Evals in the real world: regression tests from ops QA, LLM-as-judge for multi-turn agents, and why integration-style evals break faster than you expect\n* Teaching AI fluency at scale: the user \u2192 advocate \u2192 builder \u2192 native framework, ops-led training, spot bonuses, and avoiding fear-based adoption\n* Re-interviewing the entire engineering org: using agentic coding interviews internally to force hands-on skill upgrades without formal performance scoring\n* Headcount in the age of agents: why Brex grew the business without growing engineering, and why AI amplifies bad architecture as fast as good decisions\n* The future of finance software: why dashboards fade, assistants take over, and agent-to-agent collaboration becomes the real UI\n\n\u2014\nJames Reggio\n\n* X: https://x.com/jamesreggio\n* LinkedIn: https://www.linkedin.com/in/jamesreggio/\n\nWhere to find Latent Space\n\n* X: https://x.com/latentspacepod\n* Substack: https://www.latent.space/\n\n00:00:00 Introduction\n00:01:24 From Mobile Engineer to CTO: The Founder's Path\n00:03:00 Quitters Welcome: Building a Founder-Friendly Culture\n00:05:13 The AI Team Structure: 10-Person Startup Within Brex\n00:11:55 Building the Brex Agent Platform: Multi-Agent Networks\n00:13:45 Tech Stack Decisions: TypeScript, Mastra, and MCP\n00:24:32 Operational AI: Automating Underwriting, KYC, and Fraud\n00:16:40 The Brex Assistant: Executive Assistant for Every Employee\n00:40:26 Evaluation Strategy: From Simple SOPs to Multi-Turn Evals\n00:37:11 Agentic Coding Adoption: Cursor, Windsurf, and the Engineering Interview\n00:58:51 AI Fluency Levels: From User to Native\n01:09:14 The Audit Agent Network: Finance Team Agents in Action\n01:03:33 The Future of Engineering Headcount and AI Leverage", "transcript_html": "<h2>Introduction</h2>\n<p>We have like three pillars for our AI strategy. We have our corporate AI strategy which [music] is how are we going to adopt uh and like buy AI tooling um across the business and basically every single function to be able to 10x uh our workflows. Then we have our operational AI strategy which is how are we going to buy and build [music] uh solutions that enable us to lower our cost of operations as a financial institution. And then the final pillar is the product AI pillar which is like [music] are we going to introduce new features uh that um enable Preex to [music] be a part of the</p>\n<p>corporate AI pillar of our customers. It&#x27;s like we want to build features and be a solution that somebody else is saying to their board, hey we we adopted Rex and this is part of our corporate AI strategy. Hey everyone, welcome to the living space podcast. This is Allesio, founder of Colonel Labs and I&#x27;m joined by Swixs, editor of Blade in Space. &gt;&gt; Hey, hey, hey. And we&#x27;re here with Jio at Brex. Welcome. &gt;&gt; Hey, thank you for having me. &gt;&gt; Thanks for visiting uh from up in Seattle where uh I I&#x27;ve been a little</p>\n<p>bit. It&#x27;s cold up there, huh? &gt;&gt; Yeah. And we have an atmospheric river hitting the the the city right now. So, a lot of Yeah. Well, yeah, it&#x27;s uh we&#x27;re getting we&#x27;re getting the full-on winter effect right now. &gt;&gt; Well, you&#x27;re you&#x27;re here. We&#x27;re talking about the sort of AI transformation within Brex. There&#x27;s a lot of um interesting tidbits that we&#x27;re going to draw from your article but also your background. You have got a wide array of</p>\n<h2>From Mobile Engineer to CTO: The Founder&#x27;s Path</h2>\n<p>experience from stripe to um banter to convoy and uh I think also mostly I&#x27;m interested in your journey as as one of the rare people that have transitioned from like a mobile engineering leader to a CTO which I think is also a bit more rare. I used to have this comment in the past where there&#x27;s a career ceiling for people who work on client only things where usually they don&#x27;t hit CTO whereas they typically promote the the backend people the backend clouding for people the CTO. &gt;&gt; Yeah. You know it&#x27;s it&#x27;s something that I I hear fairly fairly frequently because um there aren&#x27;t that many folks</p>\n<p>with a front-end background who who reach this level leadership and it&#x27;s exciting for me to be able to represent that group. I I&#x27;ll say that even though my resume kind of reflects that I&#x27;ve been more on the the front end of things, it&#x27;s probably more my experience as a founder a couple times over that actually helped me get to this this level of my career working for somebody else. Becoming CTO was very much like a leadership and and like general business role as much as it is a technical role. And so I think it was more the skills that I built from starting companies and and trying to build those up made me a decent fit and enabled me to get the nod</p>\n<p>from from Pedro to take this on as my predecessor left about 2 years ago. &gt;&gt; Yeah. One thing I&#x27;m curious you guys commentary this is a little bit broad unscheduled but a lot of startups are bragging about how many ex-founders they have and yes to some extent you want people with the founder mentality and agency which is what what you did to be your employees and to to take initiative in the company but also I wonder if it&#x27;s becoming anti-signal sometimes I don&#x27;t know if you&#x27;ve thought about this</p>\n<h2>Quitters Welcome: Building a Founder-Friendly Culture</h2>\n<p>&gt;&gt; I think it&#x27;s more about the turn for me especially when people are hiring exfounders is like if you&#x27;re truly of the founder gene. It&#x27;s kind of hard to just stay somewhere. It&#x27;s like an IC for too long &gt;&gt; and then it&#x27;s like all right, I joined this thing and then in one year I&#x27;m back to being a founder. I&#x27;m curious for you. What was your I&#x27;m sure you thought about leaving and like doing another company instead. &gt;&gt; In fact, that was that was the the alternative. I was considering even at the time that I got the phone call where they made me the offer to become CTO, I was thinking about leaving to go start a company. And uh you know I think what&#x27;s interesting about it we we actually</p>\n<p>launched um sort of like a new recruiting and employee value proposition for Brex a couple months ago called quitters welcome where we actually intentionally are leaning into this idea that uh we have a disproportionate number of folks who go on to become founders or like heads of a department when they leave our company and and we celebrate that. It&#x27;s actually something that uh I&#x27;m very proud of and that means that like we um we welcome in people who want to get a different experience. I think that there&#x27;s certainly like uh a lot of founders who don&#x27;t make it uh don&#x27;t scale their own businesses to this to the scale that</p>\n<p>we&#x27;ve achieved at Brex. So there&#x27;s something to be learned when they come in. And then we&#x27;re very happy to like uh support people on their way out. And so I I actually really like hiring former founders or future founders. Uh the one value proposition I find that&#x27;s most relevant because a lot of the folks we&#x27;re hiring as AI engineers are kind of folks that are either like winding down their companies or or are considering um maybe running AI startup. The thing that resonates the most with them is that we often times can give them problems to solve that are interesting problems that may maybe they even want to want to like build their own startup around but with instant distribution right like that</p>\n<p>that is the that is the allure is it&#x27;s like you can come into this business and build uh like financial AI applications and instantly have that deployed to roughly 40,000 uh customers across uh you know the Fortune 100 down to you know tens of thousands of startups. So that that&#x27;s what is I think appealing the founders but but the uh the challenge then is making sure that we set them up for success in an environment that still feels a little bit like the startup that they might build themselves versus like something that&#x27;s too corporate. &gt;&gt; Yeah. Instead of doing your own company and then coming to you and be like can I integrate into Brex get all the data. &gt;&gt; How&#x27;s the engineering team structure?</p>\n<h2>The AI Team Structure: 10-Person Startup Within Brex</h2>\n<p>&gt;&gt; Yeah. So we have about uh 300 people in engineering like 350 uh total across EPD and uh for the most part we structure around um our product domains and so this means that Brex is a corporate card uh it&#x27;s also corporate bank account it&#x27;s expense management travel and accounting and so we we actually have sort of full stack uh product domains uh that are roughly like 30 40 people for each of those that um have everything from like the low-level infrastructure up to the the web and mobile experiences. That&#x27;s generally like the structure of</p>\n<p>of our of our engineering um organization. And then we have uh naturally like a organization that focuses on infrastructure security um it uh and then there are two uh additional centers of excellence that we&#x27;ve kind of built that kind of violate that org design uh where we&#x27;ve felt the need to uh to put more focus or like operate slightly differently. And AI is one of those areas where we have another team of just roughly about 10 people um who are focused prim primarily on LLM applications. And we wanted to create a</p>\n<p>bit of a separation there because the way that we were thinking about this and this is actually something we did this summer is we we paused and asked ourselves on on our AI journey towards like infusing our product uh with AI and generating customer value. We asked ourselves like what would a company that was founded today to disrupt Brex look like? And then we try to basically use the answer to that question to form this team uh internally. So it&#x27;s a little bit off to the side. Ideally everybody kind of comes up to speed and um contributes uh you know LLM features but uh but we have this sort of off on the side right now in a centralized manner.</p>\n<p>&gt;&gt; What&#x27;s the difference in AI adoption for those teams? So like are the people on the LLM team like much bigger cursor users, clock users or like do you see similar diffusion? It&#x27;s actually fairly fairly uniform across the entire engineering uh department. It&#x27;s actually kind of funny like one of our our largest cursor users is actually an engineering manager. So like and I I think that this also just like speaks to our core value of operate at all levels where we want all of our ems and everybody in leadership to still basically do the job uh that they&#x27;re managing manage the work. So it actually</p>\n<p>is I I I think the journey of getting everybody into using agentic coding was not sort of exclusive to like the the AI group. &gt;&gt; Yeah. Um I in fact I think this podcast was actually set up because I cold outreach to Pedro &gt;&gt; because he he tweeted this I I I assume this interex. He says I started a new company inside to build the future of aentic finance. No BS just builders building 96 and pushing production grade agents to 30,000 finance teams now 40,000. Um, and then he actually has like a little job description which I think is really interesting. Uh, but</p>\n<p>I&#x27;ll skip that and go straight to Brexit accelerated to grow 5x and cut burn 99% in the past 18 months. I assume that&#x27;s a mix of internal AI automation and other stuff. &gt;&gt; Um, but we&#x27;re basically I wanted to put some headline numbers up front to impress people before we dig into the details. &gt;&gt; Yeah, absolutely. And you&#x27;re you&#x27;re correct. That&#x27;s the that&#x27;s the team that we have this uh like AI team. You&#x27;re actually what was that? &gt;&gt; Very young team. &gt;&gt; Yeah, it&#x27;s very young. I mean it&#x27;s and it&#x27;s been really interesting the the composition of the team is like very young like AI native like 20-year-olds who basically grew up with the tech um kind of paired off with more like staff</p>\n<p>level software engineers that have been at a little while who can kind of navigate like the existing code bases and like understand uh the product and the customer deeply like we&#x27;ve formed these really uh couple of tight tightnit pods in the AI or where it&#x27;s like three people generally somebody who has like more of a product to customer focused background that like staff engineer who knows uh where the skeletons are and then like a much younger like AI native engineer who um can just do things with uh with agents that like the rest of us uh dinosaurs maybe uh don&#x27;t don&#x27;t uh can&#x27;t either dream of or like or where</p>\n<p>our I think I think part of it is like sometimes the too much experience or too much knowledge of how to solve a problem can actually be an impediment to thinking uh differently about it and thinking about it from like an AI first lens. But yes, we we&#x27;ve been we&#x27;ve been slowly growing that team just in the same way that like a preede startup. You want to be very very careful about talent density and like very deliberate like only hire when you absolutely need it. And so yeah, at this point it&#x27;s just about 10 people and I think it was probably four or five people. Uh I think everybody was actually in the photo that was attached to that tweet uh when Pedro put that out a couple months ago.</p>\n<p>&gt;&gt; Yeah, we&#x27;ll put it up. It&#x27;s a photo at 1:20 a.m. in a on a Friday. &gt;&gt; Yes. Oh, yeah. Yeah. cuz we we always do uh we always do like Friday Friday demos and and like that&#x27;s a time for everybody to get like kind of exec review time and so uh &gt;&gt; everyone&#x27;s in Seattle. &gt;&gt; Um those folks were all in Seattle. Uh but they&#x27;re actually ge geographically distributed. Uh we have a couple folks here, a couple in S. Paulo, a couple in Seattle. &gt;&gt; How addressible we have this like AI center of excellence which are basically the people running these teams across companies. &gt;&gt; Yep. &gt;&gt; How do you make the other engineers not feel like you&#x27;re not special? I think that&#x27;s something that I hear a lot is</p>\n<p>like, hey, you know, why aren&#x27;t these people working on all the Google LM things and like I&#x27;m stuck working on, you know, the KYC integration with whatever. Yeah. You know what I mean? It&#x27;s like, how do you build that culture? &gt;&gt; You know, it&#x27;s interesting. I I thought that that would be more of a problem, but the benefit of having really optimized our engineering culture around business impact actually causes it to cut in the other direction where where folks some folks don&#x27;t want to work on the AI products because doesn&#x27;t have as much clear direct like business impact right now. Doesn&#x27;t doesn&#x27;t impact revenue as directly. And so I uh I think folks for the most part uh we&#x27;ve we&#x27;ve</p>\n<p>enabled folks who have a strong desire to work on on um AI products to to join that team like somebody somebody transferred out of our expense management organization to come over there because they&#x27;re really passionate about taking like their knowledge of like policy evaluation and and bringing it into the the AI uh uh team. But for the most part, I think everybody understands like how their work uh ladders up and maybe there&#x27;s some like friendly rivalry because like the folks who say work on a card product, they they drive 60% of our direct revenue and so they they&#x27;re pretty happy with that and uh and they don&#x27;t feel like they&#x27;re being left out. Uh and I will also say u</p>\n<p>as you probably saw in this this piece that we we uh put out with uh first round, there is a lot of smaller applications of LLM peppered throughout all of our product and operations teams. is just some of the more novel like agentic layer that sits on top of BS that has been put together like in this in this sort of isolated team. So it&#x27;s not like folks aren&#x27;t getting to to build with LLMs or use LLMs on a daily basis. &gt;&gt; Yeah. Maybe run people through the BS agent platform. We&#x27;ll put the diagram in the video where you had the LLM gateway. You have like the whole MCP layer. We</p>\n<p>just had David the creator of MCP right before you. So this is very timely. &gt;&gt; Um yeah, how did you start building that? What&#x27;s the architecture? Yeah, the</p>\n<h2>Building the Brex Agent Platform: Multi-Agent Networks</h2>\n<p>architecture, you know, I think simple is uh is elegant and we we&#x27;ve had basically an LL gateway and and a basic canrolled platform uh from the very early days. In fact, right before being tapped to become CTO, I was leading uh like a AI uh labs team internally uh in the wake of like the announcement of chat GPT, you know, everybody saw this through technology and said, &quot;Hey, what are we going to do with it?&quot; And so one of the first things that we did um I think January 2023 that would have been uh was try to put together some internal infrastructure that made it possible for</p>\n<p>us to deploy pro deploy manage version and eval prompts uh and then be able to manage uh like data egress and model routing and uh have some very basic like observability and cost monitoring uh in an LLM gateway. So that&#x27;s that&#x27;s infrastructure that we stood up and it still continues to power a lot of those smaller uh more let&#x27;s say like precise applications of LLM. So like for instance, we&#x27;ve uh we set up a completely automated uh pipeline for um evaluating uh customer applications to get them onboarded instantly to Brex,</p>\n<p>which is something that used to require um human intervention either for underwriting or KYC, but now we basically have a series of of agents and um and particularly like research agents that will go and do the work that humans would normally do. And so that&#x27;s running on top of this uh this handrolled uh framework. And then for the agents on BREs that we announced in our fall release, which is like this agentic layer that we&#x27;re building that sort of sits on top of BS and can embody workflows that a finance team would normally uh hire humans for. We&#x27;ve actually uh started using Mastra for</p>\n<p>that as like the kind of primary &gt;&gt; primary framework for for accelerating us. They actually have built everything in Typescript um which is another like technology choice that&#x27;s uh answers the question of like what would we do if we started Brex today but isn&#x27;t the case</p>\n<h2>Tech Stack Decisions: TypeScript, Mastra, and MCP</h2>\n<p>for all of our existing backend code which is either cotlin or elixir and then we have uh we have a mix of PG vector pine cone and like I think what we&#x27;ve seen is we&#x27;re always we&#x27;re always re-evaluating the tech and framework choices as we go uh because the halflife of code has declined so significantly with agent coding it&#x27;s actually quite uh easy for us and for anyone else to to kind of try on for size a variety of different pieces of tech to to figure out what is going to be most ergonomic for solving the problem. &gt;&gt; Double click on Mastro that&#x27;s a new</p>\n<p>choice an interesting one. Yeah, I mean I think that the main the main reason that we adopted Mastro is that it provided the ergonomics that we were actually uh that the ergonomics of Master are quite similar to the internal um LLM framework that we built 2 and a half years ago. Uh whereas like Langchain was available at the time 2 and a half 3 years ago. uh it didn&#x27;t quite feel uh right to us when we were trying to um it it kind of addressed the things that weren&#x27;t the the pieces that we we needed to address which was like</p>\n<p>being able to have really simple um observability and and um logging tracing &gt;&gt; lang chain didn&#x27;t do it &gt;&gt; it I mean at that time it didn&#x27;t I think it was really I think it was &gt;&gt; well they fixed that &gt;&gt; yeah no they certainly they certainly did but but but so like we we did I&#x27;m trying to remember because this is now ancient history uh we evaluated link chain turned off of it, built our own thing, and then as we were looking, we kind of want to deprecate this internal framework that we built because at the end of the day, it&#x27;s not leveraged for us to maintain that. Uh, and Master ended up fitting the bill for um or the</p>\n<p>the feature set that we were looking for. And I think what what&#x27;s been interesting is about half of the the applications that we&#x27;re we&#x27;re building right now um on the the agent layer are running on Mastra and then the other half are actually still running on like yet another internally developed framework which is a framework that&#x27;s focused more on networks of agents. So sort of multi- aent orchestration versus more like strict uh like you know single turn or uh like workflows which are easier to use like either langraph or mastra. tell us about your multi-agent framework. I mean it&#x27;s what are the</p>\n<p>design considerations? Um why why is this the first we&#x27;re hearing about it? &gt;&gt; Yeah. Yeah. So it&#x27;s funny a big big reason why we haven&#x27;t written more about this is that it continues to evolve quite a bit and I I feel like we we actually had a blog post that we were going to put out in conjunction with the fall release uh talking about how we built this and by the time that we finished uh you know the blog post and had all the package ready it was already like halfway outdated. And so the way that this has started to emerge is this multi- aent network approach to implementation was when we were trying to scale up our um sort of consumer</p>\n<p>grade Bs assistant. So if you think about like Brex uh and our customers uh there&#x27;s really like two very broad personas that we serve. We serve members of a finance team who are generally like going to be doing like in roles like accountant or controller or head of T&amp; for those folks. uh they are uh going to be interacting with agents that are much more specific to their roles. But then the other broad cohort of of users we have are like employees of companies that have deployed Brex. So you know you go join a new company that company uses Bre you get your Brex card. And our goal for employees is for Brex to completely</p>\n<p>disappear. Like the best UI UX for Brex is just the card. like every single thing that you have to do in the software beyond just swiping the card is like an opportunity for AI to uh to eliminate some work for you. And so what we thought was the right approach to solving that for that was to um was to embody like an executive assistant uh for every employee because I as an executive at Brex I have an EA and she knows enough about me. She has access to my calendar, my email, has all the context on uh when I&#x27;m traveling and for what business purposes. And so she&#x27;s</p>\n<p>basically able to do everything that I would be obligated to do in Brex, be it like booking travel or like doing expense documentation. And so what we wanted to do is we wanted to build like that EA connected to the same data sources and see if we couldn&#x27;t simulate that behavior so that um you know you basically your interface to Bra&#x27;s uh SMS in the card. And when we started building that out, uh you know, the most naive like architecture for that would be to have um an agent with a variety of tools and maybe maybe do some some rag to ensure that it has like appropriate context for the conversation. But what</p>\n<p>we were finding is that um the wide range of different product lines that exist on Brex made it difficult for one uh like agent to perform well u being responsible from everything from like expense management to finding and booking travel to answering policy and procurement questions. And so that&#x27;s when we started breaking down the problem uh and into into a variety of sub agents that sit behind an orchestrator. And obviously this isn&#x27;t something that can be implemented using langraph or master even has the notion of these as like network switches and</p>\n<p>beta. But what we found is that it was easier for us um when it came to being able to build evals for the system. uh we we kind of just hit the eject button and built our own framework which is one in which um we have agents that are able to uh to basically DM with other agents and have multi-turn conversations amongst themselves to coordinate to um to complete a task to uh or like to complete an objective. And um what&#x27;s what&#x27;s been nice about that is it means that like you can have your Brex assistant there&#x27;s like one single one single like point of contact uh between</p>\n<p>you as an employee and the Brex product. Then behind your assistant um if the company has like expense management turned on you have that. If they have reimbursements there&#x27;s another agent for that. if they have travel attached to the red agent for that. It actually also then facilitates like our conception here is that um you know it&#x27;s like generally like software encapsulation patterns taking like sort of projected into the agent space. It also makes it easier for us to have like the team that owns and understands travel like be the ones to go and iterate on that without needing to worry about like redressing the total system um or needing like one team to own every single possible um act</p>\n<p>action you could take as an employee. And I&#x27;ll say that like I&#x27;m still of the mindset that somebody will build a great framework and we they ultimately migrate to it but or it might be us that we ultimately open source this right like but um but for us like this is uh this has worked out quite well in like l of like a couple other approaches that we we tried along the way that just didn&#x27;t perform well which was to you know overload the the the agent with a variety of tools or contextual like context switching where we try to say oh this conversation looks like it&#x27;s more about reimbursement so let&#x27;s like update the prompt with more reimbursement</p>\n<p>context like that was that was another approach that we took that didn&#x27;t perform as well is actually having a reimbursement agent that it would collaborate with. &gt;&gt; What about MCPS as like sub agents? Oh yeah, that&#x27;s another pattern. &gt;&gt; The key thing there is that we there&#x27;s actually a lot of value in having like multi-turn um uh conversations from like the orchestrator or the assistant to like the sub agent whereas like you know a tool call is basically just like one RPC. And so oftentimes what will happen is um you know let&#x27;s say let&#x27;s say the the the user reaches out to their Rex assistant and says hey like am I allowed</p>\n<p>like how much am I allowed to expense per person for dinner tonight? I&#x27;m taking my team out. Uh and the the uh you know your assistant&#x27;s going to then reach out to the policy agent. Maybe the policy agent needs to know in order to answer that question. Maybe it needs to know like whether this was um uh was like a customer event, a team event or whether you&#x27;re traveling. And so it may actually send instead of it can&#x27;t just answer the question. So it&#x27;s going to reply back to the the assistant and say hey I need you to ask uh this clarifying question. And so then the assistant will return to the user ask clarifying question and then they&#x27;ll basically have</p>\n<p>this sort of multi multi-turn conversation across multiple agents versus it just being encapsulated in like a single uh call and response tool call. And so there are still like all the all the sub aents have a ton of tools, but I I think of like the MCP and and tool usage as being like the interface to all of our conventional uh imperative systems, not not the the AI space. &gt;&gt; Yeah, that&#x27;s the conversation we were having earlier whether or not it should be an agent to agent protocol as well or like &gt;&gt; Yeah, there should be like a chat back. &gt;&gt; Exactly. Exactly. And that&#x27;s the thing is like Okay. And one of the ways that</p>\n<p>we actually grafted this into Astro before we we built our own framework was to was to make every sub agent a tool. Uh and then the input was just natural language, the output was natural language and the if you needed to have multi multi-turn um you would basically just put the full like conversation and as you kept calling calling the sub agent as a tool and it&#x27;s just like at that point you&#x27;re like okay the ergonomics are kind of the framework framework is fighting me on this. It&#x27;s actually helpful for us to basically conceive of it as an org chart and like it&#x27;s the agent or chart with uh with um</p>\n<p>you know my EA is DMing other specialists uh and having brief conversations to support me as their client. &gt;&gt; Yep. &gt;&gt; That was a really good uh deep dive. Thanks for indulging. I feel like you guys are not afraid to make your own tech which I think is a competitive advantage. I really like that culture. Maybe I we should go a bit breath first as well. Of course, I think we also deep dive a little bit too much in in one area. There&#x27;s um and we&#x27;ll we&#x27;ll put up the chart, but I&#x27;m also very interested in like the sort of internal agent stuff, the operational stuff and just the general platform scope. So, please</p>\n<p>feel free to just like go into your spiel on it. &gt;&gt; Yeah, of course. So, one of the things that I was trying to do at the beginning of the year, uh as CTO, you know, I think it really fell to me to articulate what our AI strategy was as a business. you know, every every board of director was, you know, or every me every member of our board was like, &quot;Hey, what&#x27;s your AI strategy?&quot; And while we were doing a lot of duties, we literally go, &quot;He&#x27;s got it.&quot; &gt;&gt; Well, yeah. [laughter] Uh Yeah. And and if I didn&#x27;t, I uh I&#x27;d be in trouble. I think he also was counting on me, given that I was doing the AI uh organization before CTO to to have That&#x27;s true. But but a big part of</p>\n<p>it was like we we were doing a lot with with LLMs. um it was more like these little one-off features and you know, hey, like maybe mix in some suggestions here or maybe do a little bit of ops automation over here. But it wasn&#x27;t um it wasn&#x27;t easy to to kind of create like a verbal framework uh um of all of these investments and without that framework then we weren&#x27;t able to like set a set a vision or a road map for for investments. What we did at the beginning of the year is we took everything that was going on as well as all of our ambitions, all of the good ideas, as well as like the problems we</p>\n<p>were trying to tackle as a business this year, throw it all on the table and see if there were some ways to cluster it into a framework that made sense to the business, to our board, uh to ourselves. And we came up with, I think this is not particularly novel, but has helped us</p>\n<h2>Operational AI: Automating Underwriting, KYC, and Fraud</h2>\n<p>quite a bit. We have like three pillars for our AI strategy. We have our corporate AI strategy which is how are we going to adopt and like buy AI tooling um across the business in basically every single function to be able to 10x uh our workflows. Then we have our operational AI strategy which is how are we going to buy and build uh solutions that enable us to lower our cost of operations as a financial institution because I think it it&#x27;s fairly intuitive like financial institutions like ours face a lot of regulatory expectations and there&#x27;s just</p>\n<p>like a high ops burden for running our business and so it&#x27;s sort of like a lot of kind of internal use cases like being able to do like fraud detection underwriting KYC um be able to handle dispute automation on car transactions. Those those types of operational investments or our ops AI pillar. And then the final pillar is the product AI pillar which is like are we going to introduce new features uh that um enable Pre to be a part of the corporate AI pillar of our customers. It&#x27;s like we want to build features and be a solution that somebody else is saying to their</p>\n<p>board, hey, we we adopted Rex and this is a part of our corporate AI strategy. Yeah. And so it&#x27;s it&#x27;s kind of has this nice little feedback loop and we we basically within the company split uh you know did a little bit of divide and conquer where uh folks in um IT and on our people team were more or less uh spending more of the effort driving on corporate AI really like looking for um making the procurement decisions like creating a culture of experimentation where we spotlight and incentivize people for trying uh to sort of improve their personal workflows using AI. And then the the pieces that I&#x27;ve been more</p>\n<p>involved in have been operational and product and we were just talking about products here which is like the agents on Brex and stuff but I think that the operational AI investments have been some of the the most sort of immediately impactful uh to the business because we have hundreds of people who work in our operations organization and it&#x27;s actually something that differentiates us because our uh seesat and the quality of our our support and service is very very high uh something we&#x27;re very proud of and so trying to figure out how can automate significant portion of this and uh and use LLMs in a way that doesn&#x27;t</p>\n<p>degrade the customer experience and then also kind of addresses like what is the future of the roles of the people who we already have working full-time for us. So this is where um uh Camila uh our COO who kind of co-wrote the the piece uh with First Round uh with me uh she&#x27;s been leaning really aggressively to help every member of the operations organization um start rethinking their role as being uh not people who kind of execute against an SOP but are people who are going to like build prompts, build evals and like be become more AI native and like the way that they do</p>\n<p>work. And so a lot of the engineering we&#x27;ve done has been to enable folks say in um in fraud and risk to be able to uh to refine prompts and uh and add additional automation to their workflows. &gt;&gt; Yeah. And this secret force pillar, the the platform. &gt;&gt; Yeah. Yeah. Exactly. That is the that is the thing that ties it all together. Exactly. Is is the is the platform. And I think what&#x27;s been really nice is that um even though the platform is kind of a loose uh loose loose term because it consists of a wide variety of technologies as I said like we haven&#x27;t</p>\n<p>been too religious or dogmatic about everybody needing to be on one particular thing. What we&#x27;ve seen is that um by making a variety of sort of ergonomic uh options for building with loss available, it like really has made it easier for for us to make a quick leap forward on operational AI. Like we as soon as we put our mind to it and we said like look no we want to hit 80% automated acceptance rate for all um all startup and commercial businesses that apply for Brex like we want a decision within 60 seconds that&#x27;s fully touchless no humans involved. We were able to break that down and then actually build</p>\n<p>the uh build the agents build the tools um on top of that platform really quickly and and a lot of those tools are the same tools that our product AI uh agents use as well. &gt;&gt; I was pretty sold on the conductor. I don&#x27;t know if this is under exactly the bucket, the conductor one &gt;&gt; uh provisioning command. I was like, &quot;Yep, I want that.&quot; &gt;&gt; Yeah, that was actually I&#x27;d love to talk about that. So, that&#x27;s that&#x27;s actually on the corporate side. And I think that this goes back to maybe another intuitive but but I&#x27;d say like bold decision that we made, which is that we&#x27;re not going to we&#x27;re not going to try to pick winners in the horse race between the foundational model providers</p>\n<p>or the the agent coding tools or like basically anywhere where there&#x27;s there&#x27;s an active horse race. What we do in instead of like trying to pick a single solution is we will procure like a a small number of seats like multiple solutions and then we&#x27;ll give employees the ability to pick whatever one they want to use. And so for instance like we allow employees to basically go to in Slack and use conductor one to uh get a chat GPT a claude or a Gemini license and basically you can just like build your own stack where you pick your um you pick your like chat chat provider uh</p>\n<p>as a as a dev you can pick um you know between like cursor wind surf cloud code credits like and and you can basically craft your your stack to your preference and easily switch between them. And what that does for us too is when we&#x27;re going to like obviously we have sort of enterprise agreements in place for all of them for the sake of like the you know the the privacy and non-training guarantees. But it&#x27;s fun because when we go to renew these contracts um it it we can basically resist the need to like do a wall-to-wall deployment. We can say hey look like usage trends it our our employees are voting with their fee.</p>\n<p>They&#x27;re voting with their dollars and you know maybe uh maybe your tool isn&#x27;t as uh as hot as it was a year ago. Does it give you a dashboard of what people are choosing? &gt;&gt; Yeah, actually we look at that. Um we were looking at that as we&#x27;re going into budgeting over next year. Very interesting. &gt;&gt; I would love to see that those what what&#x27;s you know anything that&#x27;s like really up anything that&#x27;s really down. &gt;&gt; It&#x27;s fascinating how how different the landscape is every every three three months. And I think one of the one of the interesting challenges we had early on was uh getting folks to just like try um these tools, try to incorporate like a coding.</p>\n<p>Yeah. I like early on I say like 12 to 18 months ago now like get folks to to just take the time to try a new workflow. And now at this point I think what we&#x27;re seeing is like even if you know uh a new model hits the same like um when when Codeex uh came out and everybody&#x27;s like oh Codeex is is better at at uh codegen but it&#x27;s a little bit slower. like I find fewer folks are like kicking the tires on new things because like the they&#x27;re just so comfortable with the ergonomics of their current workflow that that um you know uh some folks are just like I want to just stick</p>\n<p>with Cloud Code cuz I know it now. I&#x27;ve been working with it for like 9 months. So I don&#x27;t need to to keep uh keep switching. I don&#x27;t need I don&#x27;t feel the incessant need to keep trying new things because I&#x27;ve I&#x27;ve gotten I&#x27;m an iPhone person and I&#x27;m just like going to stay with an iPhone even, you know, even though there&#x27;s some really sexy Android hardware out there. Do you have one of the big numbers like 80% of all of our code is written by AI or but how do you measure it internally? &gt;&gt; Yeah. No, not really. We we I mean I what we do is we&#x27;ll we&#x27;ll measure like the attributions on the the number of commits that that have the like co- co-authored with um and we pull some of</p>\n<p>those stats but I don&#x27;t index heav like in fact I don&#x27;t index on those at all. I don&#x27;t and honestly like I &gt;&gt; I don&#x27;t know how I in honest like honestly calculate that number. I agree. &gt;&gt; Yeah. And so so I the thing that the thing that we&#x27;re we&#x27;re really just you know we&#x27;re at the point now with like our AI agentic coding journey where now we&#x27;re trying to solve the second order effects of like a little bit too much slop maybe a little uh not enough um yeah exactly not enough like rigor and code reviews. Um we&#x27;re trying to the</p>\n<p>adoption is there and now we have to figure out like how to mature in our usage of these tools uh so that we you know quality or like long-term maintainability doesn&#x27;t suffer as well as like maybe one of the other fac facets of being able to generate a lot more code more quickly is like the the drift between team members as far as like understanding of the the code that&#x27;s in their services increases is like everybody&#x27;s moving faster and more independently. Uh it that is another sort of risk that we&#x27;re starting to see like you know an incident response where</p>\n<p>folks don&#x27;t know they don&#x27;t know a service as well as they they used to because it&#x27;s changed so much in the past couple months because everybody&#x27;s moving more quickly. &gt;&gt; Yeah, this has been a major topic for me this year on codebased understanding and slop because obviously it&#x27;s so much easier to generate code but then now we have to review it and to some extent you can&#x27;t really fight AI with more AI. You can&#x27;t just be like oh just throw throw an AI reviewer on their AI code and you solved it. Uh and so so you do need to just scale human attention. And I think that&#x27;s something I&#x27;ve been pushing a little bit in terms of like well you&#x27;re you&#x27;re just going to like every engineer</p>\n<p>is just going to own more code &gt;&gt; period and and be parachuted in and be expected to ramp up and be be productive and also fix bugs and if you&#x27;re on you know pager duty or whatever to just because I mean everyone&#x27;s going to trying to be more efficient and you&#x27;re supposed to see ROI productivity because if you don&#x27;t then what&#x27;s the whole point of this? &gt;&gt; Exactly. Exactly. And I and I think it&#x27;s funny going back to the point of, you know, you could you could add AI on top to solve the problems that AI introduces and but you just keep you that&#x27;s like an endless chain. Uh and so</p>\n<p>&gt;&gt; well no I mean that the the the code rabbits of the world, the the graphites of the world would say yes actually you can &gt;&gt; and so that&#x27;s the little bit of the the tension there. &gt;&gt; Yeah. And you know, I I uh I&#x27;ve been thinking a lot about how the craft of of engineering is evolving and and I will say that I feel further away from being able to predict what what it looks like than I I did this past summer when I spent a bunch of time. Um I actually basically went on leave for a month and joined the um join the the the team that uh the the AI team that we were building just to go and build alongside them. I</p>\n<p>felt like it was really important for me to deeply understand uh the problems in the tech but and so that was me. I was I was you know writing pushing code um effectively 996 and uh and I I went through so many different moments of realization of like oh my god this is going to change everything to oh my god this is just amplifying all the good and the bad in the industry uh to oh my god engineers are not going to have a job anymore to you know it&#x27;s like and so I I don&#x27;t have any pred like I felt like I had all the predictions back then and at this point now I&#x27;m just very interested</p>\n<p>to watch the the phenomenon continue to unfold in front of us and uh I will say I was chatting with a bunch of really bright uh you know college juniors and seniors at a dinner we hosted last night and um well these folks are about to enter the industry basically having kind of come up in the the era of agentic development and LLM and I asked them like so what is your workflow when you&#x27;re like building uh like building a project uh how do you how do you use agents versus like when do you decide you&#x27;re going to actually just write code by hand and I was surprised to hear the consensus was that most people there</p>\n<p>were using agents to collaborate on like building a design document like collaborating on the architecture of the solution that they want to build and then maybe asking it to like emit you know a doc or an implementation plan but then they&#x27;ll go and write a lot of the code themselves still. Uh so it&#x27;s a little bit more of the the uh the rubber duck co-architect uh uh use case that was most prevalent in that group. I I was very surprised by that. &gt;&gt; I&#x27;m impressed the kid the kids are all right. Yeah, I know. Then they still want to they still want to actually write the code themselves. It&#x27;s interesting. &gt;&gt; Yeah. What we hear from like the Gen Z&#x27;s</p>\n<p>at open, they they just yolo everything into code and [laughter] &gt;&gt; yeah, I would say most of the code I generate is like Yeah. But but I spend a lot of time on the dock. It&#x27;s curious like when you&#x27;re like younger in your careers like you don&#x27;t really have all the mental models of the different patterns to instruct. So I feel like there&#x27;s like over reliance especially if you&#x27;re doing the design doc you know. I I feel like most of the senior engineers will spend more time on that. It&#x27;s like even things like you know what columns should you index &gt;&gt; depending on you know what queries we usually run on this table and things</p>\n<p>like that. It&#x27;s hard for any AI to know that right &gt;&gt; you know and it&#x27;s like I feel like the the role of like the more senior engineer should actually be more of this. It&#x27;s like spending time teaching the AI and then the AI can teach the junior people in a way. &gt;&gt; Yeah. Yeah. And it it everything everything looks like mentorship and management [laughter] &gt;&gt; at the end of the day, right? It&#x27;s like you&#x27;re you&#x27;re breaking down tasks, you&#x27;re uh you&#x27;re supervising work, you&#x27;re giving feedback. Like it&#x27;s, you know, it&#x27;s basically management &gt;&gt; except that there&#x27;s agents are really bad at memory still. Like they basically have zero memory and and it&#x27;s it&#x27;s in</p>\n<p>Seattle 2025. What&#x27;s going on? &gt;&gt; Yeah. [laughter] &gt;&gt; Yeah. What&#x27;s your internal stack for like a uh preferences? There&#x27;s like kind of like you know explicit preference you can use with uh you know agents MD and all that stuff. Uh there&#x27;s implicit preference with lint rules and things like that in a way where it&#x27;s like it just happens you don&#x27;t have to tell it. How do you structure that? &gt;&gt; Oh are you talking about for aentic coding or memory like platform? &gt;&gt; Yeah. Yeah. For like the coding specifically it&#x27;s like and then we can kind of talk about you know the whole bre platform.</p>\n<p>&gt;&gt; Yeah. Just just nothing nothing special just a lot of um explicit rules &gt;&gt; MD files. &gt;&gt; Yeah. And then we have uh and we um linting. We still have like traditional llinters in place for the couple of different language tool chains. And then we&#x27;re we&#x27;re big fans of creptile and we use them for basically all of sort of the um smarter than linting uh like a code review. Uh that&#x27;s been the one solution that we&#x27;ve aligned around that has served us extremely well. &gt;&gt; Yeah. Reportile. &gt;&gt; Yeah. No, we&#x27;re we&#x27;re huge fans. They&#x27;re they&#x27;ve built something really impressive. And I think the thing that constantly blows my mind about it is um</p>\n<p>the way that they&#x27;re able to just have a really impressive signal to to noise ratio. Like the the comments that it leaves are very very high signal. Uh like never I never regret going through all like 65 comments it leaves on my on my diffs because it catches so many things. &gt;&gt; Yeah, I found the codeex review to be really good. I don&#x27;t use codeex for code generation but like the review product Yeah. is like very good for some reason. Um, I used to have when I was working in Rails, there was like this project called danger systems. It was kind of like a semantic llinter.</p>\n<p>&gt;&gt; Exactly. &gt;&gt; I feel like there should be more of that now. It&#x27;s kind of like the rules are one thing a generation, but I want something in my CI that is like enforce these rules and call out where they&#x27;re broken and then I can just copy paste that uh in an agent. But &gt;&gt; yeah, when we when we started building this this new agent um codebase like cuz as we was saying like we were answering the question, what would you do if you built you know a Brex disruptor today? And it&#x27;s like it wouldn&#x27;t be to pick Cotlin and Elixir as the back end. And uh and so we actually went with the full like TypeScript stack and and we we were building on all like public interfaces</p>\n<p>and um really trying to make sure that this agent layer was uh like arms length from from the the good and the bad of of the core of our product. And um and one thing I think what we did early on and I don&#x27;t actually know if this is true because again the team keeps sort of iterating uh but we we&#x27;re having good uh good luck using um cloud code like in a GitHub action to basically go and do uh do more of that dangerous style like code review. So have a a prompt for it that went through all of the different facets that were more conceptual versus like rigidly enforceable by a llinter uh</p>\n<p>and have it leave a big comment at the end with u your conformance to the ediomatic coding patterns of the of the new repo. I wanted to spend some time. You said you wanted to dive on and operational agents uh customer support, onboarding, KYC, fraud, delinquent account disputes. This is I imagine the bulk of it of of the work. &gt;&gt; Anywhere where there&#x27;s a good story about maybe um when you started out it was going to be this way and then you discovered through building or through customer contacts that it had to go a different direction. And so that difference in beliefs is something that</p>\n<p>people can learn from. The thing that</p>\n<h2>Evaluation Strategy: From Simple SOPs to Multi-Turn Evals</h2>\n<p>immediately comes to mind is that we uh we believed at the beginning that using RL for credit decisions would actually be a like would be the way that we would end up or like credit and underwriting like how much of a of a limit should we give to this business? Um that reinforcement learning would be the way that we would go about um building a model that effectively would decision in the way that um a human underwriter would. And it turns out that it was we made this big investment. We were</p>\n<p>working with some outside uh like a company uh that specializes in this and the performance we ended up getting was inferior to just building a like a web research agent. And so so I think what what we took away what what has been most evident in operational AI is that in operations you need to be able to break down problems really granularly and be able to form SOPs that humans can repeatably follow and thus can be audited. uh because so much of the the responsibilities in operations is to uh</p>\n<p>is to have auditable repeatable processes that help to ensure that we&#x27;re operating in a compliant manner and that actually translates just so cleanly to LLM that we haven&#x27;t needed to use too many sophisticated techniques in in operational AI. uh it&#x27;s been it&#x27;s been relatively simple like new tool uh like agents or maybe even a lot of problems can be solved with just like a single turn uh chat completion and so the fact that we didn&#x27;t when we did one one sort of attempt to overengineer and use more sophisticated techniques uh and we we we</p>\n<p>discovered uh that in fact the solutions are a bit more uh more plain and and less technically sophisticated. The challenge is really articulating and refining prompts to reflect reflect the execution of the SOP and like reflect all the sort of institutional knowledge that isn&#x27;t written down uh so that agents can properly replace like the the humans or the contractors we would have making these decisions. How do you decide what is worth like spending a lot of time building versus what you think some of these models are just because some of these tasks are so generic. They&#x27;re not really about bre. Yep. like you can assume the models will be good</p>\n<p>at it versus some of them are like very specific to you. We kind of prioritize like the the tasks that are most common for the broadest number of customers. And the uh some of them are are are fairly um fairly intuitive like being able to research uh a customer to look to assess like legitimacy of the business and whether that business would fit our ideal customer profile for for onboarding because there&#x27;s certain types of businesses that we either legally cannot serve or we are not comfortable being able to serve. So that&#x27;s the type</p>\n<p>of really kind of basic research um and like a a relatively straightforward problem uh that isn&#x27;t hyperre specific. The things that are a little bit more specific who to us or or companies in our sector would be preparing documentation for a network card dispute. Like if if you go and dispute a transaction on on your your personal card, you will provide evidence to your card issuer. or the card issuer then has to put together like a three or four page word document that goes to uh the card network and then eventually goes to</p>\n<p>the acquiring bank and and all of that is like much more specific to our business. It&#x27;s a huge operational overhead for us and that&#x27;s something that we we decided to automate later because it&#x27;s not as uh it&#x27;s not on the critical path of like serving the vast number of our customers like disputes are expensive but not very common operational process and so they&#x27;re lower on the stack and uh I think we&#x27;re we&#x27;re getting there right now but this year has basically been us just kind of like looking at every single process just kind of stack ranking and um I will say like the thing that got us started down</p>\n<p>this path um was we wanted to expand our ideal customer profile to support more business like a wider variety of commercial businesses which tend to be businesses that aren&#x27;t growing as quickly. Uh so they&#x27;re not like tech startups which have a lot of growth and they&#x27;re not usually like they&#x27;re not enterprises which also tend to have a lot of growth. It&#x27;s more like a lawyer&#x27;s uh a law firm or a dentist office. U these types of like solid businesses that we should be able to serve and underwrite, but the cost to to onboard them and the cost to serve if you have all uh all the humans in the loop make</p>\n<p>them ROI negative. And so that was the first sort of use case of of AI within our ops uh ops organization that then led to us really understanding we could automate much more than that. &gt;&gt; Is this BS going back into SMBs? &gt;&gt; Ah that&#x27;s a good question. Yeah. Yeah. So, never never let let that die. Uh, you know, know we um I think the way we&#x27;ve thought about this is we want to always like offer our product to customers where we believe we have a like a an offering that is well suited to the to the needs of those businesses.</p>\n<p>And I I would say that still for very small uh businesses, our offering isn&#x27;t it&#x27;s not built for that. It&#x27;s built for it&#x27;s built for companies that have some degree of scale, typically have at least sort of one person if not a couple people in the their finance team. And so we consider these to be more like the the commercial segment. And so it rhymes with with SB, but our approach back then was uh was a little bit more naive. And I would say we also we were just going for a volumes like a volume game there.</p>\n<p>Uh our our internal controls were not as strong. we didn&#x27;t have as much experience like underwriting those businesses. And so it it was really ended up being a huge uh burden for the business uh almost existential uh for us to have those tens of thousands of customers that all were uh ROI negative. Uh so we&#x27;re trying to basically scale to serve more businesses outside of tech and outside of like the OPM market segment but um but do it thoughtfully. So, I think right now our um our minimum threshold is is uh like a million</p>\n<p>dollars a year in recur in in annual revenue or um or like $10,000 or more per month in in card transactions is kind of being like the low end of our ICP, which is obviously not what you would think when you think of a small business. Like small businesses tend to still be smaller than that. &gt;&gt; Oh, wow. That&#x27;s really small. Okay. &gt;&gt; Yeah. &gt;&gt; Yeah. Mid-market. &gt;&gt; Yeah. Exactly. And it&#x27;s funny. It&#x27;s just like the the the names of these segments. Um, you know, it&#x27;s like it I don&#x27;t know. &gt;&gt; Yeah. No, I think I like that&#x27;s that&#x27;s like Yeah. It&#x27;s like lower midmarket and it&#x27;s funny though because when what we call enterprise may be another, you know, what sales what we call enterprise</p>\n<p>is a business that Salesforce might call a mid-market, right? Like it&#x27;s just it depends on the scale of yourself as a business when you use these terms. &gt;&gt; And all of these things are built in the Bra agent platform like all these automations that people build. &gt;&gt; Yes. Exactly. Yeah. Yeah. And in fact the um most of the operational AI is running on that original platform that we have and we we built it. One element of it that I didn&#x27;t mention is that it um it also most of the UI UX for this platform is uh built in retool. And so like you you can basically go into retool and uh there&#x27;s like a a prompt manager, a tool manager, an email</p>\n<p>manager. Um and that&#x27;s sort of where much of this was built. And the goal with that was again to make it more accessible, more ergonomic to to get started. But what a secondary effect of having a more like visual set of tools for this is it&#x27;s enabled members of the apps organization to go and do prompt refinement themselves. So you don&#x27;t need engineers to go and and refine the prompts or um or even like uh test new foundational models when they come out. I think that that&#x27;s another fun thing when like a new uh when a new model drops uh folks will go into the the platform and basically run the evals on</p>\n<p>the new um on the new model and kind of see like can we get better performance here or does this have different uh different latency or different uh like cost characteristics. &gt;&gt; Yeah, you want the domain experts or the people directly using the tool not the engineers who are sort of somewhat removed from the tool. Y &gt;&gt; uh yeah I I I I do want to highlight to listeners that a lot of the BERS agent platform are just things that every company should have basically uh pro management system which we talked about where the domain experts are doing it multimodel testing evaluation and benchmarking frameworks API integrations for automated workflows MCP based architecture shared with Brexit&#x27;s</p>\n<p>external AI products this one is obviously very Brex specific uh one thing I did want to highlight that I was semi-impressed by because nobody people very few rarely talk about this is knowledge case for understanding Brexit&#x27;s business. &gt;&gt; Yeah. &gt;&gt; So, do you want to expand on that? &gt;&gt; Yeah. And this is an area where we&#x27;ve only scratched the surface here, but but a big a big challenge that that we face is that the world knowledge or the knowledge that&#x27;s built into the model about uh about, you know, what GPT 5 thinks BS does and how it thinks our business operates is actually quite</p>\n<p>different from what our business offers today or how our product works. And so we&#x27;ve had to to work on building a corpus of sort of product documentation, process documentation and like curate this set of information to basically ground a variety of our LLM applications including like that Brex assistant which is like the you know the assistant that employees uh will will talk to. It&#x27;s like we don&#x27;t want it to to hallucinate features that we don&#x27;t have or like give give wrong information there. And similarly like uh some of the operational um uh agents need to be</p>\n<p>grounded on um like what our ICP is because if you ask uh you know CHP5 right now like what types of businesses does BRS uh on board or like what types of businesses does BR serve it might not give an accurate uh explanation to that to that question. It might it might say hey we&#x27;re a corporate car for startups which is what we did you know seven years ago. It might say we&#x27;re only we only serve enterprises. And so that has been an interesting challenge. I think we&#x27;re what we&#x27;ve been trying to do there is I&#x27;m actually going to be spending time with with folks uh talking about</p>\n<p>this next week internally about like can we refresh our strategy and kind of unify it because we have a lot of product documentation that&#x27;s internal for like our operations and go to market teams. We have a bunch of product documentation that&#x27;s external for our customers. We have a lot of uh go to market uh sort of enablement material that&#x27;s more uh sales pitchy and um we have documentation that is put into Sierra which is the you know the the chat assistant that we use for um uh for frontline um support like all of this ideally could draw from the same source</p>\n<p>but right now it&#x27;s uh right now it&#x27;s a little bit fragmented. It&#x27;s just something that we&#x27;re trying to invest in though because I think at the end of the day the duplication of of efforts is just like is is is wasteful and it&#x27;s absolutely necessary to to get this right. &gt;&gt; Uh just to dduplicate uh Sierra meaning the Brett Taylor startup. &gt;&gt; Yes, exactly. &gt;&gt; Yeah. [clears throat] &gt;&gt; I would expect that you have you built so many other agents that&#x27;s that&#x27;s one you can build yourself. &gt;&gt; That&#x27;s like solving problems that are not differentiated enough. uh for us. I think what what&#x27;s interesting about the the Sierra that has been really helpful is that again it&#x27;s really easy for like</p>\n<p>the UI and UX of basically administering a Sierra agent is something that&#x27;s really accessible for the ops uh and CX strategy team which are like it&#x27;s much more low code and uh more sort of workflow and DAG oriented and we have engineers kind of going and giving it tools to to take actions but for the most part like it&#x27;s nice to not have to build build the UX for somebody to manage something like that. And I think the fact that Sierra speaks the language of of customers. Yeah, exactly. Speaks the language of CX that can do all the reporting and the telemetry and stuff that that um our you know VP of CX uh</p>\n<p>would like to see just you know it&#x27;s just one fewer thing that we have to build. &gt;&gt; What about um evals? How do you build evals? Who manages them? &gt;&gt; Well, it depends on uh it depends on the application. So on the on the operational AI side, um those eval are are basically baked into the into the platform around every um every prompt or every agent. And for the most part, I think most of these use cases kind of come online like the V1 of like our our um commercial underwriting agent or the V1 of our our startup KYC agent are co-developed between like a subject</p>\n<p>matter expert in Knops and like an engineer and they&#x27;re going to kind of co-develop um an initial eval set. But then from there generally in ops you&#x27;re always doing QA be it like on humans or on uh on on the LLM uh decisions. And so whenever like as part of our QA feedback loop whenever there is uh a mistake that&#x27;s usually almost always going to result in like another eval being written as like a regression test. Uh so all of that within ops AI is pretty pretty straightforwardly managed. on the product AI side, that&#x27;s where it starts</p>\n<p>getting a little bit more challenging because the multi- multi- aent network um is quite challenging to evaluate. And so what we do there is we try to adopt some of the state-of-the-art for multi-turn evals where we will um we&#x27;ll basically have an agent embody the user and like you know have basically the the end user agent is given an objective and then we basically have it run a multi-turn um uh conversation and then use L misjudge at the end to do all of the different uh asset assessment. The one other thing that we do</p>\n<p>technique-wise that is interesting is sometimes you want to you don&#x27;t want to do like you know I think these multi-turn eval are kind of like integration tests they um they sometimes test more than than you what you want to to assess and so sometimes what we&#x27;ll do is we&#x27;ll also pre-an like an initial preamble to a conversation or maybe a couple turns will be handwritten and we&#x27;ll basically set set the the um eval to start and we&#x27;ll see if uh we&#x27;re able to like isolate certain um certain behaviors. So, uh it&#x27;s it&#x27;s still like a work in progress. And I say like at the</p>\n<p>end of the day a lot of the just um periodic human review and and like looking at um at cases where uh we&#x27;ve detected as we go to like summarize um like what we&#x27;ll do is we&#x27;ll reflect on a conversation after a certain amount of time has has passed where we&#x27;ll summarize it like extract facets like did it seem like the user accomplished their their objective and we&#x27;ll just manually when uh a lot of the cases when that&#x27;s that&#x27;s failed and decide to write another email for it. Mhm. Are all the eval supposed to pass or do you have a set of evals that are like someday the</p>\n<p>model will be good enough and like how could it change over time? &gt;&gt; Yeah, it&#x27;s interesting. Um I don&#x27;t know if we have any that that are like oh someday I hope it&#x27;ll be good enough to do this but it&#x27;s like there there are the evals that are are blocking because they would indicate like a a regression an unacceptable regression. So these tend to be just accuracy related um evals, but then there are others that are more about like tone and uh coherency and these types of things where they&#x27;re they&#x27;re more subjective and we we&#x27;re just looking at those over time as a as a metric. &gt;&gt; Uh but the the team is actually</p>\n<p>interesting. I think we&#x27;re going to get a big update on like how the team is thinking about eval uh our Friday review. So it&#x27;s this is an area where I&#x27;d say the largest challenge like the the largest change we needed to make and how we were executing sort of as like a lab or an incubator back uh earlier this year to like where we are now where we&#x27;ve we&#x27;ve shipped and like we&#x27;re trying to to increase the rigor has been around uh like avoiding regressions and having more and more increasingly robust eval. Yeah, I work with a company called VI that does user</p>\n<p>simulations and I think like that&#x27;s what&#x27;s been interesting. Some of these things they just don&#x27;t expect like the customer does not expect the model to do &gt;&gt; but they want to track the saturation of the model in a way if that makes sense. And I feel like most companies know what they don&#x27;t want to happen but it&#x27;s almost like they don&#x27;t they cannot quite articulate oh I want in the future the model to be able to do this. They can do it today but I I&#x27;ll keep running this eval. That&#x27;s actually really really interesting to me and I I&#x27;m going to take that away and and and start thinking about this because there are there are going to be certain I mean we</p>\n<p>already seen this where where uh users will ask the assistant for help with things that we don&#x27;t support yet or we haven&#x27;t implemented yet. It&#x27;s like those are opportunities actually for us to build a like effectively write a test that&#x27;s going to be fail like failing for for weeks or months and and eventually will go green, but is a way for us to actually kind of show like the progression of sophistication of the assistant. I I really I really like that as an idea. &gt;&gt; Yeah. I wonder how you also catch hallucinations and of things that it doesn&#x27;t have. That&#x27;s usually the That&#x27;s usually the problem is it it&#x27;ll it&#x27;ll it&#x27;ll pretend like it can assist with something and it&#x27;ll uh like one thing</p>\n<p>that is really annoying that has been tough to um to prevent is that the the assistant because it is used to speaking to other agents um that can support it in like accomplishing various tasks. If you ask it to to help with a task that it thinks it probably should have an agent to uh uh to to work with, it&#x27;ll just hallucinate that it, you know, it&#x27;s like, oh yes, I&#x27;ll I will like, you know, I&#x27;ll reach out to the finance team on your behalf to uh to pass this question along, but it&#x27;s not doing anything. There&#x27;s like no finance team. There&#x27;s no way for it to do that. This is something that comes up a lot. It&#x27;s</p>\n<p>like, would you like me to ask the finance team? And there&#x27;s no there&#x27;s no actual tool guards for that. &gt;&gt; Yeah. Yeah. That that was something that we had to &gt;&gt; like a reax like &gt;&gt; Oh, no. But we don&#x27;t I think we&#x27;ve been a we&#x27;ve been able to just beat that up at system with a system prompt. But uh but the we don&#x27;t have as many guard rails in place right now just around a couple of like potential uh like things that could get us into trouble. &gt;&gt; Really? Yeah. I just Yeah. It&#x27;s surprising when I I guess two years ago was first kicking around the idea of all</p>\n<p>these things. I would have said that probably guardrails would be more prevalent, especially in finance use cases, but surprisingly they&#x27;re not. &gt;&gt; Yeah. And that was actually part of what we that was like a feature I believe we built in the L L1 gateway early on is like the the sort of last chance like uh um hard. &gt;&gt; Yeah, exactly. Here&#x27;s some redes. Yeah, exactly. here just, you know, in the way that like if you go way a field on uh chatbt, you just get like the inline 500 error. It doesn&#x27;t even tell you that it can&#x27;t help. It just like craps out. Uh like I we kind of built a couple of</p>\n<p>those circuit breakers or like the ability to put those circuit breakers in and and I don&#x27;t I don&#x27;t believe we&#x27;re using them for anything.</p>\n<h2>AI Fluency Levels: From User to Native</h2>\n<p>&gt;&gt; One last thing I want to get your thoughts on was AI fluency levels. &gt;&gt; Mhm. &gt;&gt; Which you guys have a framework of user advocate builder native and everyone goes through it including Camila. &gt;&gt; Mhm. &gt;&gt; And I just think it&#x27;s interesting. I think it&#x27;s a model that other people are thinking about adopting, but they&#x27;re worried about rolling it out &gt;&gt; that everybody&#x27;s going to be bad and then I &gt;&gt; Well, and also like how do you have like this in-house training course that you keep up to date? Just tell us more about it. &gt;&gt; Yeah, so in in the operations or uh they&#x27;re actually more ahead of even</p>\n<p>engineering on this front as far as like trying to create um create like learning pathways for this. Uh and I think that part of the reason why they&#x27;re ahead of us is that in operations they&#x27;re much more uh they have to be able to operate training at at scale. like training is a big very big part of um of how people build aptitude around their their job function within ops whereas like in EPD a lot of it is sort of uh getting hands-on building experience like going along getting mentored getting code review but uh it&#x27;s been really neat because I think we&#x27;ve really like we created an environment we managed to by</p>\n<p>by speaking openly uh about the the transformation that we saw would happen in this industry towards AI sort of displacing a lot of um a lot of the operations and CX roles and we were just honest about it and I think what what in the same breath that we said hey a lot of these job responsibilities will go away we also said we don&#x27;t anticipate that meaning that your job has to go away it&#x27;s just that your job has to change and so the the fluency framework and then the the like the training and support and like the positive sort of</p>\n<p>culture where we celebrate people making progress has been really helpful for like avoiding a culture of fear or like oh you have to do this or you have you&#x27;re going to get this is going to go in your performance evaluation. I think the &gt;&gt; it does &gt;&gt; it well it&#x27;s not like as wrote as like oh like what is you know what is the like how much are you using AI and is it is it enough it&#x27;s it&#x27;s more I think we&#x27;ve built a pretty like positively frame culture where we&#x27;ll do like spot bonuses for for people who have like particularly novel uses uh of AI on in their day-to-day um in our company all hands every two weeks we&#x27;ll do an AI</p>\n<p>spotlight and it&#x27;s very rarely somebody in EPD for the most part it&#x27;s folks in PTMs ops uh finance the people organiz ganization showing off like how they&#x27;re building agents, you know, in chatbt or on glean or how they&#x27;re they like just found some new use case that they thought was helpful. So, we&#x27;re trying to create create like um I think at the end of the day like we&#x27;ve hired a bunch of really smart people who like I have full confidence that that that this type of work is in within the reach of anybody who&#x27;s motivated to like sort of challenge themselves. And so, we&#x27;ve we&#x27;ve done that. Then in engineering,</p>\n<p>there&#x27;s one other thing that I want to call out because I think that this is kind of fun is that we adapted our interview loop to be more AI sort of aentic coding native. So instead of um we had uh like a coding and a system design question that we basically have revamped into um a project where we&#x27;ll give you like a brief before you come on site and then like an additional sort of spec when you do um when you start. You know, we expect you to use agentic coding to complete the the task. In fact, it&#x27;s like kind of impossible to get all the way through it if you don&#x27;t. And so, we&#x27;re evaluating, you know, your knowledge. Like we&#x27;re kind of watching</p>\n<p>how you work. We&#x27;re evaluating whether you understand the code that&#x27;s coming out. We we, you know, we&#x27;re kind of probing at you as you go. But what we did in order to kind of maybe bootstrap the process of all of our existing engineers, like getting familiar with Agenta coding is that we as soon as we had the interview um ready to ship, we started we said everybody in engineering, including all the managers are going to have to go through this interview. And so we reined everybody internally and it&#x27;s like it&#x27;s one of those things where it&#x27;s like it&#x27;s not a we didn&#x27;t like keep a score like or like you know I don&#x27;t have any data on like who passed or failed or what they what</p>\n<p>they scored but what we found is like as people would take it it would actually cause them to have moments of realization where it was like oh I I can uplevel my skills or sort of like I have like I want to be better at this and so we&#x27;re trying to find like um a way like a variety of techniques to kind of push the culture along. Um, and I think as I reflect on like the year, cuz this is the year where we really put all the effort into it, um, I&#x27;m really satisfied to see the extent to which everybody&#x27;s leaning in on a on a daily basis. Going back to like even I I was shocked when</p>\n<p>we were looking at our cursor logs that like the number one user is is an engineering manager on for org. It&#x27;s like that that is super cool to me. It means that like folks have have uh have taken this to heart and found found ways of um doing their job differently. I guess my I I had a closing question or I guess a parting question and this is broadening out from Brex and this is just you interface with other engineering leaders all the time. &gt;&gt; Did we not cover anything that other CTO&#x27;s are having as top of mind today like their number one problem is underscore?</p>\n<p>&gt;&gt; The thing I find myself discussing with with folks that and I I don&#x27;t want to shy away from like scary topics. Uh, in fact, we were just just kind of on one that was adjacent, which is like how do you evaluate somebody&#x27;s like progression towards being more AI native. &gt;&gt; The the the the cousin to that question is it&#x27;s like will we need as many people um to operate our businesses? Like are there layoffs coming? Are how are we how are we thinking about like um headcount growth? &gt;&gt; Junior versus senior. &gt;&gt; Junior versus senior. Yes, exactly. Like level mix. Um and I still have more</p>\n<p>questions than I have answers there. I think I think what has been really interesting is that I view agentic development as being something that amplifies all the all the good just as much as it amplifies all the bad and amplifies uh uh sloppiness, poor architectural thinking, um uh misunderstanding of of the requirements like there are for all of the the acceleration of good outcomes, it also accelerates bad outcomes. And think what has been interesting is that there&#x27;s been when you sum that all together</p>\n<p>there&#x27;s less of a obvious um like capacity increase. It&#x27;s it&#x27;s more it&#x27;s more nuanced than that. And so I&#x27;m not looking at headcount planning uh as we think about it next year as as being something like oh well because AI is giving us so much more leverage we we don&#x27;t need as many people. Um, we&#x27;ve actually, the thing I&#x27;m really proud of in in my tenure as CTO is that we we haven&#x27;t grown engineering at all. What we&#x27;ve done is we we&#x27;ve grown the business significantly, but we&#x27;ve been able to build like greater efficiencies</p>\n<p>and and how we execute, like how we how we uh we think about building, how we roadmap um what we choose to do and what not to do that we&#x27;re able to uh to serve significantly more customers with more lines of business um without needing to grow engineering headcount. I think that that&#x27;s kind of the way that we&#x27;re going to just continue on this road is like I like having 300 engineers. Like I would love love love to just you know a year from now have 300 engineers but we&#x27;re still you know 30 50 100% more efficient. that that that is the thing that comes up with uh with other engineering leaders and the other part of that conversation is like how much is</p>\n<p>AI getting blamed for just sort of ordinary performanceoriented uh rims you know like if if Microsoft is letting go of like 4,000 people as a business what they have 150,000 employees I believe uh is that really like AI causing that or is it them just using it as a way to uh to avoid some harder like perf management decisions I&#x27;m not entirely sure But I&#x27;m I&#x27;m listening more than I&#x27;m speaking on the on this topic because I every time I feel like I have a pretty firm point of view, some new uh anecdote or experience</p>\n<p>comes in that kind of challenges or invalidates it. &gt;&gt; Yeah. Well, you know, I I take these signals as it&#x27;s my job to go find people who think they have answers and surface them. And you may or may not disagree, but at least you have something to use as a straw man in in your work. &gt;&gt; Exactly. Exactly. And I I think as an industry we&#x27;re just early innings on on on this transformation. So I&#x27;m looking forward to seeing uh uh you know listening to this this podcast episode a year from now and and and seeing you know what we got right, what we got wrong and what&#x27;s different uh because so much changes uh quarter over quarter.</p>\n<p>&gt;&gt; Yeah, I do think AICOE is a very well established pattern. I think uh internal platform is very well established pattern and this uh fluency thing is something that people are figuring out that I think you guys are ahead on. I&#x27;m happy to hear it&#x27;ll be my feedback. &gt;&gt; Yeah. &gt;&gt; Any final call to action for things that you want to buy? Like what should people build for you like problems you&#x27;re trying to solve that you would love people to reach out for to to help with? The call that I&#x27;d make is for folks who are interested in in multi- aent networks to to get in touch with us because I I do feel like this is</p>\n<p>something where where we&#x27;re we&#x27;re innovating in in service of of our customers and where I I feel like the the frameworks, the tooling um and the the research is is is there. There&#x27;s actually quite a lot of like interesting papers and things that we lean on. Uh but I would love to uh would love to see more of that like encoded in the um in the what&#x27;s available at large in the industry because I feel like my intuition has been that trying to graft LLMs into deterministic workflows and DAGs is is kind of underelling like the</p>\n<p>power that they have to actually plan and execute in a more sophisticated like fluid way. and and I and I just want to see like the the industry lean in more um on uh on these agentto agent uh uh interactions. &gt;&gt; Okay. So I&#x27;ll I&#x27;ll dive in a little bit here cuz I have a minor opinion. You keep using the word networks. Is that a reference to a specific paper or it&#x27;s your term for it? &gt;&gt; It&#x27;s just it&#x27;s our term and I think that that is that&#x27;s actually the term that master uses as well um for it. it we um yeah initially we used to call them</p>\n<p>agent run times uh internally and then we just yeah switched to networks. &gt;&gt; Uh and then I think the other thing I wanted to get a clarification on is is it mostly a full agent talking with a full agent or is there kind of like a orchestrator boss agent talking to a sub agent and I think that does matter for a subset of people who are building all these things because when you say multi- aents people don&#x27;t agree what that means. &gt;&gt; Yeah. So it&#x27;s it&#x27;s a tree more than it</p>\n<h2>The Audit Agent Network: Finance Team Agents in Action</h2>\n<p>is a graph. So it is like yeah we have &gt;&gt; and when you say network it feels more of a graph. &gt;&gt; Yeah. &gt;&gt; But it seems more directional as a tree like there there is a hierarchy. &gt;&gt; There&#x27;s a hierarchy. Yeah. But there but there are some violations of that. Like one of the one of the interesting use cases uh and this is where like the power of of having an an assistant for every employee plus having agents that run and and embody members of the finance team is really powerful because there&#x27;s this interesting use case that that we brought to market which is that</p>\n<p>um one of the finance team agents that we we uh launched is an audit agent where like an audit agent kind of embodies is the work that a lot of larger finance teams will do to look for patterns of waste, fraud, or abuse or like systematic uh avoidance of policy that isn&#x27;t as obvious with a single expense. Like you can evaluate a single expense and the metadata around it to see if if it&#x27;s um within policy or not, but um what if you start seeing an employee often make a large number of like $74 transactions when receipts are required at 75? Or what if you what if</p>\n<p>you see um certain things like oh okay there&#x27;s actually a fair number of like Door Dash expenses during business hours from this individual like on on days that an office launch is provided or maybe you see like ride share patterns that are are um where you have to look at broader context. Um so we built this audit agent that can like ingest your SOP and and look also ingest your &gt;&gt; this is a Pikes customers SOP. &gt;&gt; Exactly. Yep. And uh and what it does then is it&#x27;s it&#x27;s basically always looking for potential violations. And what it does is it it is extremely zealous like it it wants to have a</p>\n<p>minimum number of false negatives. So it will raise a large number of potential violations and then a separate agent a review agent will then apply wisdom the wisdom of like is this important enough to follow up on? Is the dollar amount in question high enough? Does this user seem to have like a high compliance behavior? More generally, it makes a judgment call about whether it&#x27;s worthy enough to take that violation and make it into a case. Then once it&#x27;s made into a case, generally what happens is that you need to get more information from the individual. So if humans were doing</p>\n<p>this, there there&#x27;d be some outsourced team that&#x27;s like looking for all the potential violations. Then you have some full-time employee on the finance team who who&#x27;s looking at all the violations. Oh, these are the ones that are important. We need to follow up on it. Then what they do is they hand it off to somebody who will go and slack that employee and be like, &quot;Hey, what&#x27;s going on here?&quot; And so what we have is like the audit agent looks for violations, the review agent decides whether it&#x27;s worthy enough to turn into a case. And then from there uh when the case is filed the that that will trigger an event to the Brexit assistant for that employee and like any additional information about like the business</p>\n<p>justification um can be collected or maybe the assistant already knows because it in its conversation history of the employee knew something about why this this expense looked out of out of policy. And so you start having the the network becomes interesting when you have the finance team agents communicating with uh the assistant for various employees and then behind there you have other other sub aents and so then you start seeing like more of a graph uh emerge but when you look at just what serves the employee it looks more like a tree. &gt;&gt; Amazing. Wow. I didn&#x27;t know you were</p>\n<p>going to go into that level of detail. Yeah. Sorry about that. No no no. I&#x27;m actually really glad I asked. That is very impressive and uh I hope you uh do more content about that. &gt;&gt; Yeah, absolutely. We&#x27;re we&#x27;re really excited about it. I think uh it&#x27;s it&#x27;s been it&#x27;s been good to finally figure out uh a use for for agents and have the technology be as uh like as sort of robust as it is to start realizing this vision cuz it&#x27;s something that we we kind of dreamt of a couple years ago and the tech like to your earlier point the tech just wasn&#x27;t there when we were trying to make the make the a similar concept work with GPT 3.5. It was like, &quot;No, we were hallucinating tool calls in</p>\n<p>uh back in that day.&quot; &gt;&gt; Um, awesome, man. Thanks so much for joining us. This was fun. &gt;&gt; I really enjoyed it. Happy holidays, guys. Thank you for having me. &gt;&gt; Thank you. &gt;&gt; [music]</p>", "chapters_count": 10}