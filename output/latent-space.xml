<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Transcripts: Latent Space</title>
    <link>https://friedchronicles.github.io/yt-transcript-feeds-public/latent-space.xml</link>
    <description>YouTube video transcripts from Latent Space</description>
    <atom:link href="https://friedchronicles.github.io/yt-transcript-feeds-public/latent-space.xml" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Wed, 18 Feb 2026 13:01:16 +0000</lastBuildDate>
    <item>
      <title>[NeurIPS Best Paper] 1000 Layer Networks for Self-Supervised RL — Kevin Wang et al, Princeton</title>
      <link>https://www.youtube.com/watch?v=25FsKN0f8gQ</link>
      <description>From undergraduate research seminars at Princeton to winning *Best Paper award at NeurIPS 2025,* *Kevin Wang, Ishaan Javali, Michał Bortkiewicz, Tomasz Trzcinski, Benjamin Eysenbach* defied conventional wisdom by scaling reinforcement learning networks to *1,000 layers deep*—unlocking performance gains that the RL community thought impossible. We caught up with the team live at *NeurIPS* to dig into the story behind *RL1000:* why deep networks have worked in language and vision but failed in RL for over a decade (spoiler: it's not just about depth, it's about the objective), how they discovered that *self-supervised RL* (learning representations of states, actions, and future states via contrastive learning) scales where value-based methods collapse, the critical architectural tricks that made it work (*residual connections, layer normalization, and a shift from regression to classification*), why scaling depth is more parameter-efficient than scaling width (linear vs. quadratic growth), how *Jax and GPU-accelerated environments* let them collect hundreds of millions of transitions in hours (the data abundance that unlocked scaling in the first place), the "critical depth" phenomenon where performance doesn't just improve—it *multiplies* once you cross 15M+ transitions and add the right architectural components, why this isn't just "make networks bigger" but a fundamental shift in RL objectives (their code doesn't have a line saying "maximize rewards"—it's pure self-supervised representation learning), how *deep teacher, shallow student* distillation could unlock deployment at scale (train frontier capabilities with 1000 layers, distill down to efficient inference models), the robotics implications (goal-conditioned RL without human supervision or demonstrations, scaling architecture instead of scaling manual data collection), and their thesis that *RL is finally ready to scale like language and vision*—not by throwing compute at value functions, but by borrowing the self-supervised, representation-learning paradigms that made the rest of deep learning work.
We discuss:

* The *self-supervised RL objective:* instead of learning value functions (noisy, biased, spurious), they learn representations where states along the same trajectory are pushed together, states along different trajectories are pushed apart—turning RL into a classification problem
* Why *naive scaling failed:* doubling depth degraded performance, doubling again with residual connections and layer norm suddenly skyrocketed performance in one environment—unlocking the "critical depth" phenomenon
* *Scaling depth vs. width:* depth grows parameters linearly, width grows quadratically—depth is more parameter-efficient and sample-efficient for the same performance
* The *Jax + GPU-accelerated environments* unlock: collecting thousands of trajectories in parallel meant data wasn't the bottleneck, and crossing 15M+ transitions was when deep networks really paid off
* The *blurring of RL and self-supervised learning:* their code doesn't maximize rewards directly, it's an actor-critic goal-conditioned RL algorithm, but the learning burden shifts to classification (cross-entropy loss, representation learning) instead of TD error regression
* Why *scaling batch size unlocks at depth:* traditional RL doesn't benefit from larger batches because networks are too small to exploit the signal, but once you scale depth, batch size becomes another effective scaling dimension

—
RL1000 Team (Princeton)

* *1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities* (https://openreview.net/forum?id=s0JVsx3bx1): https://openreview.net/forum?id=s0JVsx3bx1

00:00:00 Introduction: Best Paper Award and NeurIPS Poster Experience
00:01:11 Team Introductions and Princeton Research Origins
00:03:35 The Deep Learning Anomaly: Why RL Stayed Shallow
00:04:35 Self-Supervised RL: A Different Approach to Scaling
00:05:13 The Breakthrough Moment: Residual Connections and Critical Depth
00:07:15 Architectural Choices: Borrowing from ResNets and Avoiding Vanishing Gradients
00:07:50 Clarifying the Paper: Not Just Big Networks, But Different Objectives
00:08:46 Blurring the Lines: RL Meets Self-Supervised Learning
00:09:44 From TD Errors to Classification: Why This Objective Scales
00:11:06 Architecture Details: Building on Braw and SymbaFowl
00:12:05 Robotics Applications: Goal-Conditioned RL Without Human Supervision
00:13:15 Efficiency Trade-offs: Depth vs Width and Parameter Scaling
00:15:48 JAX and GPU-Accelerated Environments: The Data Infrastructure
00:18:05 World Models and Next State Classification
00:22:37 Unlocking Batch Size Scaling Through Network Capacity
00:24:10 Compute Requirements: State-of-the-Art on a Single GPU
00:21:02 Future Directions: Distillation, VLMs, and Hierarchical Planning
00:27:15 Closing Thoughts: Challenging Conventional Wisdom in RL Scaling</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>[music] Light space to fire up. [music] >> So, welcome to L Space. Uh, we are basically trying to provide the best optimal sort of podcast experience of neurops for people who are not here. Uh, and congrats on your paper. How does it feel? >> Yeah, it was very exciting. Um, >> yeah, >> we had a poster yesterday uh yesterday and then today we'll have an oral talk. >> We just like mobbed. Oh yeah, there was a lot of people. It's like three hours straight of like you know like waves of</p>
<p>people to like that were we were trying to but >> so I've never received the best paper. Do you just find out on the website like what uh >> oh I just like woke up one day and like checked my email and then >> ah they just tell they just >> they was like oh like it's like I just saw email oh you award like been awarded best paper on >> but maybe you know from the reviews as well right? Yeah, we know from the reviews that we did well. Um, but there's a difference between like doing well in the reviews and getting best paper. So, that part we didn't actually know.</p>
<p>>> Yeah. >> Okay. So, I I I skipped a little bit. Uh, maybe we can go sort of um one by one and and sort of introduce um you know, who you are and what you did on on on the team. >> I'm Kevin. I was an undergrad from from Princeton. I just graduated and yeah, I guess I led the project like started the project and then was very happy to collaborate with Isan and Nicole and Ben also. Um, >> right. And were you in like the same research group? Like how do you how what's your social context? >> So, so yeah, so we're all from Princeton. Yeah. >> Um,</p>
<p>>> with and thanks to Ellen for booking you guys. >> So, this project actually started from like an IW seminar. So, uh like like independent work research seminar uh that Ben was teaching >> and this was like actually like like one of my first experiences in like ML research. Um, so it was really valuable to like get that experience and then Ean was also in that seminar and working on adjacent things. And so we collaborated um a lot during that seminar and then yeah the project turned out to have some pretty cool results and then later on also like the whole working on sort of similar things also um joined on the project and became like a good</p>
<p>collaboration. >> Yeah and um I I don't know if any of you guys want to want to chime in on like other elements of coming into like deciding on this uh problem. So it's like broadly my lab works on deep reinforcement learning but historically deep meant like two or three or four layers >> not 10,00 [laughter] >> when Kevin and Sean mentioned they wanted to try really deep networks kind of skeptical it was going to work. I've tried this before it doesn't work other people have tried this before and doesn't going to work. So I was very very skeptical starting out. I don't</p>
<p>know if I made this at the time, but that was my prior going in because >> But do you do you view your job as like screening or like, hey guys, this is probably isn't going to work. You should try a different idea, you know, like or should you be encouraging even if it's dumb? >> It's selecting bets. >> Yeah. >> And this was a bet I was willing to make. >> What what made you willing to make a bet? It seemed relatively low cost uh in that we Mihal in particular had spent</p>
<p><strong>[03:03]</strong></p>
<p>the past year developing infrastructure that made it a lot easier to run some of these experiments and the precedent was deeper numbers could do a whole lot better like that's what the deep learning re revolution has been over the last decade >> yeah I know why do we stop making them deeper [laughter] >> uh and reinforcement learning was like this one anomaly where we continue to use these really shallow networks and that's particularly true in the settings that we were looking at where you're starting from scratch you're starting from often. >> Any other perspectives you guys want to chime in with? >> I guess maybe I should just go over like an overview of our project. >> Yes. Okay. Sorry. Yes.</p>
<p>>> So, the way that I kind of view our project um is that if you look at the landscape of deep learning, you know, you have NLP like language, vision, and then RL. And like as Ben kind of alluded to, you know, like in in language in vision, we've sort of converged to these like paradigms of scaling to massive networks, right? Like hundreds of billions of parameters, trillions of parameters. And there's been you know a lot a lot gained from in deep learning from from that right and then but then it seems like in the third sort of branch of deep learning in deep RL that has not yet been the case. Um like I was very surprised like coming into some</p>
<p>like you know Ben's class and seminar when I was looking at the networks. Oh why were you just using like a simple like two-layer MLP for like these frontier sort of you know state-of-the-art RL algorithms. Um and so I was very curious like can we design RL algorithms? can we sort of put together a recipe for RL that can allow it to scale in potentially you know analogous ways that language envision might scale and so what we did is that we know that traditional RL like let's say like value uh value based RL doesn't really scale right this is pretty clear from the literature so we tried a</p>
<p>different approach to RL um called self-supervised RL where instead of learning like a value function we're learning representations of states actions and future states such that the representations along the same trajectory are pushed together the representations longterm strategies are pushed apart and this is just like a different approach to uh RL that's allows us to learn in a self-supervised manner so there's we can solve task reach goals without any human crafted reward signal and so we know that self-supervised learning is scalable in these different areas in if deep learning so can self-supervised RL scale</p>
<p>in similar ways um when we first tried it it actually didn't work like we made the networks deeper the performance like totally degraded but then we also but then I separately was like there's also some other work like um in in our literature like we tried like residual connections um and then there's other a few other architectural components that we had to put into the recipe and then all of a sudden like one day like I ran this experiment and there was like this one environment in which there was like like going from like like doubling the depth didn't really do anything but like</p>
<p>doubling the depth again with these different components suddenly like skyrocketed performance in this one environment. Getting this to work was very non-trivial in the sense that like usually when we need to think about doing hyperparameter optim optimization we try changing A see if it makes it better try changing B see whether it makes it better and if we just made the depth bigger makes it worse we just add residual connections didn't make it better and it was really this combination of factors that Kevin and</p>
<p><strong>[06:04]</strong></p>
<p>Ean figured out that really made this work >> and as a precursor to that we also tried scaling along different dimensions so scaling the batch size uh scaling the the width of the network so the hidden layers and Yeah, pretty much kind of similar to just scaling depth naively. Um, and then once we started introducing residual connections, layer these specific architectural choices, that's when we saw these significant jumps in performance like these critical depths at which performance multiplies by a pretty huge factor and that's where we really noticed like unlocking some significant performance gains uh as</p>
<p>opposed to scaling just along with which did yield some performance improvements. Um, but when you look at the number of parameters that your network has as you grow width, it's roughly a quadratic as opposed to something like growing depth. So it's more in some senses more parameter efficient, also more sample efficient from the experiments that we conducted. >> Nice. Um, in some ways you're sort of replicating stuff that is seen in the wild but on on a very small model that you can study. Is that would you would you say that's >> Yeah. So I kind of add to what Kevin said earlier. We saw these huge</p>
<p>performance improvements in language models, image generation models by making them larger, making them deeper, which seems very intuitive. Yeah. >> Uh and so that's why our work we we draw from like foundational research, right? Like uh residual networks which employ residual connections to avoid uh vanishing gradients. And that's something that we show in some of our ablations in our in our paper like further down probably in appendices. We did experiments without these residual connections. And so sort of borrowing these concepts that have existed in other fields and applying them to this setting with uh RL and showing that it</p>
<p>works. >> Before Ben has to has to go, I'll leave the sort of last word uh to him. What additional work does this inspire that like that that you want to push on next? >> I think there's one thing I'd clarify about the paper and then I'll directly answer the question. I think the thing I might clarify about the paper is I think a lot of people reading the title are like wow big networks they're great. I'll take big networks and >> you solved it now. We can just go. Yeah, we say big network add them to PO, add them to SACE, add them to your favorite reinforcement learning algorithm. But I think that's actually not the main conclusion. I think the main conclusion is that using big networks not only</p>
<p>requires these architectural tricks, but also as Kevin mentioned before, it requires using a different objective. This objective doesn't actually use rewards in it. And so there's another word in the title, reinforcement learning, that also might be a little bit of a misnomer because we aren't directly trying to maximize rewards. Our code doesn't have a line of code saying maximize rewards here. And so is at the end of the day this a reinforcement learning method? I don't know. It looks much more similar to the self-supervised methods in other areas of machine</p>
<p>learning. And so I think that the method the work really stands in some sort of interesting intersection of reinforcement learning and self-supervised learning research. And we had this little figure on the bottom left of the poster which was the screenshot of a slide from Yan Lakun talking about how to build intelligent systems and whether that's going to be done by unsupervised learning or</p>
<p><strong>[09:04]</strong></p>
<p>supervised learning and reinforcement learning. And I think what her paper really suggests is that the boundary between these things is really blurry and maybe the keys to building intelligent systems are going to be lever insights from all of them. >> Yeah, the layer kick. >> Exactly. [laughter] >> U well thank you for your time. I know I know you have to go soon for Jon. >> Yeah, thank you so much for coming. I I think that that insight of like blurring things is interesting. I I don't know if you like you were talking about so like uh the abstraction layer of representation learning. I don't know if if that triggers anything in terms of like the mix between self-supervised and</p>
<p>reinforcement learning. Is that is that something fundamental that you've discovered or that we that people don't understand when they when they read the paper? Yeah, I think the best way that I would explain it is that we know that standard RL is not super scalable. And so like why can this different approach or different objective RL be scalable? I think it's because we're fundamentally shifting the burden of learning from something like like Q-learning or like regressing to like TD errors which we know is quite sperious and noisy and biased to fundamentally like a</p>
<p>classification problem. We're trying to classify whether future state is along the same trajectory or along a different trajectory. And we do this with representation learning right and we know that classification cross entry loss and representation learning is scalable in the deep learning literature right if we think about language um and like some of the objectives there so in some sense we're kind of blurring the the lines we're doing reinforcement learning it's still an actor critic reinforcement learning algorithm it's like a goal condition reinforcement algorithm but the objective the burden of like learning of that of of solving</p>
<p>that RL task shifts to something that's more similar to what's objective that you might see in language in vision that we know have scaled so much and so I think yeah I think that's like one of the fundamental insights that we've seen is that um it seems like by approaching RL in this different approach we were able to like get so much more out of we were able to scale our networks like significantly beyond what is like standard used in RA >> can I jump in I will just give a bit of more of context about the architecture because uh yeah we use another objective the uh influency so the contrast plastic</p>
<p>plus. However, the architecture is quite similar to the previous works uh of previous uh papers like bra or or uh simba simba and simba fu 2 uh simba one simba 2. So we we also tweaked a bit of this uh architecture. However, it's not that we like uh invented the wheel for the first time. It's the merging between the architecture and the objective that makes the scale uh really uh like go up and and performance follow the the</p>
<p>scale. >> I think that's something that we should uh probably mine deeper. Um do you think I guess like what domains what industry like if you you've applied it on on multiple different uh types of networks or or data sets is there a particular affinity that you think like has is like sort of low hanging fruit? >> Yeah. So actually if you look at a lot</p>
<p><strong>[12:05]</strong></p>
<p>of our tasks they're particularly sort of like robotics tasks. Um so this is personally I'd be very curious about how a work like this could impact like the robotics field. Like my understanding of robotics is that a lot of robotics right now there's kind of multi a few different approaches. Like one approach is we want to train robots using imitation learning. So we try to collect like an insane amount of data. But we have a ton of cub human cuban civ supervision and we try to scale up this data and we're like learning with imitation learning like but on the other hand potential like perhaps there's another approach which is like for example like goal condition</p>
<p>reinforcement learning where we can actually train robotic agents and trillion RL agents to solve meaningful tasks with absolutely no human supervision no demonstration >> it's much more scalable yeah >> so yeah so this could serve as an alternate approach and perhaps instead of like scaling data like scaling manual like human supervision which which is you know not super scalable If there are ways to sort of make goal condition reinforcement learning scalable and like we can just scale the architecture or we can scale >> because you're focused on your objectives. Yeah. >> Right. What's with certain different objectives? I think that could be very exciting and see to see how that can affect a field like robotics for</p>
<p>example. >> Yeah. Uh double click on on just one one thing on the efficiency which you you was talking about. I would expect very deep the deeper it is this should be quadratically worse. I I'm not familiar with like the the pre-existing literature. I'm just like sort of working out intuitions. But um >> basically uh what are the trade-offs that you've found that I think you might want to warn people about >> because because you you were the guy who mentioned efficiency. So >> sure, sure. Yeah. So I was referring to like one of the figures on our poster also in our paper where we compare like</p>
<p>the number of parameters that models have as we scale along the axis of depth and as we scale along the axis of width. >> Yeah. from our baseline architecture, the most baseline one would be like a width of 256 like the hidden layers have 256 neurons and then the depth is four four layers or hidden layers. Um and so the point I was making there is that when you scale along depth you're the number of parameters that your model has is going to grow roughly linearly. Uh whereas with width you're making your network outputs wider and then the input to the next network is also growing as</p>
<p>well. And so the the number of parameters your network's then going to have it grows approximately quadratically. And so one of the experiments we did was sort of examining as we grow the number of parameters in our model by scaling along these two different choices which one for the same like approximate number of parameters yields a better performance. And the depth curve kind of goes like this. It jumps up pretty fast. That's like present throughout our paper. For with it grows a little bit more slowly. And so that the kind of takeaway from that is that if you are a bit more resource constrainted scaling along that might be</p>
<p>better because there's fewer parameters with a smaller model to a smaller number tool learnable parameters >> with is expensive >> which is expensive. Exactly. And in general of course like more parameters is also going to be more expensive. So that's just like another consideration to think about when using these networks and I suppose. >> Yeah. Any other sort of rules of thumbs like that that I can extract that? This is just the most basic one that I could think of. >> Yeah. Uh I don't know if there's any others.</p>
<p><strong>[15:05]</strong></p>
<p>>> Yeah, I guess like to your original question of like the trade-offs um like one of the trade-offs, one of the limitations that we say is like obviously if you make the networks bigger the it it will takes longer to run, right? So if you like double the depth at some level of depth you you it might take like twice as much to like take make a forward pass through the network, right? However, this is not so like within our paper like for most environments um we are able to like saturate like get to like almost perfect performance within just you know we don't even need to get to like a thousand layers like maybe just 64 layers for example is sufficient um and</p>
<p>in this regime like like the latency of the network is not necessarily actually even the uh not necessarily like a significant bottleneck like you can imagine there's a lot of tasks in which especially in RL that like collecting data might be the bottleneck right and making four passes through our network may not be the bottleneck. And so in our environment, we in our research, we specifically used the Jax GCRL environment, which is a Jax based GPU accelerate environment. So we can collect like thousands of like environment trajectories like in parallel at the same time. So that we're</p>
<p>able to like make uh like oh this is built in, >> right? This is built in so that we can collect you know like like a thousand trajectories at the same time along all these environments and so um makes that make sure that like we have enough data to like saturate the learning from >> wow >> that's like work data columns >> okay and you I don't know if you want to explore expound upon that on the drug maybe >> and you know most people are familiar with Pyth less familiar with Jax >> with J I think Jax is getting the uh the traction especially in RL field because</p>
<p>the in for online reinforcement learning getting as much data as you can is is the most important. There's got to be a pie to equivalence. But anyway, >> how are any tips for other people also exploring this kind of roll out? Yeah. >> Yeah. So, I think I can also recommend like uh for for go conditioned RL, I I'm recommending JRL, but there are also like multi- aent J implementation and others. So going back to our paper, if you look at the plots, we only see this like huge performance increase when we</p>
<p>cross like 50 uh millions of uh transitions gap. So so I think the data is crucial like here. Yeah, I guess even to build on that like I I like drawing analogies to like successes in other areas of deep learning like for example in large language models the reason why we're able to scale to such large networks is that we found a paradigm in which we can leverage the entire internet scale of data to learn right >> and so data in RL traditionally has been hard to come by um but now with these</p>
<p>like GPU accelerate environments we can collect hundreds of millions of time steps of data within just a few hours and so I think that this serves as like a really good test bed for us to be able to also find ways to scale up um like network capacity and get similar kind of gains. >> I think that has to go. >> Are you saying that you have a difference you would do pre-training differently in LLMs? Like what's the</p>
<p><strong>[18:05]</strong></p>
<p>what's the difference uh objective now? Um yeah, very simply the the paradigm that you're referencing is next word or next token, right? >> It's very robust. [laughter] >> Yeah. >> How do you change that? Oh, I'm not saying I want to leverage insights from that to apply to morale. >> I I feel like you should go the other way. >> You think you should go other way? >> Maybe. I mean, that would be a very interesting research direction, too. But actually, yeah, even on that point, like one of the things I was thinking about is that the way that our our objective works is in some it's not exactly next</p>
<p>word prediction, but it's kind of like next state prediction, right? You imagine you're at some current state and you're at some current action and we want to predict whether or not this future state this this certain state is a future state along the same trajectory or a different trajectory and so in some sense we are actually doing some sort of like >> implicit world model >> implicit like uh you know like in >> I don't know if that's a bad word these >> or like in language you you do a cross entry loss to classify the next token right and here we're just doing a binary classification of like whether or not some next state is some</p>
<p>>> yeah yeah it's a classification yeah >> and so I do see that there are some like sort of parallels here that perhaps we should dig into deeper and see like what is the core to of what enables deep learning to scale and then how can we like leverage that how how we can distill those like insights and then apply those across like all different fields whether it's language or reinforcement learning. >> Yeah. Uh did you did you get my my meaning about the world model stuff? >> Yeah. Yeah. actually and I I heard I think I might have heard professor Eisenbach yesterday talking about this at a poster and he's explaining to a</p>
<p>couple people that because this is like doing representation learning and trying to learn these meaningful representations for a given state and action but for a given goal in some sense you can think of it almost like learning a model of the environment learning a model of the world but without having to do any sort of like next frame prediction or stuff like that that's a little bit more highdimensional and complex. Yeah. >> Yeah. I I will think like um the the angle that I'm trying to think about and push is instead of learn the next world they're basically like generate a number of candidates possible worlds and</p>
<p>classify them uh to your point uh which is exactly how I do things. Let's say I'm playing poker and I'm trying to classify what hands you have. Well, there's a range of hands based on what you're you're doing. And the more information I get, the more I resolve to, oh, I know exactly what hand you have based on what you're showing, you know, or you're buffing. But that's that's a different thing. But you know what I mean? Like that I I feel like that is the ultimate sort of end goal of representation which is a world. But I don't know if that is too vague compared to the more concrete types of world</p>
<p>models that let's say the video gen people are doing. >> And then I I guess one one other thing like I'm also exploring I you mentioned like the deep models being slower or more expensive. Yeah, that that is a trend in the inference world of making models shallower, right? And I wonder if this like short catchphrase I was thinking about like deep teacher shallow student would be a good</p>
<p><strong>[21:05]</strong></p>
<p>deployment paradigm. >> Yeah. >> Like you push the frontier capabilities with the with with F and then you distill >> distill it back. >> I actually this is like a good point like if you go out to our website like this is one of the future directions that we list at the very bottom. >> Okay. >> Yeah. uh we we we we would love to see if we could get similar performance like we pushed the you know like we do achieve state-of-the-art performance on u gold condition RL in Java CRL by a significant amount and so it was very exciting to see the like the the sort of frontier of the ability to train RL agents uh sort of pushed um and if we</p>
<p>can do that in a way that also sort of is just as efficient as a standard uh you know networks that would be very cool so you know like is able >> yeah because training uh doesn't have to be the the same thing that you deployed inference, >> right? You know what I mean? Like >> Yeah. So, yeah. So, if there's ways to like distill down to a smaller model or prune the model and maybe not still retain performance, that's a very interesting research direction that we're choos what what else is your personal passions? >> Yeah. So, uh currently I'm pursuing direction of uh stitching in</p>
<p>reinforcement learning. So we are trying to generalize uh reinforcement learning from shorter subbehaviors so that they are stitched merged uh during the test time and uh yeah I think this is one of my uh last papers that I will tackle during the PhD. >> Personally I would I'm very curious of like can we like what's the like real like can we push I'm I'm I'm curious about like advancing the frontier as much as possible. Um so if you actually look at our paper we focus on scaling</p>
<p>depth but we notice that we see that scaling width actually also improves performance and we also find that actually by scaling depth we actually unlock the ability to scale along batch size as well. Um so this is one of Yeah. Uh so so okay so I guess >> colinear like yeah >> right so like okay I guess for context like in traditional RL like value based RL scaling batch size is not super effective but there's we also can see there's also other work in other areas of deep learning that show that scaling batch size is only most effective when there's like a large enough network capacity to take advantage of the scaled</p>
<p>batch size and we actually find that you know perhaps so one hypothesis might be like perhaps the reason why scale batches isn't that effective in traditional RLS because like we've been using these tiny networks that haven't be able to capture And one of our experiments is that like because we are enabled successful training of deep network we actually were able to this is a great test bed for you know like testing this hypothesis and we find that indeed as we scale the network capacity we also unlock this different dimension of scaling by our site and so all that to say is that I'm very curious for someone</p>
<p>like with enough compute to like take some of these environments scale up batch uh scale up depth to the maximum capability also scale along width also scale along batch size And let let's like basically like in the same way that in language we're scaling on so many different aes can we unlock different dimensions of scaling as well and what capabilities and how far can we push the frontier of training these RO agents from doing that >> before we pass Sean uh when you say</p>
<p><strong>[24:05]</strong></p>
<p>enough compute what kind of compute budget did you have how does it how I just want to see what you guys got >> good question so we we wanted to make sure that this is we we wanted to make it such that like uh you know it's quite accessible so actually the nice thing is that all of our experiments even the thousand networks can be run on one single 80 gigabyte H100 GPU. >> Um, >> so that's dollars. >> Yeah. >> Right. Right. Right. So everything can be run on one GPU. Um, but in theory if we had, you know, like a distributed training setup and like can just like blast compute through this and really wanted to push the frontier, it'd be</p>
<p>very interesting to see how things go. >> Yep. Cool. >> Uh, and I've actively been trying to learn as much as I can about vision language action models, um, role models at Europe's going to a lot of >> machine language action models. >> Vision language. >> Vision language. Yeah. Yeah. Um and yeah, curious about applications of representative for these. Yeah, exactly. For robotics. Um actively trying to explore more in that area. So just reading a lot of literature, talking to as many as I can. >> Yeah, we just released our episode with uh General Intuition. >> Okay. Um where if you know a bit about their history, they started as a gaming</p>
<p>clipping company and uh they basically have a vision language action model >> which um I I saw I saw a preview. It was very impressive. I'm not sure exactly how transferable it is to embodied use cases, but it doesn't have to like screen is fine, you know, like Yeah. I I don't know if you have any takes on. >> Yeah, it's an exciting research direction. Definitely. >> Yeah, I I think um the the the the concept of actions as as something that you are outputting is actually not that</p>
<p>popular in industry, right? O only because text has completely dominated the last three years and tool calling and which is a just another form of structured text. Uh and and I I feel like the uh action research is is kind of like I don't know how I don't know what needs to happen in order to unlock the next phase in in that. I don't know if you anything interesting out here shout it out. Yeah, there's a lot of cool work on like leveraging pre-trained VLMs and</p>
<p>>> you freeze it and then you apply and then you on top of that like some sort of experts to output actions. Um also like systems for doing like hierarchical planning maybe outputting some higher level plan that and this is like a larger network that takes a long time to a little longer to do inference and so it outputs its plans with less frequency like some sort of chunk and then from there there's like some sort of uh second system that operates a bit more fast. I think there's quite a bit of interesting research in that direction. So that's sort of what I'm looking forward to.</p>
<p>>> Cool. Final question. Uh hardest question you were asked at the postal session or just favorite encounter anyone famous that you met. >> So I actually haven't gotten a chance to go to the conference that much. I'm actually working full-time now. So uh [laughter] yeah. Uh so so far I I actually literally just got my badge like a few moments before session. So I guess I wouldn't be the best to answer that question. >> No, no, no. Like you like people ask you</p>
<p><strong>[27:07]</strong></p>
<p>stuff, right? Oh. Oh, that might I might post >> because people asking you or meeting you and like you know just just give a vibe of like what people are saying and >> yeah I think people were very I think it it's sort of like a very eye opening I think that the general question is that people thought it was a very eye opening paper because like the objective is quite simple. It's quite elegant and for us to be able to like you know like I I don't want to say like overturn but like sort of challenge the conventional wisdom that like RL is not super scalable and push it to such limits like a thousand layers deep and see continue</p>
<p>improve performance. I think the general impression that I've gotten is that, you know, this this this could be like a really cool like if we can sort of build along this direction and that like we can really scale along all these different dimensions and push the frontier of the ability for RL. I'm very curious to see how that goes. >> All right. Well, thank you so much for dropping by. Uh, congrats on the paper again. Yeah. >> And, uh, good luck in your future work. >> Thank you. Thanks for having us. Yeah. Yeah. Thanks. >> [music]</p>
<p>[music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=25FsKN0f8gQ</guid>
      <pubDate>Wed, 31 Dec 2025 16:00:04 +0000</pubDate>
    </item>
    <item>
      <title>[State of AI Papers 2025] Fixing Research with Social Signals, OCR &amp; Implementation — Team AlphaXiv</title>
      <link>https://www.youtube.com/watch?v=T0mZJjl_dsA</link>
      <description>From late-night dorm-room hacking sessions at Stanford to building the most-used platform for navigating AI research, the founders of *AlphaXiv* have spent the last few years watching the archive firehose explode from thousands to 30,000+ papers per month—and building the intelligent layer on top that researchers, applied engineers, and even VCs now rely on to make sense of it all. We caught up with *Raj, Rayhahn, and the AlphaXiv team* live at *NeurIPS 2025* to dig into the origin story (spoiler: it started as a class project with a "view comment" button next to archive paragraphs), why they beat Hugging Face Papers by obsessing over UI and getting Laura's authors to comment directly on their own work, how they've evolved from comments to feeds to benchmarks to AI assistants that answer paper questions using Gemini's 1M context window, what's broken in the academic review process (20% AI-generated reviews at ICLR, quality collapsing under exponential submission growth), why *papers are becoming less important* than implementations, Docker containers, and real-world usability, their vision for making it trivial to spin up any trending paper's code directly in your browser, and the power law they see every day: archive has 2.4M papers, but applied researchers only care about the top 0.1% that are actually implementable—and AlphaXiv is building the ranking, tooling, and social signals to surface exactly those.
We discuss:

* The *origin story:* from a late-night web dev class project ("view comment" buttons on archive PDFs) to going viral on LinkedIn and becoming a full-time startup backed by Stanford advisors
* Why AlphaXiv beat *Hugging Face Papers:* better UI, direct commenting on the paper itself, and early traction with high-signal authors like the Laura/DPO/Llama teams
* The evolution: *comments → feed → benchmarks → AI assistant,* using social signal (views, comments, Twitter) to filter the archive firehose and surface what actually matters
* How they handle *PDF parsing and OCR:* DeepSeek Coder for cost/accuracy, Mistral OCR APIs, and passing diagrams directly into multimodal models like Claude
* Why *semantic search alone fails:* 3M papers, buzzword overload, and the need to weight by social signal (views, engagement) to return actually relevant results
* The *state of academic publishing:* exponential submission growth (30k/month in CS), 20% AI-generated reviews at ICLR, review quality collapsing, and the rise of AI-written papers flooding the system
* Why *papers are becoming obsolete:* the future is implementations, Docker containers, and interactive sandboxes—not PDFs describing what code does
* The *power law:* 2.4M papers on archive, but applied researchers (Spotify, Expedia, Nintendo) only care about the top 0.1% that are actually implementable and useful
* AlphaXiv's roadmap: *Docker containers for papers,* agent-automated setup, ranking by "implementation ease," and making it trivial to spin up trending work directly in your browser
* Papers of the year: *Tiny Recursive Models (TRMs),* evolutionary strategies at hyperscale, Agent R1 (RL-trained agents for complex reasoning), and the rise of AI for science (Agent Laboratory, Bionemo)
* The *Qwen vs. DeepSeek* dynamic: Qwen's breakout year, DeepSeek's continued influence (OCR models triggering a flurry of competitive releases), and the open-source fine-tuning ecosystem around both
* Why *AI reviewers* could help (Stanford's Agent Reviewer, using AI as a linter for clarity and quality) but assessing novelty and lit review remains hard without search + social signal
* The vision: AlphaXiv as the *tool layer for research*—not a social network, but the best place to discover, understand, and implement the ideas that actually matter in AI

—
AlphaXiv Team

* AlphaXiv: https://alphaxiv.org
* X: https://x.com/alphaxiv

Where to find Latent Space

* X: https://x.com/latentspacepod
* Substack: https://www.latent.space/

00:00:00 Introduction: The Origin Story of AlphaXiv
00:03:12 Beating Hugging Face Papers: What Made AlphaXiv Different
00:04:17 From Comments to Discovery: Building the Research Feed
00:04:55 PDF Parsing and OCR: The Technical Infrastructure
00:06:01 Becoming a Company: Beyond Papers to Research Artifacts
00:06:37 Benchmarks and State of the Art: Replacing Papers with Code
00:07:54 Personalization and Search: Building the Best Research Assistant
00:08:54 Docker Containers and Reproducibility: Making Papers Runnable
00:10:52 Favorite Papers: Tiny Recursive Models and Evolutionary Strategies
00:13:15 AI for Science: Agent Laboratory and the Future of Research Automation
00:18:41 Agent R1 and RL for Long-Horizon Tasks
00:20:54 Qwen's Year and the Open Source Landscape
00:22:43 The Crisis in Academic Publishing: AI Slop and Review Quality
00:25:04 Search and Social Signals: Why Semantic Search Isn't Enough
00:28:15 The Future of Research: Beyond the PDF
00:31:17 Implementation Ease as the New Ranking Signal</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>[music] Light space up [music] light. >> Hi guys, welcome to Lind Space. Thanks for having us. You guys are the founders of Alphacive, which I've confirmed is the official pronunciation. What's uh each of you's uh sort of origin story of coming together and starting this? >> Sure. Yeah. So, I'm Raj. Um I I guess all of us were were at Stanford before. I was roommates with Rayhon actually. um</p>
<p>and and still am for last like 5 years and I guess um yeah we I would started off doing you know AI research like a lot of others uh at Stanford and um was in Dorsa's lab also working with Percy a little bit and was kind of working at the intersection of robotics and NOP and I think um honestly like the the real origin story is one day Rahan and I we were in like our operating systems class it was like really late at like 1:00 a.m. Rahan's like, "Hey, do you want to</p>
<p>see my like final webdev class project?" And I'm like, "Hey, what makes you think I want to see that right now? We're trying to get this like pet in and go home." And he was like, "No, you should look at it. It's pretty cool." And and I was like, "Okay, sure. Show it." And it was basically like uh like really jank like view comment button next to like every paragraph of like an archive paper. And the idea is like, "Hey, you could like comment on papers." And I was like, "Hey, this is like kind of cool. we should like just like work on it. It's kind of strange that there aren't</p>
<p>comments on archive papers given that how how many people are reading and like AI papers is are growing exponentially. So, we kind of just like started to work on it for fun. We showed it to like our lab and they're like this pretty cool like they'd use it if it if it takes off. But I think um it really became serious when like it went viral like one of our friends um who was at fair at the time posted about it on LinkedIn went viral and yeah from there we were like we started working with Sebastian who was our adviser um and yeah um it's been pretty cool. Yeah, I online. Um, yeah,</p>
<p>my name's Rahan. I before working on Alpha Kai as an undergrad was doing uh research in robotics and RL. A lot of this was still before GPT. So, the thinking with as an undergrad as as as a meager undergrad, your PhD mentor gives you papers and you struggle to understand them. And so, the thinking was that there should be some stack overflow analogy for for archive papers that surely there are other undergrads out here that have these questions. Um and obviously since then it's taken many forms but that was kind of the initial initial iteration. I</p>
<p>>> mean like Raj and Ron I also did research in undergrad um this sparse deep burning research with Stanford's group and I think kind of like what is like we all had this shared experience as undergrad researchers and it was like it seems very simple you know just uh the simple premise of comment section for arcade papers. >> Yeah. Um, and it was just sort of a project that we worked on, you know, 20, 30 hours a week whenever we had, you</p>
<p><strong>[03:01]</strong></p>
<p>know, free time. Um, and obviously it's just sort of sort of launched from that. Um, and it's been a really exciting journey. I guess the other the the reason that when I first saw Alpha Kive, I was I didn't necessarily think about it as new was because I knew that Hugging Face had also launched like a paper discussion thing and like, well, so you know, Hugging Face is huge. Why did you seem to win or break out versus them? >> Yeah. >> Yeah. I think there were a few things. Um, even when we were just comments, I think we I think the core of it was we were our products ICP. Like we knew</p>
<p>hugging face papers existed. Um, we felt like some of those problems were unsolved. Something as simple as being able to comment directly on the paper that the user interface. I think when we started we were very deliberate about getting authors like Laur from Laura DPO Lama and actually having really useful cool exchanges and then those would get shared and >> did you like reach out to them and say hey please do it here. >> Yeah we would use our existing kind of like people from people we knew from research >> that's rough. Yeah. >> Yeah. But but from there it would grow like really really quickly. Um and I</p>
<p>think from comments there's been a really natural progression that we've we've taken that hasn't been with other kind of paper platforms. So when commenting took off, we're like, "Okay, we have a good idea of what papers people are reading, right?" So if you're trying to discover papers, on one end of the spectrum is like the archive sort by new, right? So you have like a thousand papers every day sort by new and on the other end of the spectrum is like Twitter. So like, okay, actually this is a good chance to put together some higher signal feed of papers based on the interaction. So commenting became a feed of papers and then from there it's like, oh, a lot of the questions people</p>
<p>are asking can be probably answered by AI. Gemini came out with the 1 million context thing. Okay, let's throw that on here. So we've kind of progressed and built a more cohesive experience just growing our user base um and and and pulling that thread. >> Just on a Gemini note, do you parse uh PDF to text and then context or just throw the images in? >> So so it depends on the models some of these like we support for a bunch of different models. Some are just text based. Um we with models like claude we try to pass in the relevant diagrams as well. So >> what's the best PDF parsing model?</p>
<p>>> Oo that's a good question. In the last month, there's been like a flurry of different OCR models that have that have come out starting. Well, yeah, Deepseek, how's that? So, I think in terms of cost and accuracy, DeepSeek is pretty good. Like, if you if you host it on your own A100s and just like you you batch things properly, probably Deep Seek is best bang for your buck. Um, there are services like M like Mistral that have their APIs, those are probably Ocr and those might be a little bit more expensive if if you're using their API offering, but I think like deepse is very very good. >> Any Moon Dream? Oof. Uh I probably add later. Yeah. Yeah. But the OCR example</p>
<p>is really funny. Like it's one of the cool things we see on AlphaCive, which is >> on day one, you know, someone will release an OCR mode and then the next two three days like four other people will who've been probably working on this for the last many months will go put out their own OCR mode. >> Yeah. And so it's kind of cool to see the the value that Alpha Kai brings there. Yeah. >> Yeah. Cool. Um so what's the progress of</p>
<p><strong>[06:02]</strong></p>
<p>the the company? You guys decided it was a side project. when did you decided to I guess take it to the next step and also where are you at now? >> Yeah, for sure. So I think for the first year or two of working on it was a project and what really forced that transition for us was we were looking at you know papers with code weights and biases hug and face and we're like okay we have a user base that really loves what we're doing and obviously a research extends so much more beyond papers right like we want to do things with other artifacts of research. One of the things we're we're going to be working on is making it really easy for people to play around with like</p>
<p>implementations of tapers directly on the site. >> Are you are you uh fully fully replacement for papers with code already or >> I would say so. Yeah, cuz we have like benchmarks and a lot of like the state-of-the-art page. >> I know a lot of people Yeah, it was taken down. So, uh we've added the feed, the benchmarks, whatever. But I think because I think the thing that drove us to be a company was there were so many different artifacts of research beyond papers and you know it just kind of made sense to go from there. Yeah, I don't know if any the other the others want to chime in. >> Yeah, I think like like kind of the analogy of like archive existed and we</p>
<p>came in and like built like an intelligent layer over archive, you know, much nicer interface with tools to quickly understand core ideas on paper is comment like if we can do the same thing for like benchmarks, right? You mentioned like papers with code used to be good. it was still a lot of like communitydriven like people would upload their own benchmarks or like you know um implementations or whatnot like now with LLMs it's like really easy like we we can just parse through like using OCR like from charts and tables like what are the leading like models and papers</p>
<p>ideas on benchmarks and just bring them all in one place and and kind of one way to think of it is like just help people make sense of like the fire hose of AI research and that's just not papers like benchmarks you know models So yeah, implementation. >> Yeah, I mean the thing I really want which which uh is easy to do now is um just like put out a monitor of like when this topic comes up, let me know. >> Uh so like a custom feed of stuff. >> Yeah, that's also where we want to put a lot of work into our assistant is like kind of have the best research assistant</p>
<p>that you know knows your interests and can like give you most relevant and upto-date like research. So it's really like a personalization and ranking system, a recommendation system of papers. Anything else that is underappreciated? Like do you see yourself as a social network, a rexist? >> Yeah. >> Good reads. >> No. Yeah. We've gone through multiple iterations and I think where we kind of provide the most value is as concretely like not a social network but a kind of a tool over research and maybe there's</p>
<p>some some signal that's from there some type of social signal but concretely people use this as a tool to understand research and I think one thing that's underappreciated uh is that papers are like the tip of the iceberg of research and I think we're very well positioned to help people beyond just the workflow of reading papers. So you could imagine uh one thing we're really interested in is it's not uh publicly released yet uh but we want to start maintaining like docker images for papers and make it really easy for people to spin up</p>
<p><strong>[09:03]</strong></p>
<p>implementations directly from their browser. Um and you know a lot of times when people are reading papers why are they reading papers? Our our audience is like a lot of applied researchers. So they're figuring out what research is relevant for them. And to get that you need more than what a paper can tell you. At the end of the papers are great but they're like a puff piece for the implementation. That's what they are. And so getting closer to what the signal of the research is is means you need to allow people to play around with the implementation. So I think one one thing that's underappreciated is how much potential there is here to do things beyond uh papers in this in this realm of making tools for for researchers. Um</p>
<p>yeah >> I think even with the the current state of the site I would say that if you want a sort of bird's eye view on the state of AI research on the state of AI in general I think Alphac is the best place to do it. And if you look at places like Twitter, I mean it's there's a lot of stuff on Twitter, but it's very noisy. If you look at hugging face, sure there's, you know, the model 8 data sets, there's certain pieces here, but it's not really a cohesive holistic experience. And I think that something that we've done is obviously we started as a comment section, but we sort of uh created a platform that anyone ranging</p>
<p>from ML researchers to like VCs if they just want to stay up to date with what's going on in AI, I think alive has become that place. And obviously like Ran has alluded to, we don't want to go deeper. It's we don't want Alchive just to be a place where you like you just look at things and you're just surveying things. You want it also to be a place where eventually you are doing your own experimentation. You are actually working with lots of implementations of papers and I think that we're just sort of scratching up surface of what's possible here. So, uh, we're here in Europe. Uh, you've been here the longest</p>
<p>and you have some like I asked you to guys to like maybe talk about like maybe talk papers of the year, talk papers on new rips, >> your favorite paper that you can't shut up about. >> Obviously, working at Alpha, you have to love papers. >> Yeah, >> go for it. >> Go ahead. Okay. Sure. Um I think so one category of papers and I'll start by prefacing we saw the quote on on Twitter from from Ilia that like the age of scaling is down now it's aging I feel like we see some embodiment of that in alphaive where obviously the people</p>
<p>posting open research may not always have thousands of GPUs right and so you're forced to come up with like really creative solutions that uh to to problems with limited compute and you know whether these translate to like practical applications or not is maybe a separate thing but I always find these type of papers very interesting. So one paper that like for the last month I can't shut up about is a tiny recursive models TRM >> simplification of related to HRM just a simplification of HRM which is like biologically inspired TRM cuts out all the biological inspiration says hey let's take like a 7 million uh parameter</p>
<p>transformer model have it pass its latent vector and and output itself recursively um and use that so a 7 million parameter model to get like really really good results relatively speaking on really specific but challenging puzzle tasks. So like Sudoku, ArcGI, I believe he gets like 45 50% on ArcGI1 compared to like it's not</p>
<p><strong>[12:06]</strong></p>
<p>state-of-the-art, but you compare that to like Deep Seek R1 or you know Gemini 2.5 is like pretty good. So I think these types of creative creative solutions are really fun. They're wellreceived on Alchive all the time like when when these type of papers come about and so I think just as a as a nerd for papers I find that really really cool. Another one is like really recent I believe from like the last week or two the uh evolutionary uh strategies or at at hypers scale. So basically we're going to cut out gradient descent and use like low rank perturbations at like million parameter scale and get like really really good results. >> So so use search rather than gradient</p>
<p>descent. >> Yeah, some random perturbations. You can't do this over a million parameters at once, right? that doesn't make but you can make a low rank perturbation sets of low rank perturbations >> and then apply that and >> so it's evolutionary right and get like pretty good results compared to like I believe based compared to like GRPL or on some tasks and I I thought that was also it was trending on alpha and I was like this is a works like this [laughter] yeah and again whether it scales to anything I don't know but it's it's cool to see uh</p>
<p>it's kind of work >> love it yeah great Rex Let's let's keep going around. >> Um I think one area that's also been really exciting especially among our user base and I'm seeing here at Europe as well is like AI for science like kind of >> you know >> we're starting a dedicated AI for science pod. It's my my second podcast that >> that's awesome. Super exciting. Yeah. We we kind of have like a big community as well. And actually we're doing an event on Saturday. Oh Yeah. Um >> well I'll I'll bring my new host here. >> Awesome. Yeah, that'll be exciting. So we have like one of the co-founders of</p>
<p>Laya um James Zo from Stanford um Jeff Cloon as well and I think yeah like I I I was first really excited I saw this paper it was earlier this year called agent laboratory um from Sam Schmidgo from deep mind and the idea is like hey like what if we basically automate the scientific process itself you know like basically it would take like have different LM agents for like different parts of the scientific method like one for like literature review one for like running experiments and one for like analyzing those results and writing the</p>
<p>paper. Um, and and it was pretty impressive. I mean, it's still like I think going to take some time like a lot of like human like in the loop still like makes a lot of impact but like they were able to show that you know like with like a few dollars from like idea to like final report for for certain topics. is like pretty interesting and I mean we're seeing a lot of companies too kind of forming in this area like um Axiom recently um kind yeah like it's it's I think like a really interesting space of</p>
<p>>> I have a problem with it maybe maybe others do maybe others don't of just lumping everything into science >> yeah it's definitely like pretty like broad like there's like you know like like you said like math and then there's like more like the life sciences which feels like a little bit more difficult Well, >> we have nothing in common. >> Like um like like automating like ML research seems a little bit easier. That's like kind of my in like intuition</p>
<p><strong>[15:08]</strong></p>
<p>than like you know building like >> AI for ML research. Can we agree that it doesn't count as AI for science like >> like >> I think that's a that's a fair take. >> Yeah. >> Then everything is nice. I don't know. >> You can make an argument ML research. It's not re [laughter] like >> more of an art. >> Okay. Okay. So, just generally AI for science. Any particular paper or discussion that you've had in the last couple days that like sticks in your head? >> Yeah, I think I think uh yeah, like the Asian laboratory paper. >> Oh, yeah. Asian lab. Okay. Yeah. So, my comment on that is every quarter >> there's somebody who will be like, "Oh,</p>
<p>yeah, we've done AI scientists." >> Yeah. >> And then I don't know if it goes anywhere. I don't know if it's like a proof of concept. Like >> what is going on? >> It depends on the example. like going back to the AI for life science. I'll make the distinction one of we had a speaker he wrote this paper called bio only keshing and it started as a paper right now it's you know he's kind of got a lot of traction and he's kind of continuing to develop it I think that was the first time it's like okay conceptualized it people are using it as a tool and obvious I think things are going to take time part of it is maybe</p>
<p>to get traction you call it like you know it's we're automating all of science but it seems to be like in his case with biiomni that there are people at pharmaceutical companies that are or or by organizations that are using it as a tool. And to what capacity, I'm not a life science expert, but I think there's probably steps from it being a tool to like automating like a lot of things. I'm skeptical of like the automation of the whole process. I I like the idea of thinking it more of as a virtual lab. So James O frames these AI scientists as as a virtual lab where it's like, okay, >> it's cheap to experiment and explore.</p>
<p>>> Exactly. So the person is the PI and I think that vision makes a lot of sense. I think to replace I don't know how much of it is pitching versus science where someone's like we're going to just completely build a scientist because in my way one of the thesis of Alpha like as we go on there's going to be more researchers and maybe there's more tools to help them but the actual like process of doing research is like like human human work is going to be like that is one of the final frontiers of human work is to be able to intuit it and and ideate and figure out what's next right so I see these as all like tools um >> yeah I think I think I think just</p>
<p>extending what Rayon said I think yeah maybe for like you know pitching or like what we see in the news like people are like oh we're building like AI scientists or like an AI material scientist but you don't actually need to do like that whole endto-end workflow to provide value right like what Rayon was me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me me mentioning about Bioni like I know they're like providing a lot of value with like their tools for like oh we'll like sort through the literature for you right and like find out what's what's relevant and then like help you get some experiment set out you know that's like you're not automating the whole workflow</p>
<p>but It's it's saves people a lot of time where like researchers or biotech you know applied people. So I think yeah like I'm also align with you that a little skeptical about automating that whole end toend workflow but I think there's value intermediate parts. >> Yeah skeptical that it is actually put in practice rather than just done for a paper for marketing purposes. >> Yeah. >> But yeah we we can move on to other</p>
<p><strong>[18:10]</strong></p>
<p>papers. I was just touching touching on that one one last thing just a lot of the science that is done is not sound very sexy. It's a lot of like oblations. It's a lot of very sort of like road work that's manual and obviously it's like it's not sort of the doesn't require the ingenuity or sort of the the human brilliance but I think that agents are very much capable of doing a lot of the things that are required for like a lot of scientific processes. And so I think there's a tremendous value there. Yeah. It's not like quite what you're talking about like a men scientist but I think there there is something there for sure.</p>
<p>>> Your paper. >> Oh yeah. I think I mean for me I think something that has emerged in the last year or so has sort of been building agents with RL um for you know long horizon you know complex paths and I guess one specific paper that you know I found interesting is like agent R1 and so they've basically >> agent R1 >> agent R1 >> I don't I think I missed that. Um I mean what they did is they they built an an an endto-end sort of RL framework for training agents on sort of like these complex reasoning tasks. And the the specific reasoning task that they chose was sort of like multihop uh question</p>
<p>answering. And so you have to train an agent to be able to interact with sort of a tool environment and and sort of ask uh and respond to complex queries. And so, you know, when you've applied that framework on top of, you know, a base model like quen 2.5, you suddenly now have an agent that is, you know, very very good at these sort of these more complex tasks. And I think the reason why this is interesting to me is because you see a lot of parallels in this paper to what's actually happening in like industry, you know, like one, you know, one example of this is just like cursor, their composure model.</p>
<p>There's a lot of rumors online that that this was built on top of you know qu 2.5 people saw like Chinese reasoning traces and whatnot but I mean we don't know exactly how they built it but sort of these these techniques you see them >> I see them >> yeah I mean yeah I mean there's a very popular technique to like you know build on top of 2.5 but I mean there's a whole crop of like you know RL as a service finetuning as a service these sorts of things to help you know companies build in-house agents and so a lot of what we see you know in enterprise industry these days is a reflection of sort a lot of the research that's done with, you</p>
<p>know, AI agents um in RL. So, I think it's definitely very exciting. >> And one thing to add there is like so there's Frontier Labs aside, there's these startups that have raised like collectively hundreds of millions of dollars. They're all like fine-tuning Quen like specifically Quinn because there's like Quen if you like talk to some of these folks like Quen was specifically designed to be like very sample efficient when it comes to like RL fine tuning. like they designed this but when when building the model and it's so cool to see all these different startups and goes right back to Quen and like the open source landscape um and then yeah you see papers like agent R1</p>
<p>that >> but was that deepse literally DC one or >> so I I don't know why they threw in R1 in the name but they were fine the Quinn model right okay I >> was like why is it >> yeah I don't know >> uh Justin's here uh from from the Quen team u >> yeah so he's roaming around and uh I I think the this is very much Quinn's Yeah, >> I think uh in fact comparatively I think</p>
<p><strong>[21:10]</strong></p>
<p>DeepS actually started the year super high >> and then fell off a little bit >> which is kind of interesting. I was just talking with one of the uh folks here about presence of deepseek as far as like what we see on alphaive they still pull a lot of weight in terms of when they put out work people listen um even >> great work it's great work so an example was with OCR right they put out DC OCR and then like the next day like OCR 2 comes out the the startup called Chandra puts out an OCR uh model so >> yeah but if I if I'm not mistaken their numbers were worse than deepse still</p>
<p>>> yes so but but I think it still goes back to the point of like when Deep Seek publishes other people are very much still intact. So whether or not they I don't know it's it's shortterm to say it's all enough or not that >> like difference of one day it's not there's not motivated by Deep Seek I don't know. >> No right but I think they feel pressure it's it's like the parallel between research and like product releases almost is like you kind of see that here. >> Yeah. I mean look a lot of interesting papers and and uh coverage in New York we just covered a best paper for DRL. I think another like what I'm trying to</p>
<p>make kind of exploring is like just the health of the academic paper ecosystem which obviously you guys have care a lot about my sense from talking with area chairs and program chairs is that there's a lot of review process is not keeping up therefore the conference quality process uh submissions and quality is just going down. ICLR had a big scandal recently with leaking names and like 20% of their reviews are AI. Yeah. >> What's your take on just like all this? >> It's uh it's a show for sure.</p>
<p>There's I think at multiple levels. Um one fun project we've wanted to do in Alchive. I'm skeptical on if we this would be good for us or not. But first of all, we've always been hesitant to put out tools that allow people to write papers with AI. We help people understand papers with AI. We feel like something feels wrong about like, okay, people are going to write papers, which they're probably already doing. But one thing we want to put out is which papers that are trending are actually like AI generated and you'll probably see at a very high scale a lot of these there's a lot of pressure to put out lots of work</p>
<p>and do it really fast and uh there's a lot of pressure to write papers quickly and uh it's a very much AI in AI out thing that we're going into and so it's definitely problematic and yeah I I guess there's pressure to publish and then there's the AI tools that help people write papers and uh the quality of the paper itself I feel uh has probably taken a hit but >> yeah I think it's also interesting. Um, like I know some people are also working on like AI reviewers too, which is also interesting. And I mean I have some concerns, but I think overall if done</p>
<p>right, it could be good for for authors actually if like authors get access to this and they're able to like just run their papers through these reviewers before and they can see like, okay, like what's maybe unclear in my paper and like what can I do to make it better? And and maybe that just like now the reviewer doesn't need to spend as much time just trying to like understand and</p>
<p><strong>[24:10]</strong></p>
<p>parse through and can just like actually point out like or or do like a literature review and like understand. >> Yeah. It's like a llinter like quality check. >> Yeah. If it's not done right then maybe people like find a way to like hack around the you know AI repair. Um I know Andrew recently put out like the Stanford agentic reviewer which has been getting some traction but um >> oh I wasn't aware of that. Are you guys developing it? >> Uh, we haven't really thought too much about it yet. Yeah, >> I think like one of the tools we'll want to incorporate is some type of reviewer just for the assistance of for an author</p>
<p>before they publish like hey let me through we can tell you how much is obviously >> yeah I generated what parts are like what works are similar and kind of do a breakdown. I I think the llinter analogy is good. Like I think assessing novelty is probably tricky, but as a first step, I think it can like weed out like either low quality work or I think that's probably promising. >> Yeah, I I I was wondering just specifically on novelty and lit review, how important is search and your presumably you have an index and you you</p>
<p>do rag over the index and all that? >> Yeah. >> Um are models good at it? So I I guess it depends on the type of queries. I I guess when it when it comes to novelty is concerned like archive has had this problem like even early on where people would use it for like I forget the exact term but people would post a paper even when it's not fully developed and then you kind of claim >> for preprints >> right right for preprints right so I think in terms of finding relevant literature it's it's good one of the things we kind of where our assistant differs is we'll use social signal as</p>
<p>well so maybe someone has some paper that wasn't that didn't get a lot of traction and um I think just building semantic search isn't super helpful. Um but yeah, >> so like Twitter or other signals, >> it's a combination. So like >> you you don't reveal the the >> So we'll use like view count, semantic signal, whatever. Basically, if you kind of rambling there, but if you just try to build a semantic assistant, things get noisy fast. So like we have an index over like >> these people throw in buzzwords. >> Yeah, exactly. Of >> course. Right. Right. So um we have like three million papers in archive. If you</p>
<p>just try to use semantic and given some query like what are papers that do agentic reasoning you are not going to find the relevant work. So what we do is like we know what papers people are reading and that's a factor that weights in when we when we pull up relevant results. This is something we've talked about with other like other people have built like semantic searcher like semantic search tools on top of archive. Inevitably it sounds good and then it just ends up like not returning relevant info. Again though the reason I was kind of rambling before is we build the assistant more from the angle of literature review rather than assessing</p>
<p>novelty. So so and those are maybe two different things. If you're purely trying to assess novelty maybe the social signal doesn't matter in terms of building like a really good literature review assistant. You need some way hitch on like at the end of these are three million papers. A lot of them are low quality. So you need some signal. Um, I have my doubts on whether just Twitter is the the best form of social signal there, but we have our own in-house.</p>
<p><strong>[27:10]</strong></p>
<p>>> I'll also shout out Emergent Minds, uh, which is turned YouTube into a signal, which are very Yeah, they they also we we like the guy Matt. He like he's part of our Lean Space Discord. >> Nice. Yeah, he So, he also has like the Alpha Guy like stats on. >> So, you can sort Yeah, I also pan on. >> Ah, yeah, I see. I see. We have like a lightweight collab, >> but the YouTube thing is his thing. Not not yet. Yeah. >> Yeah. >> I find the YouTube thing very very useful. >> Yeah. Kind of extending off that like I think uh you know nowadays research it's it's not just the paper right it's like a whole package of like more and more</p>
<p>people are opening the code base which is great and and we hope to continue seeing that and so like there's a paper GitHub if it's available people have like a tweet thread there's like a website if it's like robotics for instance there's like video demos like there's so much more than just the paper. So like you know one day if we can index all of that into our assistant kind of make it as good as possible for like finding all of this relevant research information you you already mentioned you know we're doing papers with codes so like benchmarks as well like I think that will be really powerful. Yeah, all I will say is a lot</p>
<p>of people are cheating at the GitHub thing where they only put put like a rem and we make sure to not like >> okay >> one thing to build on that is in the next five 10 years like the paper as an artifact is going to be less and less useful like I I think like research will move away and we're feeling that here right like at the end I was saying this earlier about what a paper is it's like describing what the new implementation is what the numbers were but that's an implementation like how much does the paper tell you there but I think having really organized useful implementations is going to be the future of research</p>
<p>that you know paper is just a PDF is such an antiquated way of sharing information um is Lindy right and that's why we exist and make that more frictionless >> it also like totally aligns with what we see in our user base today like we have like tons of users who are like not necessarily trained researchers like they're not in academia they're in industry they're not at like the big labs they're like kind of what we call like applied AI researchers or like research engineers and like they're at companies you know like for instance, like they're obviously big companies,</p>
<p>but they're obviously there there's also like companies like Spotify, Expedia, Nintendo, where it's like you don't traditionally associate with like AI research, but like as part of their job now, like they need to keep up to date with the latest research for like some product or feature that they're building, right? And it's like to be honest, a lot of them don't really care about papers. They're just like it's part of the job and like for them it's like really hard starting with what paper should I be reading to okay like how do I quickly understand the core idea and what they all ultimately want to know is like will this work for like what I'm trying to build right and like</p>
<p>getting to from that like paper to implementation is like a huge gap right now starting with like like you said yeah people are cheating on the GitHub like it's not actually useful or like even if it's there like a lot of times like setting up the codebase with the right dependencies is like a huge pain so we're thinking about Brhan mentioned like doing docker for containers for papers and like >> yeah that was the original idea for replicate yeah they pivoted I guess >> and then I think now I would point to</p>
<p><strong>[30:12]</strong></p>
<p>harbor I don't know if from the terminal bench folks >> that would be an interesting equivalent not not not paper focused more environment focused but >> you could reuse the infra >> I mean yeah this just going back onto sort of the whole like the whole paper discussion I think like you just have to look at the graph of like archive submissions the last like 10 years it's just exponential and you basically see like this the CS Yes, exactly. Yes, the AI like >> 30,000 a month. It's great. >> I mean like and you can have things like an you know like an AI agent to review these stuff for like peer review, but like ultimately I think the the real the</p>
<p>end state here needs to be that we just don't care about papers as much. Um at the end of the day like all researchers want is to >> useful ideas. >> Just open source the repo, open source the weights and give me a sandbox where I can quickly verify your thoughts or verify whatever ideas and whatever contribution that you're making. And the the easier that you make that loop instead of having to like read the paper and then like do a lot of nonsense to work through their like slot repo and try to replicate it yourself, faster you close that loop, you'll see a lot less</p>
<p>of this like slop submission and stuff like that. And I think it'll be a lot healthier for an ecosystem. >> Okay. Like obviously you guys do what you you think is best. If I were a friend and adviser to the company, I would be like you might be biting off more than you can chew. >> This is an impossible task. >> Yeah. So one thing about the point about replicate which I was just going to add which is rather than viewing this as like a reproduction or like a a reproducibility problem of like oh we need to try to re reverify the results on all these papers and make it easy. Uh I think we go back to the point which</p>
<p>user base are applied researchers. They don't care about which works have the best numbers in their paper but they care about like which works are easiest to implement right and so there's some subset of those and like can we make it easy for those folks? So if there's if there's an academic that says hey I want this arbitrary thing implement it's like hey we can't do this um use the fact that there's some power law here in terms of implemented papers that people really want and make that experience really easy for example even think about we were talking about the other day with one of these uh one of the researchers at Nurips who was like I haven't actually like played around with Lorac I was like like what has stopped you from</p>
<p>doing that it's like can we make just super simple on really popular works make it really easy to iterate on that directly on top of the >> form and and whether there's you know some obscure work that has great numbers like whether we can reproduce that is not in the s you know space of things we care about >> specifically if you have anything involving GPUs I would call it launchables from Nvidia >> uh they that's came through an acquisition of brev which I used to be investor in >> yeah I mean there's like domain specific uh ways to do these things but they're</p>
<p>not widely socialized >> I think the last thing what Ryan briefly touched on is like this power law that we a lot is like archive has over 2.4 million papers, but there's a huge parl like what are the actual like most viewed papers and it's very long tailed, right? It's like if we can just like kind of focus on like what would be most helpful for the vast majority of people and just make those really easy for</p>
<p><strong>[33:12]</strong></p>
<p>people to build off of. I think that's like a good starting point. We don't need to do this for every paper. And even if initially it's like a little bit of like scaffolding like I think if you're saving time for this like exponentially growing group of like research engineers like >> one super last point here which is uh you know one thing we're working on is having an agent that can autonomously set up you know a docker container for a paper based on the codebase and now let's say on a certain set of papers it doesn't work and it's going to need handholding right one thing down down the line is the onus of this we want kind of want to put on the author which</p>
<p>is hey we rank papers by implementing ation ease. >> So that's what applied researchers care about, right? So if the agent can >> usually need to incentivize >> if the agent already we have authors that come to us saying like how do I rank higher on the alphai feed or like for one if you don't have a GitHub code base like you should add one and that'll help you. But you can imagine now if people are our user base or apply to be searches so they care about only implementation ease. We sort it by implementation ease and whether the agent can knock out some hard uh you know some issue setting up paper that's now on the onus of uh of of the author</p>
<p>there and our user base wouldn't be super interested in implementing them in the first place. >> Awesome. Very good cause. I happy to promote it and uh congrats on uh your success so far. We'll see what happens in 2026. >> Yeah, I appreciate it. Thanks for having us so much. >> Yeah, thank you. >> [music] [music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=T0mZJjl_dsA</guid>
      <pubDate>Wed, 31 Dec 2025 19:47:58 +0000</pubDate>
    </item>
    <item>
      <title>[State of MechInterp] SAEs in Production, Circuit Tracing, AI4Science, "Pragmatic" Interp — Goodfire</title>
      <link>https://www.youtube.com/watch?v=CoZp6xaTbWw</link>
      <description>From PhD research on grounding and language models to shipping interpretability tools in production at *Goodfire,* *Jack Merullo* and *Mark Bissell* are building the infrastructure to crack open the black box—making models not just powerful, but *understandable, steerable, and safe.* We caught up with them live at *NeurIPS* to dig into the state of mechanistic interpretability heading into 2026: why interpretability is no longer just a research curiosity but a *practical tool for deployment in high-stakes industries* (healthcare, finance, life sciences), how *Goodfire's platform* turns unsupervised feature discovery into real-world applications like *paint.goodfire.ai* (painting directly into Stable Diffusion's mental map) and *PII detection for Rakuten* (500x cheaper than GPT-5 as a judge, higher recall than LLM-based methods), why the *memorization vs. reasoning spectrum* matters more than binary "memorized or not" framing (factual recall sits somewhere between rote memorization and logical reasoning), how *cross-layer transcoders and circuit tracing* are scaling interpretability across every layer of a model (creating attribution graphs through interpretable features), why *Neil Nanda's pivot to pragmatic interpretability* isn't a retreat but a validation that interp is ready for real-world impact (managing by outcome, not just reverse-engineering from scratch), the *Pasteur's Quadrant* philosophy that drives Goodfire (bouncing between foundational research and applied use cases, like Pasteur pioneering germ theory _and_ vaccines), and their thesis that *interpretability unlocks latent capacity in models* that you simply can't access by treating them as black boxes—whether that's finding novel biomarkers of disease in genomics models, scrubbing PII from customer chats, or giving creative tools direct access to a diffusion model's internal concept map.
We discuss:

* Jack's path: *PhD student (2020–2025) working on grounding in language models → full pivot to interpretability research → Goodfire*
* Mark's path: *Palantir healthcare engineer → Goodfire in March 2024,* focused on applied interpretability and platform engineering
* What Goodfire does: *AI research company building a platform for interpreting models across modalities and domains,* focused on real-world deployment in high-stakes industries
* *paint.goodfire.ai:* using interpretability to paint directly into Stable Diffusion's mental map—unsupervised concept discovery (animals, backgrounds, scenes) lets you select and paint concepts on a 2D canvas, a totally new interface for a text-only model
* Fact editing and the *ROME paper* (rank-one model editing): updating facts ("capital of France is now Marseille") is an interpretability problem, not yet deployed at scale
* What's new in 2025: *interpretability showing up in model cards, evals, red teaming exercises* (Gemini, Claude), and real production deployments like Rakuten's PII detection
* AI for science: *narrowly superhuman models in genomics, medical imaging, proteomics, materials science*—interpretability unlocks novel biomarkers of disease and scientific discovery in domains humans can't natively understand (base pairs in, base pairs out)
* *Cross-layer transcoders and circuit tracing:* scaling SAEs across every layer, tying features across layers, and creating attribution graphs to trace how models produce outputs through interpretable primitives
* Why *Neel Nanda's pivot to pragmatic interpretability* isn't "interpretability is dead" but a validation that interp is ready for real-world impact—managing by outcome, not just reverse-engineering models from scratch
* *Pasteur's Quadrant:* bouncing between foundational research (Niels Bohr understanding the atom) and applied research (Edison inventing the light bulb), with Pasteur as the model for doing both (germ theory _and_ vaccines)

—
Goodfire Team

* Goodfire: https://goodfire.ai
* Paint demo: https://paint.goodfire.ai
* Careers: https://goodfire.ai/careers

00:00:00 Introduction: GoodFire's Mission and the State of Mechanistic Interpretability
00:00:56 From Grounding to Interpretability: Jack's PhD Journey
00:03:04 Paint.GoodFire.ai: Interpretability for Creative Control
00:05:30 Disentangling Memorization from Reasoning: The Spectrum of Model Capabilities
00:06:40 Unlearning vs Suppression: The Challenge of Removing Information
00:08:15 Real-World Deployments: PII Detection and Enterprise Applications
00:10:20 AI for Science: Genomics, Proteomics, and Novel Biomarker Discovery
00:11:20 Circuit Tracing and Cross-Layer Transcoders: Anthropic's Contribution
00:15:44 The Neil Nanda Pivot: Pragmatic Interpretability and Managing by Outcome
00:18:37 Pasteur's Quadrant: Balancing Discovery and Application at GoodFire</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>[music] Light space to fire up. [music] >> Okay, we are here live hive at Europe's with two good folks. We want to cover the state of Mechinf basically and you guys are very passionate. Mark, I had you on for AIE before. Welcome back. >> Thank you. >> Uh and Jack, you're new but uh also part of Goodfire. Yeah. >> Uh how how would you describe uh what do you guys do and your path into Mecher? Maybe Jack, you want to go first?</p>
<p>>> So we do interpretability research primarily. Uh we're a company focused on like making models interpretable and and robust and safe. And I guess I would describe like the type of work we do as like the science of deep learning which is trying to make models not just black boxes like that we things that we can actually trust and like deploy in in high stakes uh industries. I guess my own path into interpretability um was I was a PhD student um from 2020 to 2025. I graduated in May. Uh and I started</p>
<p>working on I was working on language models. I was working on grounding in language models which is basically the idea that you need more than text data to represent meaning in the world. And it was basically right away as I started like at some point, you know, GPD3 had come out and I had a moment of like, oh, this is actually like, you know, just really good at like understanding the world and like it was kind of a slow transition, but somewhere along the way in grad school, I switched to fully doing interpretability.</p>
<p>>> Cool. Yeah. As like Jack mentioned, Goodfire um is an AI research company uh focused on building a platform for interpreting models of all kinds across lots of different modalities and domains. My path looks very different uh than Jack. So prior to Goodfire, I was at Palunteer as an engineer on our healthcare team and then joined Goodfire back in March. And I guess one cool thing about kind of the the state of interpretability as a field and also how Goodfire is set up um is there's a lot</p>
<p>of foundational research still to be done that folks like Jack are working on and our team. But at GoodFire, I'm more focused on sort of like our applied real world use cases for for interp. building out a platform that can help in use cases like scientific discovery or for kind of like inference time monitoring of models um for deployment in enterprises uh and things like that. So I think there's a lot of exciting just totally new theoretical research to be</p>
<p>done. But uh especially over the past year I think we're starting to see interpretability have practical use cases be actually deployed in like especially situations where models are being used for high stakes industries um and problems which is which is super exciting. >> Yeah. I I think a lot of people are ignorance of the fact that we're actually at this point where people</p>
<p><strong>[03:01]</strong></p>
<p>could actually apply these things for for real life use cases. I saw the platform directly. I you guys had like a launch party in your office for the diffusion thing. Maybe you want to recap what that was so that people can go play around with it again. >> Yeah. Yeah. So, this is uh kind of a a research preview that we put out. It's at paint.goodfire.ai. It's live. That was a use case of interpretability for um sort of like the creative domain. I think just giving one hint at like interpretability gives you a set of I think of it almost as like power user tools for accessing models and doing things with them that you</p>
<p>might not have realized you could. Uh so that example is you take stable diffusion XL turbo. So a model that typically you use text input to prompt the model and you get an image out. But using interpretability techniques you can sort of like plug directly into the mind of the model and you get a 2D canvas where you can basically like paint directly into its mental map of the image. And so we used unsupervised techniques to basically figure out all the concepts that the model like has</p>
<p>internally. So animals, backgrounds, um, scenes and stuff like that. And so you can select any of those concepts, which again we're found totally unsupervised, and you can just like paint a lion over here and then drag it and move it over here. It's just a totally new way. Like this is a model that only takes text prompts as input, but when you plug into the brain, you can do these cool things. >> Yeah. and then highlighting other work because I think people that one went relatively viral. Uh Jack, I don't know if you want to take a turn at like other highlights of your year in terms of stuff that you shared.</p>
<p>>> You start with the memorization. >> Okay. Sure. So there's a lot of work and like going back years on like so models memorize a lot of their training data. That's a privacy concern, but it's also I think just scientifically under interesting to understand and there's sort of like an unclear picture of like how should we think about how it's represented like within a model when it's is is it like a a computer with like like a file system that we can kind of tap into where maybe files are redundantly stored like you know memorized training sequence are kind of spread throughout or should we think</p>
<p>about like some other way and we kind of proposed a a kind of like different lens on it. I think a exciting question around the work was um whether we can like disentangle like core cognitive capacities in a language model from >> from knowledge right >> from knowledge >> I was thinking like GPQ QA0 >> and uh you know if 100 >> exactly >> right so this is a paper we put out recently in October um and uh we showed u there's this like nice spectrum of like capabilities in so there's another</p>
<p>I think another new lens on on memorization which is that in language models it's not so black and white like what is memorized and not so like memorizing that the the capital of France is Paris is you know memorized in some sense but it's it's a lot different than just like memorizing like single page like license agreement document that shows up a thousand times in the pre-trending data or something like that</p>
<p><strong>[06:01]</strong></p>
<p>you can actually see like a the way that we like disentangle memorization you can kind of see this uh like gradient of memorization uh in between both mechanistically and and behaviorally with like logical reasoning tasks being quite distinct from wrote memorization and then like factual recall it's kind of somewhere in the middle. >> What's your uh follow on work after you've done this? >> So I think like understanding um like the reasoning capabilities is is very interesting. I think like understanding how postraining affects those capabilities in general is super</p>
<p>interesting and I think we're going to keep pushing on that. >> Can you induce forgetting so activity? we show and so yeah so there's there's a lot of work on like unlearning uh I would I would describe it more as not unlearning but maybe suppression um I think there's like really like I guess guarantees that you've fully removed information from a model is is I don't think it's been convincingly showed anywhere yet and I I think that it's I think maybe some of the point that we want to make in our work is that when we look at memorization this way it makes a</p>
<p>a point about how hard or or you know possibly like intractable that is >> I don't know if this is a related problem or it's too different but instead of unlearning but updating >> so I moved the capital of France to Marseilles >> but there's so much training data saying the capital of France is Paris >> I need like a date I need to be able to tell the model hey like this is now out of date but like short of tagging a date on everything I don't know when that makes sense you know that's like a weird</p>
<p>>> yeah yeah a really big uh like paper from a couple years ago on this exact thing on fact editing was the the Rome paper. It's rank one model edit and um >> nice acronym. >> Yeah. Yeah. Here we go. Uh and they look at exactly like exactly what you're describing. Um it's a really nice nice paper. I don't think it's it's being used like in deployment anywhere. But I think that's like and it's it's really an interpretability paper. And um I think that's somewhere where like you know from a basic science perspective like interpretability has like so much</p>
<p>to to offer is understanding how we can do something like that which is currently just very difficult. >> So call you over to market a little bit for industry stuff. What can people do at the end of 2025 that they could not do at the start of 2025? >> So I'll preface with saying we we still have a long way to go but but we are excited about seeing like the seeds of you know things actually being deployed in practice. I I would say that we're just continuing to make progress on understanding that models have a lot of latent capacity that you can't get at by</p>
<p>treating them entirely as as black boxes. So, we're seeing interpretability show up in like model cards now for um eval people are running um for various like red teaming exercises and stuff. Yeah, I know. Uh there's some stuff in Gemini 3 um Claude I think Claude 4 and it's definitely >> Victor. >> Yeah, for sure. I mean, yeah, the interp team there is is phenomenal and I think</p>
<p><strong>[09:03]</strong></p>
<p>works across some of their other teams, but and then from Goodfire's perspective, you know, we so like one of our partners, Racketin, is deploying an interpretability based tool in production with one of their language agents. This is a really cool use case where if you what they needed to do was take chats between uh their customers and their agent, find instances where personally identifiable information is mentioned. So names, emails, phone numbers, things like that, and scrub it out. And interpretability turned out to be the the best way to do this at scale</p>
<p>and in a cost-effective way. It was both more effective and cheaper than alternative techniques. >> And you do that by having like a PII feature. >> Uh yeah, exactly. So what you do is you the uh customer is talking with an agent, but then separately you're putting that transcript through what we call like a sidecar model. And you could, what's really interesting is if you ask that model, try to like use it as an LLM as a judge, it's not very good. But if you probe its mind and you sort of detect when the features related to personally identifiable information</p>
<p>are firing, that gets you the highest recall of anything. It's it's the equivalent of using GPT5 as a judge, but it's, you know, like 500 times cheaper. So So they're they're deploying that. And then I you know am personally very excited about the use cases for scientific discovery. So some of our partners in the life sciences and in materials uh there there are these narrowly superhuman models for things like genomics, for medical imaging, for proteomics, for material science and</p>
<p>those are um especially uninterpretable because they are working in domains that yeah we can't well they're they're huge and like we can't native I don't speak genome you know like it's literally base pairs in base pairs out but they're super human at tasks. that are very interesting. So, we have some some, you know, exciting early results in in finding novel biomarkers of disease with some of our partners that um we're excited to share soon. And yeah, I mean, I think AI for science is, you know,</p>
<p>becoming a very hot topic and I think for good reason. >> Yeah, we're literally starting AI for science pond like we're spinning out a separate inspace AI for science bot. >> I love to hear that. Yeah. >> Yeah. Featuring other work uh done notable work this year. I feel like I have to mention in topics circuit tracing paper. I don't know if there's much discussion internally for you guys. Let's just let you riff like what do you think? >> Yeah, Jack I know has done a lot of circuit tracing. You want to >> Yeah, when that so when that came out we we did a so that was um back in like March. Yeah, March or April.</p>
<p>>> If you could uh let's say people know about the SAPE work. Yeah. >> What is the difference? Because I I struggle summarizing it. My my summary you can correct me. It's like take an individ like have a full access to a model. take an individual layer and train a replacement circuit that simplifies what it uh does. >> Yeah. So, yeah, that's right. And a good way to maybe put it is that um you take a representation in the middle which is this like weird dense uninterpretable</p>
<p><strong>[12:03]</strong></p>
<p>like web of concepts and you know features in a representation and the the essay like decomposes it into like primitives like um concepts firing for like mentions of coffee shops or mentions of like uh New York City or something like that and those are those are much more interpretable and that it basically decompos it shatters the representation to like many many pieces. The uh big change with the the cross layer transcoder so the circuit tracing work was a to to really scale up the models to incorporate like every layer uh so like cross layer uh so the the model is called a cross layer transcoder</p>
<p>so it's incorporating like it's um tying features across different layers and then the the tracing part is a method for creating like an it's called an attribution graph through those features which are interpretable to like describe how the model is like producing one output like through every layer and through like a bunch of token positions. I can talk a bit more about it. So, when that came out in like March or April, we um so still pretty early for us. There were like eight to 10 of us at the time, I think. So, we we tried to we like uh went had to replicate some of their</p>
<p>findings. We're really curious about, you know, what would look like training one, like what it's like to use it and basic scientific questions as well, which is like could it rediscover um some like rich representations from like previously understood circuits. So we we put out a post I don't remember when exactly that went up but over I think over the summer like uh May or June on um our replication effort on that and like um how that works. Is there is there an obvious next step like what is what is this all leading up to? Because to me it's like basically just always</p>
<p>scaling it up, always making it more unsupervised. Uh I don't know if there other trends that you can see like yeah these are the core principles that we're just exploring. That's like a good point to bring up interpretability as as an alignment science versus as like a science for understanding models like more broadly which is like if you >> you're one in the same kind of >> I think it depends on what your motivations are. Um so if you are focused on reading a model's mind um and understanding like what's going on internally to make sure it's not having bad thoughts or like it's not</p>
<p>misaligned. It makes a >> bad thought. >> Yeah. [laughter] uh it makes a lot of sense to to like do exactly what they're doing. So I think and that I think they're just not they're I don't know exactly what they're up to but uh applying like these techniques to like read out what's going on inside these models mind. So it's like very nice detection like framework but um if we want like really robust control uh like really robust control of models like what they learn during training I think like that's where there's so much more work to be done. I think you know many</p>
<p>different teams are all working on on these problems but in terms of next steps that's uh yeah like some other directions to go. >> Yeah. No, I think to to Jack's point I think the the circuit tracing work is super cool and I think it's um it's sort of one arrow in a in a larger quiver of techniques that are useful and you know what technique is useful depends on the task at hand that you're interested in.</p>
<p><strong>[15:04]</strong></p>
<p>So to Jack's point, like the techniques that are useful for alignment science and evaluations, you know, that's one use case, but there's a whole world of other things that you might wish to apply other techniques for. And maybe maybe you don't need circuits for things that can be uncovered using probes or using other techniques to understand how you know post-training changed a model. also this like model diffing use case and maybe the things that you want to use as like inference time sort of guarantees for for models look a little bit different. So I think it's super</p>
<p>cool but yeah to your point kind of like there's there's a variety of um a variety of techniques depending on the the final use case that you're interested in. >> Last thing that a lot of people hear in New York I've had like two conversations about it already is what's happening with Neil Nanda. I don't like celebrity culture, but it's hard to avoid Neil's impact. And he basically said he's pivoting his team at Deep Mind uh to no longer focus on whatever and now it's like pragmatic interpretability. What's that conversation about? Like what what</p>
<p>do insiders think? >> You know, I I read uh Neil's post and I think like I also share the the hope that of of a pragmatic future for for interpretability for sure. Um I think a lot of the >> Why didn't we think of that earlier? we should [laughter] medic like oh god >> I well I also think I also think a lot of the um the response to that I'm not sure that folks actually kind of read it all the way all the way through I think there's a lot of reading of that for some reason folks thinking oh like interpretability is dead or something if anything that says interpretability is</p>
<p>alive and well like there are use cases for not black te blackbox techniques that that can be brought to bear in real world use cases so I you know I I think we're Um we we probably agree with Neil on that. And then um you know I think also good for like different companies to have different agendas that they're pursuing. Uh I think it's dangerous you know a field will stagnate if everyone sort of is just converging on the exact same same approaches of things. Not sure if you have other >> to the point about like is um</p>
<p>interpretability like dead or something I think is like a a like gross like misattribution of like what that post is about. to melt your wine. >> Nobody's I'm not saying it. Nobody nobody I talked to was saying it. It was much more like the existing approaches for him were not scaling to some extent and he was the way I put it is kind of like it's like like well okay let's forget about complete understanding and let's manage by outcome and like let's measure ourselves by our ability to steer outcomes and forget if we know</p>
<p>precisely what's going on. >> Yeah. Yeah. I totally So in terms of like should we just be focusing on like reverse engineering models like from the ground up? So that's maybe where the big change is. I think like we we probably agree that like interpretability should be useful and we're trying to get it like like used right now and we are getting it used right now. I think like maybe where like where we kind of align on this is like we're really focused on</p>
<p><strong>[18:05]</strong></p>
<p>like use case inspired like basic research. So yes, like we're very pragmatic, but there's also like a like a lot of like very deep foundational science to be done on understanding models, which is yes, driven by outcomes, but we also want to like develop like the understanding of of what goes on in models because if we're not doing that, then every other uh like lab producing models is just a blackbox AI lab. And you know, that's not how things should be done. or Jack snuck in a little um reference there that I think was was good. So there's this concept</p>
<p>that we like to talk about internally called pastor's quadrant. So Louie pastor like >> is this like a statistical thing with like the the same distribution but four different that's an okay >> yeah no this is more like a conceptual kind of framework but the idea is that you can have so the two axes of the quadrant are I've heard it described as like discovery and invention or like pure research versus applied research like something in in that kind of domain but the idea is that you can have just pure basic research and so the classic</p>
<p>example there is like Neil's bore like understanding the atom, understanding the electron just for sort of like theoretical physics uh interest and then you have the uh corner that is just like purely applied research purely you have an end in insight which is like Edisonian research. So Thomas Edison says I want to make a light bulb I don't really like I'll learn the chemistry that I need to but it's just this goal is is the goal and then there's Louis Pastor who like spans both of these. And so the idea is that it's not just this linear thing between basic research and</p>
<p>applied research. You can kind of have a combination of those two. And so past your, you know, sort of pioneer of like germ theory, but also engineered some of the first vaccines. And so this idea that you should sort of be bouncing back and forth between kind of open-ended foundational research, but then also having a goal in sight and be able to really foot back and forth between the two. I think there's a lot of cases where you see this being a really productive way to make progress in a field. So, there's a company like hero mascot. >> I think it's Tom McGrath, one of the</p>
<p>co-fire co-founders of Goodfire, who um he started the interp team uh in Deep Mind back in the day, and he uh he has a lot of these like great references from other domains of science that are just uh I think really good kind of grounding points for for how we operate. >> Amazing. Let's get a quick call to action in. Uh you guys are hiring. What are you hiring for? What's what's hard to find? >> Definitely actively hiring. Uh, I hope the AI engineer community um should should definitely go to um goodfire.ai, check out our careers page, actively</p>
<p>hiring for researchers and uh engineers and >> and you hear nuts. >> Yeah. Yeah. Exactly. Exactly. There's a combination. Um uh so so yeah, we're you know we're doing foundational interpretability research, but we're building out a platform to apply this in real world use cases. You know, customers across life sciences, materials, government, um financial services. So if you're uh someone with an MLE background, no interp experience</p>
<p><strong>[21:07]</strong></p>
<p>required whatsoever. You can see we have different backgrounds. I'm kind of coming from industry from engineering. Jack's coming from a PhD. Other folks at Goodfire have backgrounds in like quant trading firms or frontier labs um all sorts of places. But if you like training big models, uh building agents, engineering systems, um we're hiring across uh a variety of roles and really looking to fill some of those engineering gaps. Exit. I think that's it. >> Yeah, that's what it Thanks for having us, son. >> Thanks for coming. >> Yeah. [music]</p>
<p>[music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=CoZp6xaTbWw</guid>
      <pubDate>Wed, 31 Dec 2025 19:50:01 +0000</pubDate>
    </item>
    <item>
      <title>[State of Code Evals] After SWE-bench, Code Clash &amp; SOTA Coding Benchmarks recap — John Yang</title>
      <link>https://www.youtube.com/watch?v=MxB-xRGXxkk</link>
      <description>From creating *SWE-bench* in a Princeton basement to shipping *CodeClash,* *SWE-bench Multimodal,* and *SWE-bench Multilingual,* *John Yang* has spent the last year and a half watching his benchmark become the de facto standard for evaluating AI coding agents—trusted by Cognition (Devin), OpenAI, Anthropic, and every major lab racing to solve software engineering at scale. We caught up with John live at *NeurIPS 2025* to dig into the state of code evals heading into 2026: why *SWE-bench went from ignored (October 2023) to the industry standard* after Devin's launch (and how Walden emailed him two weeks before the big reveal), how the benchmark evolved from Django-heavy to *nine languages across 40 repos* (JavaScript, Rust, Java, C, Ruby), why *unit tests as verification are limiting* and long-running agent tournaments might be the future (CodeClash: agents maintain codebases, compete in arenas, and iterate over multiple rounds), the *proliferation of SWE-bench variants* (SWE-bench Pro, SWE-bench Live, SWE-Efficiency, AlgoTune, SciCode) and how benchmark authors are now justifying their splits with curation techniques instead of just "more repos," why *Tau-bench's "impossible tasks" controversy* is actually a feature not a bug (intentionally including impossible tasks flags cheating), the tension between *long autonomy (5-hour runs) vs. interactivity* (Cognition's emphasis on fast back-and-forth), how *Terminal-bench unlocked creativity* by letting PhD students and non-coders design environments beyond GitHub issues and PRs, the *academic data problem* (companies like Cognition and Cursor have rich user interaction data, academics need user simulators or compelling products like LMArena to get similar signal), and his vision for *CodeClash as a testbed for human-AI collaboration*—freeze model capability, vary the collaboration setup (solo agent, multi-agent, human+agent), and measure how interaction patterns change as models climb the ladder from code completion to full codebase reasoning.
We discuss:

* John's path: *Princeton → SWE-bench (October 2023) → Stanford PhD with Diyi Yang and the Iris Group,* focusing on code evals, human-AI collaboration, and long-running agent benchmarks
* The *SWE-bench origin story:* released October 2023, mostly ignored until *Cognition's Devin launch* kicked off the arms race (Walden emailed John two weeks before: "we have a good number")
* *SWE-bench Verified:* the curated, high-quality split that became the standard for serious evals
* *SWE-bench Multimodal and Multilingual:* nine languages (JavaScript, Rust, Java, C, Ruby) across 40 repos, moving beyond the Django-heavy original distribution
* The *SWE-bench Pro controversy:* independent authors used the "SWE-bench" name without John's blessing, but he's okay with it ("congrats to them, it's a great benchmark")
* *CodeClash:* John's new benchmark for *long-horizon development*—agents maintain their own codebases, edit and improve them each round, then compete in arenas (programming games like Halite, economic tasks like GDP optimization)
* *SWE-Efficiency* (Jeffrey Maugh, John's high school classmate): optimize code for speed without changing behavior (parallelization, SIMD operations)
* *AlgoTune, SciCode, Terminal-bench, Tau-bench, SecBench, SRE-bench:* the Cambrian explosion of code evals, each diving into different domains (security, SRE, science, user simulation)
* The *Tau-bench "impossible tasks" debate:* some tasks are underspecified or impossible, but John thinks that's actually a feature (flags cheating if you score above 75%)
* *Cognition's research focus:* codebase understanding (retrieval++), helping humans understand their own codebases, and automatic context engineering for LLMs (research sub-agents)
* The vision: *CodeClash as a testbed for human-AI collaboration*—vary the setup (solo agent, multi-agent, human+agent), freeze model capability, and measure how interaction changes as models improve

—
John Yang

* SWE-bench: https://www.swebench.com
* X: https://x.com/jyangballin

00:00:00 Introduction: John Yang on SWE-bench and Code Evaluations
00:00:31 SWE-bench Origins and Devon's Impact on the Coding Agent Arms Race
00:01:09 SWE-bench Ecosystem: Verified, Pro, Multimodal, and Multilingual Variants
00:02:17 Moving Beyond Django: Diversifying Code Evaluation Repositories
00:03:08 Code Clash: Long-Horizon Development Through Programming Tournaments
00:04:41 From Halite to Economic Value: Designing Competitive Coding Arenas
00:06:04 Ofir's Lab: SWE-ficiency, AlgoTune, and SciCode for Scientific Computing
00:07:52 The Benchmark Landscape: TAU-bench, Terminal-bench, and User Simulation
00:09:20 The Impossible Task Debate: Refusals, Ambiguity, and Benchmark Integrity
00:12:32 The Future of Code Evals: Long Autonomy vs Human-AI Collaboration
00:14:37 Call to Action: User Interaction Data and Codebase Understanding Research</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>[music] Light space tornado wake up [music] light. >> We're here at Nurips with John Yang of Sweet Bench and many other things. But welcome. >> Thanks so much for having me. Yeah, really happy to be here. >> Uh last year I talked to Oier and uh I think Carlos as well, one of your co-authors. >> How's CBS doing? like just just generally the project is like one and a half years olds.</p>
<p>>> Yeah. Yeah. I think one and a half years old in terms of when it was actually useful and we put it out October 2023 and then people didn't really touch it too much and then of course like cognition came on the scene and Devon was an amazing release and I think after that it kind of kicked off the arms race. >> Did they tell you beforehand or they just showed up? >> It was you know I got an email about like two weeks ago. I think it was from I think it was from Walden. He was like, "Hey, you know, we have a good number on it." I was like, "Wow, congrats." You know, thanks for using it. And then the release was like mind-blowing. I was</p>
<p>like, "Wow, these guys did an excellent job." Yeah. >> Amazing. And then Sweetbench verified was like maybe last year. >> That's right. Yeah. >> Um, catch us up this year. Like you have uh other languages. You have there's like a whole bunch of varieties of Sweetbench now. >> Yeah. >> So, what should people know? >> Yeah, for sure. Um, I think there's a couple extensions that are happen. One is like more Sweet Benches, Sweet Bench Pro, Sweet Bench Live. Um >> Oh, Sweet Bench Pro. Was that with you guys? Because it looks independent. It's like different authors. >> It's completely independent. Yeah. >> So, they just call it Sweet Bench Pro</p>
<p>without your blessing. >> Yeah. >> I think uh I think we're we're we're okay with it. Uh when we came out, we were like, "Oh, cool. Interesting." It would have been, you know, fun to be part of it. But, you know, I mean, congrats to them. It's a great benchmark. Yeah. Yeah. >> Uh but yeah, uh multimodal. >> Yeah, we did multimodal and multilingual. Um, and I think like those have multilingual seems to be >> is it like JavaScript? What else? >> Yeah. Yeah. Multilingual is like it's like nine languages across like 40 repos. But yeah, you got them like JavaScript, Rust, Java, C, you know, Ruby. Yeah. Yeah, you got him. Yeah.</p>
<p>>> And then Corbench itself, a lot of people like they they talk about the the Django focus. >> Yes. >> Is there is there is there like I don't know how do we how do we move past Django? >> Yeah, for sure. I mean, it's cool to see um a lot of the newer benchmarks like really try to diversify the repos. Like in the two follow-ups we did with multimodal and multilingual, we made it a point to do that. So, I think >> but you can also just put out Swbench 2025 and just >> That is true. And do a new distribution. Yeah. Yeah. So, it's been cool to see</p>
<p>the follow-ups. I think quietly and and it's an open question for me. I'm excited to see how people curate the next sets. Like it's kind of interesting to see in the literature or in their blog posts like how they're justifying why they're creating their separate split the easier ones where like oh more languages more repos. And then I think now people are like well ours is more difficult because of this curation technique. And I'm yeah I'm excited to</p>
<p><strong>[03:02]</strong></p>
<p>see how how long that lasts and you know where we're going to like guide the evaluations towards. >> Yeah. And more recently you're working on code clash. >> Yes that's right. Uh so let's give people you've already done other episode other podcasts about it. I refer people to to that with your chat with Andy but just give like a people like a one-two sentence. >> Yeah. No, happy to do it especially on your podcast. It's honor. Um yeah. So basically the idea is I don't like unit tests as a form of verification. And I also think there's an issue with bench where all of the task instances are independent of each other. So the moment</p>
<p>you have the model kind of submit it, oh it's done, you know, and and that's the end of the story, end of the episode, you know. So with code clash, what we're thinking is let's try to really evaluate like long horizon development and uh development on a codebase that is consequential and condition upon what a model did, you know, before to that codebase. And so the general idea is you have two or more language models and they play a programming tournament. And what that means is each model maintains</p>
<p>their own code base and each round of the tournament first they get to like edit and improve their code base however they see fit very self-determined and then in the competition phase those two code bases are are pitted against each other. So the code bases are run and there's generally an arena you know we have a lot of diverse arenas but the arena is determined like codebase A is better than codebase B and then you kind of repeat that across multiple >> as determined by an ele judge. >> Yeah. Yeah. So element judge is</p>
<p>definitely one of the mechanisms. Uh we started with some pretty like simple programming games. So one of the cooler ones is like howlight which uh my >> Oh yeah, I played it uh for Jane Street. >> Yes, that's right. That's right. You know that's awesome. Yeah. Highlight one, two, three. Like Michael Troll of Cursor wrote this uh game. >> Two Sigma Jane Street. >> Yes. Oh. Oh. Two Sigma. Two Sig. Two Sigma. >> I worked at Two Sigma. I'm like, >> "Oh, there you go." Yeah. >> This is too long ago. >> There you go. Yeah. 2016 at this point, but we're bringing it back. You know, >> headlight is fun. I I would say if</p>
<p>you've never done a programmatic competition where you have to control fleets of uh ships and attack things and defend things and collect resources. >> Yeah. It's like play Starcraft but you can code, right? >> Yeah. Exactly. Exactly. Yeah. Yeah. >> A lot of games. >> Yeah. >> Is there are there non-games or you focus on games? >> I think that's an excellent point. So for kind of the initial release for scientific purposes, we kind of use existing programming games. Uh the current ongoing effort is you know to build economically valuable arenas. That's you know the popular word these</p>
<p>days. So >> yeah, sweeter is a big one this year. >> Yeah. GD GDP. Awesome. Yeah. just uh I mean I think the big selling point of terminal bench and sweep bench and these evals is that you know it's really close to real world utility and so I think it's resolvable for code clash and that's what we're working on. Yeah. >> Okay. Yeah. >> Um so you're part of group. >> Yes. >> Um the other students have also been putting out a lot of other stuff. What</p>
<p><strong>[06:03]</strong></p>
<p>would you highlight? >> Yeah. No I mean OIR is such a prolific mentor when it comes to benchmarking. So efficiency I really like in the line of performance >> operation. What's the one? Yeah, for sure. Um, so efficiency was wrote by this PhD student called Jeffrey Ma who happened to be my high school classmate and the idea there was like you take a codebase and you just want to you know do modifications that will literally make the code run faster. So I think this like parallelization sim operation stuff like that. Yeah. >> So so no no behavior change just faster. >> Exactly. Keep the unit test passing but</p>
<p>I want better runtime. Okay. >> Yeah. Yeah. And then and then there there's algo tune that is kind of in line with that. And then there's also kind of pushing along like the scientific coding domain. Uh yeah, exactly. Psycho 2 is awesome. They did like a quick >> and for for people is the way I explain psychode is it's human val but better. >> Yeah. Exactly. Exactly. I think you know there's a lot of good stuff that these days where Yeah. That's that's the way to go. >> Which is like sweet bench is expensive to run. Any agent tech benchmark is</p>
<p>expensive to run. Actually, you do need some completions benchmarks that just just >> complete. Exactly. Like, you know, you can do well on those first and then sort of graduate to the multi-turn expensive stuff. Yeah. Yeah. >> Uh okay. Other than that, just like broadly other work in the field in 2025, uh in terms of coding evals, um obviously we shot up Meter. They use VBench and they have a very interesting like I guess human hours worked number. Yeah, they like the x-axis being sort of the runtime and or yeah, y-axis being the completion, you know, like we can do</p>
<p>more longunning speed and tasks. I think the projections are are quite interesting and I definitely appreciate them kind of using sweet bench verified to to sort of proxy a lot of these things. But yeah, they're great. Yeah. Okay. >> Any other work that like caught your eye? >> Yeah, I mean I I think within the Okay, terminal bench bench. Uh yeah, critical point was kind of cool. Um >> critical point. Yeah, that it's like a very new benchmark that uh OPIR did. Um and I think it's kind of related to physics. Um there's this one called secbench kind of related to cyber</p>
<p>security. Yeah, exactly. Sbench, which I I think is affiliated with lot like it's just cool to kind of see people really dive into different coding domains. And then stepping a little bit outside of coding, um I'm personally think it's quite interesting to think about the user simulator stuff. So like TW badge badge too. Yeah. and vending bench and >> I got mix feelings. >> Yeah. No, I'm interested. >> Well, I mean it's it's like it's like you're sampling one path. I I don't know how realistic it is to be honest. It's just the elements but it is cool.</p>
<p>>> No, for sure. Yeah, I agree. I I think it's a good initial effort. Um to me I think it's super cool to see companies like you know I'm sure Merore and stuff are focusing on building environments like for code beyond code and so I think it it might be interesting to have like work gym style stuff. This is stuff that my adviser D. Young at Stanford thinks about a lot. So, yeah. >> Yeah. >> I just realized we we're talking about</p>
<p><strong>[09:04]</strong></p>
<p>Terminal Bendy. Yes. In front of a lot of folks. >> Yeah. Yeah. >> You know, really, really, really good work. Uh just overall, um yeah, let's talk about towbench cuz you mentioned towbench. >> Yes. Yes. Uh there's some discussion or some people are saying that tobench is uh impossible to get a high score on because some of the tasks are underspecified or just impossible. Yeah, >> I don't know if you're up to speed on that. >> I'm a little bit spicy. Yeah, it's a bit spicy. I think I saw so I you know for like I worked with Shinyu and Caric back</p>
<p>in Princeton very closely. I think Caric I just saw posted a tweet kind of um yeah like rebutting some of these claims. Um yeah, I mean it it's I I think I get the concern. Um but yeah, I think it it's also brings up just maybe like interesting research problems to solve of like okay like why is it impossible? Is it the ambiguity? Is it kind of the user simulator that has issues? And I think generally we all agree that you know we'll improve on these things over time for Ubots. >> So I actually really like benchmarks that intentionally uh I think we should</p>
<p>intentionally include impossible tasks >> as a flag. Yeah. Of like hey you're cheating. >> Yes. Yeah, it's kind of sad that like Karthik actually is defending it because the master move would be like, "Oh, yeah, you caught us." Like that that that was uh you know like everyone reporting above 75 on top bench retail uh you'd be cheating. >> Yeah. Oh, interesting. That would be that would be cool. Yeah. I mean, yeah, you'll have to ask the Tow Bench authors, but yeah. No, that that's that's fun. Um yeah, I I think there was uh Impossible Bench was a recent benchmark. Uh maybe from was it from</p>
<p>Anthropic? I don't know. But they basically took Sweetbench verified and they changed the issues to make them impossible and they checked like how often the models would be like I actually just can't do this. I don't know what's going on. >> Oh, like for refusals. >> Yes. Yes. Yes. So, >> oh, how did they do? >> I thought that was interesting. I think they're all the models are all kind of attempting and saying like, oh, I did it, you know. So, maybe not great. >> That's cool. But no, that's a that's an important one. >> Yeah. >> Uh, how does Cody evalance evolve next year? >> Wow, that's a great question. And I mean honestly I think I think it's people</p>
<p>will make more sweet benches. Um I think terminal bench has really got something going where you you ask people to you know a sweet bench you're you're confined in some sense to the domain of issues and PRs that already exist. Um which I think has its benefits of being close to reality and natural but I think with terminal bench there's a lot of creativity that you can infuse into that. So I would personally be really excited like the 2.0 job was really excellent and I'd be super excited to see you know 3.0 4.0 because of like the environments. >> Yeah, I mean the environments, you know,</p>
<p>bringing more people into the fold, you know, I think, correct me if I'm wrong, Mike, but early on you had PhD students, very smart CS people who are adding task and, you know, what does that look like when you fold more coding environments for non-coding tasks, non-coding environments in general, and ask people to make stuff there. So, that's pretty cool. And then, of course, for myself, I think just like this longunning sweet agent kind of thing just feels very compelling. I think the vision of like,</p>
<p><strong>[12:06]</strong></p>
<p>hey, I tell it a goal. I don't have to be super specific about my task. I have like a decent verifier that proxies what I want. Something literally like a codebase that makes the most money in this like setting, you know, like that's my verifier, you know, and I walk away for 5 hours. The thing is just running. I'm hanging out with you, talking to my friends. I come back and it gives me like literally a soda codebase on on on that, you know, task. I think that would be super cool. Okay, I'll push back. We're part time. >> Yes. >> And we are uh emphasizing a lot of interactivity</p>
<p>>> because the the point is that you're going to underspecify, >> right? Right. >> And actually what people want is back and forth, back and forth and on like a really fast time frame, which is terrible for a benchmark author, >> right? Because how you [laughter] do that? Yeah. >> Uh but but realistic. >> Yeah. So, um I I think like that this uh this this is where I'm a little bit anxious or cautious about this push for long autonomy, right? We're g I mean, you know, let's say this time next year, we'll have 5 hours is is pessimistic like it'll be it'll be 24</p>
<p>>> long. Yeah. Right. Days. >> Um but I don't know if that actually materially changes the industry. >> So, we'll push it like as an evals, you know, we have the people people make evals here. >> Yeah. Yeah, we push the industry in ways that we wanted to push, but I don't know if we like that's a productive way because that's more of like a a stunt that that like Yeah, it's a proof of concept that proof existence proof it can be done. >> Yeah. >> But will you use it in >> for real life? >> Yeah. Yeah. I mean, honestly, um to me, I think there's there's potentially room</p>
<p>for growth. So, I I would actually agree with your take here. Um I mean uh with my lab at Stanford with DE like there's a you know her emphasis is on human AI collaboration and so I I definitely don't believe in this idea of just kind of getting rid of the human. Um but yeah maybe just like finding the balance of like you know just because the developer ecosystem is so diverse and there's so many participants in it who want different things out of it like just enabling different levels of abstraction. Um, and you know, it depends on the task. Like there's</p>
<p>settings where you want to be, you know, more involved and more sort of hands-on and so you want to use Windsurf for that. But then maybe there's kind of this general data processing thing. It's just a lot of JSON parsing you don't really care about and that's the one I kind of want to walk away from and just let it figure it out. Um, so yeah, I would agree with you generally. >> Yeah. Yeah. Amazing. Any calls to action? What What do you want help on? How can people uh I guess like find more of your work? Definitely for the call to action. Super jealous of all the great data that cognition and you know cursor would get like that user interaction</p>
<p>data is like really fascinating. From an academic standpoint it feels like there's two difficult approaches to resolving that. Either you build like a really compelling product like El Marina that people have people use consistently which is I mean really tricky in and of itself or you build like really good user simulators that try to mimic sort of these settings. But that is also like</p>
<p><strong>[15:07]</strong></p>
<p>non-trivial. I don't think it's as simple as hey chatbt act like a human, right? >> Yeah. So it would be really cool to sort of get inspiration of like what exactly does that data look like or or between the two like what's the best way to scale up sort of evaluating human AI interaction and then I think for visibility for my own we're pushing more arenas like I think for for code clash what I'm excited about is the current framing is really long running sweet agents but you know you could have multi- aents like two agents work</p>
<p>together on the codebase and what happens you have a human and an agent work on the codebase versus just AIs, what happens there? You know, like when the models improve and hopefully they hill climb and they become better at digesting logs and iterating on analysis, you know, how does how does human AI interaction like change with model capability. Um, and so I'm kind of hoping, you know, I'm trying to inspire and and and convince people that it's a very cool test bed where you can do a lot of different sort of combinations of like human AI on different arenas</p>
<p>playing one arena at a time and arenas at a time, you know, and just, you know, >> yeah, I think very interested to work with you on on the interaction stuff. Oh, that would be awesome. >> And then I I think uh one one more thing I'll add is for cognition uh is going to be pushing a lot of codebase understanding which is kind of codebased retrieval plus. >> Yes. >> And mostly it is helping um humans understand their own code bases better to enable humans >> or to to sort of mind meld the human with the machine uh to do the highest possible task that LM could not do</p>
<p>alone, humans couldn't couldn't do alone. And then the other thing is also like basically automated context engineering for an LM. So that that that is like sort of like a research sub aent uh that we're that we're working on. >> That's so awesome. Yeah. >> So I don't know what the benchmark would be because like how do you how do you benchmark understanding >> that is true [laughter] >> uh apart from I think like yeah it's mostly like you freeze a repo um have some manually cured answers and then you know pose trivia questions that's very easy to saturate. So, I don't know how else to</p>
<p>>> Yeah, I think um I I think Silus tweeted a while ago like sort of like the the wiki the code wiki s that that's incredible. I mean I I use >> with Google actually just came out their own version. >> Oh yeah, with the the anti-gravity people. That's >> uh No, no, no. This is like a separate >> team. Gotcha. Gotcha. Um >> but cool. That's the state of code. >> Yep. >> [music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=MxB-xRGXxkk</guid>
      <pubDate>Wed, 31 Dec 2025 20:00:18 +0000</pubDate>
    </item>
    <item>
      <title>[State of Research Funding] Beyond NSF, Slingshots, Open Frontiers — Andy Konwinski, Laude Institute</title>
      <link>https://www.youtube.com/watch?v=ZagdY6UJYL4</link>
      <description>From co-founding *Databricks* and *Perplexity* to launching the *Laude Institute*—a dual venture fund and nonprofit designed to turbocharge the path from *research breakthrough to breakout company*—*Andy Konwinski* is building the infrastructure to recreate the Databricks motion at scale: fund researchers doing open work, help them ship products that matter, and turn paradigm-shifting ideas into trillion-dollar companies. We caught up with Andy live at *NeurIPS 2025* to dig into the origin story of Laud ("right resource, right researcher, right time"), why the *Databricks founding model* (eight co-founders, deep research scars, years of collaboration) is becoming the gold standard for AI startups (not an anomaly), how Laude's *venture arm* backs researchers-turned-founders with 50+ professors and PhDs as LPs (Jeff Dean, top Berkeley/Stanford faculty, Databricks and Perplexity co-founders), why the *nonprofit arm* does no-strings-attached grants to fund open research before incorporation (the upstream funnel that feeds the next generation of companies), the *slingshot program* funding breakthrough projects like *DSPy, Terminal Bench, LMArena, and continual learning research,* why *NSF isn't broken but insufficient* (it's $1B/year for computer science when we need $10-100B, and Silicon Valley's picker model can deploy capital more effectively), how the *"post-post-training" layer* (prompt optimization, context management, RAG, memory curation, tool usage) is becoming the new frontier above pre-training and post-training, why *Chinese labs are outpublishing Western labs* in open research (Moonshot, DeepSeek shipping twice as many interesting papers as American startups because OpenAI and the frontier labs stopped publishing), the launch of *Open Frontiers*—a live-streamed conference in San Francisco bringing together the 100 most influential open researchers (Yann LeCun, François Chollet, Jan Leike, Percy Liang, Berkeley AI Research, Allen Institute, and more) to share roadmaps and unify the ecosystem, why the *Laude Lounge at NeurIPS* became the VIP gathering spot (Starlink WiFi, free food, couches, and the gods of AI hanging out because conferences need a place for the VVIPs to actually sit down), and his thesis that *open research is the path to world-changing impact*—and Laud is the bridge from grant to company, from paper to product, and from researcher to billionaire founder. We discuss:
* What *Laude Institute* does: dual structure with a *venture fund* (backing researchers-turned-founders post-incorporation) and a *nonprofit* (no-strings-attached grants for open research pre-incorporation)
* The *slingshot program:* funding *DSPy, Terminal Bench, LMArena, continual learning, and Jepa-style prompt optimization* projects across Berkeley, Stanford, MIT, CMU, Wisconsin, Caltech, UI Urbana-Champaign, Toronto, Waterloo, and beyond
* The *"post-post-training" layer:* compound systems, prompt optimization, context management, RAG, memory curation, tool usage—the layer above pre-training and post-training where innovation is exploding
* *GEPA and DSPy:* evolutionary prompt optimization (genetic algorithms reinvented by PhD student Laxia) and the DSPy framework (a reverse compiler that takes code and compiles natural language)
* Why *NSF isn't broken but insufficient:* $1B/year for computer science (and they're trying to cut it in half) when we need $10-100B for frontier AI research—Laud complements NSF with Silicon Valley's picker model and high-velocity grant writing
* The *PhD entrepreneurship clubs:* Computer Science Grad Entrepreneurs (CSGE) at Berkeley (started 2012), Agent at University of Washington, Research to Impact at Wisconsin, Saplings at Stanford, and more clubs forming at CMU, MIT, UI Urbana-Champaign
* *Open Frontiers:* a live-streamed conference in San Francisco (next five months) bringing together the 100 most influential open researchers (Yann LeCun, François Chollet, Jan Leike, Percy Liang, Berkeley AI Research, Allen Institute, and more) to share roadmaps and unify the ecosystem
* The vision: *open research as the path to world-changing impact,* and Laud as the bridge from grant to company, from paper to product, and from researcher to trillion-dollar founder

 — Andy Konwinski

* Laude Institute: https://www.laude.org/lounge
* X: https://x.com/andykonwinski

00:00:00 Introduction: Andy Konwinski and the Laud Institute Vision
00:01:17 The Databricks Motion: From PhD Research to Billion-Dollar Companies
00:02:15 Laud's Two-Sided Model: Venture Fund and Philanthropic Grants
00:06:37 Slingshot Program: Funding the Layer Above Foundation Models
00:07:56 JEPA and DSPy: Evolutionary Prompt Optimization
00:10:29 Beyond Berkeley and Stanford: Expanding the Research Network
00:13:22 NSF Complementarity: Not Broken, Just Insufficient
00:17:03 The Laud Lounge: Creating a VIP Experience at NeurIPS
00:18:41 Open Frontiers: Reclaiming Leadership in Open AI Research
00:19:06 The Open Research Crisis: Why</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>[music] Light and space to fire. Wake up. [music] Light. >> We're here with Annie Kinsky of Loud Institute. Welcome. >> Yeah. Thank you so much. >> I It's weird to welcome to you to your own lawn show. We'll talk about that. >> No, I am welcome here. >> Yes, >> everybody that everybody invited is Yes, I'm here. So your intro is very very long but you're a co-founder of data bricks and perplexity and and and now I think um people are less familiar with</p>
<p>the law institute and what you're what you're doing it's brand new we covered your terminal bench events recently that was a little bit of a coming out party but maybe just for folks who are out of the loop what is the law institute >> oh good question LA itself beyond just law institute is an organization that gives resources to researchers at the exact moment that they need them. So we say right resource, right researcher, right time and there's a sort of dotted line in the life cycle of a researcher who makes a company which is when you</p>
<p>incorporate. So after that you want to raise venture funding. So law has a venture arm that funds researchers and researchy technologists that aren't coming from PhDs or or professorships to build companies. And that's actually a path that I am extremely passionate about with data bricks perplexity being companies founded by PhDs at paradigm at the heart of a paradigm respectively big data for data bricks and public cloud and then kind of search as genai came around for perplexity. So that's a a part of a lot that I'm very passionate</p>
<p>about just helping researchers build the next data bricks the next trillion dollar or so far hundred billion dollar company and before that dotted line researchers need uh in an unprecedented way like an existential existential way more resources to participate in frontier AI research uh people doing research in the open and inspired by data bricks and my history I want big breakthroughs to happen in the open from research things that move humanity forward like computing, you know, silicon or the personal computer or</p>
<p>internet or big data with data bricks or the next paradigm of search with perplexity. So I want all that to be happening in the open. So I want to fund the researchers who are doing open research just in a philanthropic motion and across both of those there's so so there's law ventures for the venture fund law institute this nonprofit for the upstream stuff and across both this obsession with shipping uh in in the case of the venture fund shipping your code as product in the case of the nonprofit shipping your research as open source projects and then there's this ethos to bring meccratic principles</p>
<p>which researchers love into the whole thing as a governing philosophy. So for researchers and by researchers so the venture funding there's 50 professors and PhDs the top names like Jeff Dean and the top faculty of Berkeley and Stanford and my co-founders of data brace and perplexity who've invested in the fund and then give advice and make deal flow introductions and on the institute side it's my money as as sort</p>
<p><strong>[03:01]</strong></p>
<p>of a successful technologist plus the tech the money of other billionaires who have done well by their tech companies giving money into the nonprofit to do this no strings attach grant writing that ultimately becomes a funnel for recreating the data bricks motion you know like make the breakthrough in the open go become a billionaire tech founder that translates your research insight into people products in people's hands so that's a lot law ventures law institute just kind of this one la vision >> a lot of visions I'll I'll dive in I want to talk about slingshots and as as a funding vehicle but you mentioned a a bit of this the data bricks motion and I</p>
<p>think that's an interesting analogy because data bricks is very unusual as founding story >> and I'm wondering if it's replicable. I thought >> why do you think it's unusual? >> Uh eight co-founders >> the seven eight. >> Yeah. So basically it what was the model that you think really worked for data bricks and does it transfer to the other law grantees who are not as big who don't have the >> and I think openai had quite a lot of co-founders. I think VMware had a decent number of early research team turned</p>
<p>co-founding team. I think snorkel AI is a unicorn came out of out of Stanford to have a pretty good size founding team. Core research teams wouldn't when they gel at a university or outside at a research lab in an industry after years of of deep scars from working together knowing each other's ins and outs and team like the the dynamism and like the the gestalts like some is greater than the parts of working together as a team. They're great bets. So you kind of derisked the founder divorce risk with uh that's one of the most big problems</p>
<p>that VCs have to watch out for by having built a breakthrough together. You've also proven traction that you know how to come up with a disruptive idea. You still have to prove that you can turn it into a product market fit. But I think that the founding team sizes is one thing that there is some precedent for the path from I cited a few examples of the path from research just massively successful startup. the Google co-founders Larry and Sergey obviously took research NSF funded research by the way and uh turned it into one of the most iconic companies in the history of</p>
<p>humanity. I cited some VMware. There's even examples, you know, more and more examples of researchers besides perplexity coming out of Berkeley like Ella Marina more recently has become the standard for evaluation for all of the the the whole ecosystem and that was a research turned startup product now project now. So uh 10 years ago I would have agreed that it was and when we found a data brick sure it was a little bit more rare. Now I think it's becoming the gold standard and the thing that you know all the VCs want in on and being at</p>
<p>inside the heart of it all lot institutes sort of bridges those two and is obsessed with that path to impact that so many researchers want to have. So I would say that um I do believe going you know we are we have reached the tipping point already of this being the most high lever path from</p>
<p><strong>[06:03]</strong></p>
<p>breakthrough to breakout company to world changing you know hopefully we'll get to that trillion dollar or beyond we'll get some we'll be at 10 trillion dollars in value uh so I think it's been a new norm for and it'll be great for the world to have this path because researchers are they're both amazing founders and they're genuin ly good people in that they tend to want they're very utilitarian. They want they want to bring all of humanity forward very pragmatic and reasonable and evidence-based. So kind of I think the world needs a little bit more of much of</p>
<p>that right now too. >> Yeah. Amazing. Uh let's talk about some of the research directions that you have funded through your slingshot program. Um so we we already featured Terminal Bench. People can go look at that episode. Uh Arena we already mentioned. We also just did an episode. Um, one thing that we haven't talked about as much is Jeppa, which you have a couple of Jeepa uh co-authors in there. >> That's right. Um, maybe Jeepa and then any others that you care to pick out. But yeah, I've been asking people what's been buzzing as I've been talking to people at Nurups this year and this idea that the language models themselves are more like compute or GPU a generation</p>
<p>ago where what can we build at the layer above and in software systems we've traditionally thought of be a more great example. You have the operating system and the underlying architecture. you know, how do you make it a layer of abstraction above that empowers higher leverage decisions and choices and more applications that are more profound and powerful. So, I think that's neat to think that there's this layer above which is getting a lot of attention in the last year with context management, rag being the most well-known kind of implementation of this. It's this layer above where we're managing what the how</p>
<p>the agents work and how they make decisions, how many memories they can keep track, how and how many memories they keep track of. That's all at this prompt management. And it's not just prompt anymore. It's like tool usage and and memory curation and lots of innovation happening at the earlier above the core stack pre-training, post- training, distillation and and now talking about one of the Delara the PhD student working on some of this this prompt up from his jeep related stuff talking about post post training and that's tying over</p>
<p>>> post. I like that. I like yeah exactly. So there that's tying over to um there's actually a little bit of bleed over where this layer above now what we talk about as compound systems or prompt optimization where and and context management is kind of getting a new level of buzz and a new level of of enthusiasm and maybe a level up of the number of researchers working hard at that layer. Jeepa I would say is the probably up there for or the most successful example of adoption so far of ESP</p>
<p>>> well it is kind of embedded in adjacent to so kind of like >> I I don't know what to think of it you know it's like has like a couple years of branding and >> we're in ESPI the way I think of it is DSP is something closer more akin to a lang chain so it's a a framework for writing agent type codes you give it a task and it like com it it it takes natural language and comp uh it actually takes code and compiles natural</p>
<p><strong>[09:04]</strong></p>
<p>language. So it's like a reverse compiler that's DSPI and the optimizers which came kind of were born in the context of this DSPI project they uh take a prompt and make it better. So that >> so Jeppa is the part of optimizer and it's using this evolutionary genetic technique like a very old area of research reinvented by Lakshia the mean PhD student on Japa really the way I zoom out and think about it is this layer above the core AI model stuff and now it's broaching into</p>
<p>doing model weight updates as well and there's this big debate about is it in the context is it rag plus vector database plus whatever and sort of like lightweight, very few like if I if I tell you I had yogurt for breakfast this morning, you can remember that. It's a very bespoke piece of information that I just told you. Uh you didn't have to like go pre-train your brain weights for another round of like 10,000 GPU hours. You're able waiting that. So that's kind of the parallel with this. And that also takes us not to get too sprawling here into continual learning. Another buzz</p>
<p>area that overlaps a ton with this layer. So really excited about that. that higher layer compound systems is kind of a word that people are using to capture it. Continual learning factors really close to it. Context management, comp optimization, jeepa, better together. Those are some of the buzzwords. Florida in that in that area. A lot of what your source of alpha is is coming from the Berkeley ecosystem. Uh obviously you're not exclusively Berkeley. There's a lot of >> more and more Stanford. >> More and more Stanford. >> I'd say kind of equal partnerships, >> right? Is there a risk of that you're</p>
<p>just doing only west coast universities and no like you know the good ideas come from anywhere right >> that's true and we have massive focus to beyond Berkeley Stanford I would say it's power law big focus >> which obviously like wrote let's let's be objective Berkeley and Stanford have produced a lot so yeah more than anything else but walk around here right now and you'll talk to Jackson Clark second year PhD from UIC think second year your first second year from UI in Illinois several UI teams here he's running a thing called S bench we have teams Stanford and MIT uh sorry not Stanford CMU and MIT very well</p>
<p>represented Wisconsin PhD students around here Caltech PhD students lots of great projects are of our slingshots I would say the majority are non Berkeley Stanford or something maybe I don't I need to double check that number but lots of great projects and a heavy push one one example is I started this PhD club focus on entrepreneurship at Berkeley when I was a PhD student in 2012 called computer science grad entrepreneurs it transformed in my brain eventually to this venture upon computer science grant ventures CSGV and that was a pro</p>
<p>prototype that turned into lot eventually so has this lineage I' been doing this a while PhD yeah yeah 2012 PhD club now we've taken that I've been that and I've went to University of Washington so starting with another west coast school and we started a club called agent there's their CS department is in a building called Allen uh so it's Allen graduate entrepreneurs agent it's a PhD student club on entrepreneurship</p>
<p><strong>[12:05]</strong></p>
<p>then we went to Isconin has started one called research to impact. There's another one at UIC forming right now. There's another one one at Stanford called saplings. So, but a big focus on going to find the PhD students who are interested in shipping their research at CMU, UAC, MIT, Wisconsin, Taltech, all the top 15 universities, Toronto, Waterlue and uh McGill. >> Yeah. And then the the like vector and ma so North America bringing Canada into the mix because they've got also equally</p>
<p>great universities up there. >> Yeah. There is a really big focus on bringing creating a fat pipe of bandwidth to the root of where all the action like you can't get around that the root is of all of it's happening in Silicon Valley. Yeah. It's really opening the the like flying them out here more engaging them deeply in the programs giving them money. Uh, let's be honest, getting you as a mentor. >> That's right. And animal collecting other Braden who did snorkel. He was a PhD of Chris Ray at Samford who went on to do a unicorn company. He joined a</p>
<p>lot. So, assembling a partnership like a venture partnership, but for PhDs and researchers turn founder turn unicorn founder. >> Yeah. I want to get to open frontiers, but one more question about just the NSF, which I think you uh you mentioned once for for about data bricks, but also I think you're trying to target the law grants as sort of NSF level prestige and and and and impact. I think one thing I maybe as like a spicy question is well what's broken about the NSF process you're trying to fix? >> Uh I don't think NSF is broken. I love</p>
<p>NSF. It has been probably the best investment the American popular public has ever made. Thousands xx return on their investments. I mentioned Google earlier, data bricks, all these researchers came from NSF funding. Uh and it's just been a paradigm generation. So DARPA was important for at some point. There's been these these paradigm shifts in how open research has been funded. DARPA was a key one. NSF's been a key one. And now NSF is not is not big enough. It was $1 billion a year</p>
<p>for computer science and that they're trying to cut that into half of that. But we need $10 to hundred billion dollars to do frontier AI research. So it's not broken. They are trying to break it more is insufficient. >> You need more NSF. >> And and I think in addition to the mechanisms NSF has used to to boy the money which are very effective. We have secret sauce in Silicon Valley of how startups find product market fit. We have really good pickers. We have venture capitalists that that have their own sort of evolutionary uh Hunger Games</p>
<p>way of like finding who is great who are really good at at having intuition when they meet a person to pick the next winner. That's a that's a a unique approach that NSF does not use. They do not make a little venture partnership inspired group of research partners who go grant do grant writing. you don't get equity back, but you do uh empower the</p>
<p><strong>[15:05]</strong></p>
<p>right project. If you if you got good pickers, you can actually you can actually have much more effective deployment of a way you can actually deploy an order of magnitude less capital and have more impact with this approach and it's very complimentary because you can still have traditional NSF. INSAP is is brings faculty, many of which are very near and dear friends, into a centralized organization that is in charge of deploying billions and billions of dollars of funding, has an insane track record, and we are going to complement that with a much more Silicon Valley inspired approach of a high</p>
<p>velocity, turn the funding around really quickly, have very strong opinions about the type of research we fund and re and be very narrow in the type of research. NSF funds everything from like all types of research from biology to you know like computer to uh and beyond uh English research and anthropology and sociology like I said 1 billion to computer science 10 billion total so one10enth going computer science and NSF has to manage all of that law and law like approaches were focused very tightly on high impact computer science</p>
<p>research yeah especially AI but AI systems you know crypto cryptography security in networking and architecture. Um, so like other areas besides core AI as well. So by being so laser focused, we can go deeper by bringing in researchers who have shipped, researchers who started companies, researchers who had found product market fit. We can actually identify projects that are more likely to become a data bricks or an Apache spar or array or an almarina sooner and and and with more confidence. And so I think it's a very</p>
<p>complimentary model, but nobody's ever done it before. and you needed someone to kind of step up and propose a pretty fundamentally different architecture or organization shape like you need to hire people in a different way. It's a nonprofit which is you know got its own challenges but you wanted to behave in a very lean extremely high velocity way like a startup. So that's what we did. It's working amazingly so far. >> Congrats on on everything. Final question. Uh we have the open frontiers uh announcement and you're also doing the law launch here in Euros. How are you able to get such amazing people? You</p>
<p>got Oral Vignyals and Jeff Dean and I saw Dylan Patel was a long long one. Yeah, that's so fun. >> Uh so so basically like uh what is open frontiers? Where is this all going? >> Great. Yeah. So L lounge basically Nurips has needed a VIP lounge. We have VVIPs the gods of computer science and AI is you know obviously AI particularly walking around the halls of that Giants conference hall. It's actually hard to find a chair much less a comfortable couch. You got meeting room 23A on good.</p>
<p>[laughter] >> Yeah. >> No, no, no. It's It's not a VIP launch at all. It's just Yeah. Yeah. So, the idea here is open 8 to 10, 8 a.m. to 10 p.m. Identifi and it's not too hard to know who the big names are. The high impact researchers, Yan Stoka, France Shalet, Yen. Yeah, Jen D is over there and I mean Tallwalker and we kind of know the circles already. So, it just took a lot</p>
<p><strong>[18:06]</strong></p>
<p>of emails and texts to like, "Hey, if you're going to be in nerves, cross the street. We're just going to throw VIP lounge Wednesday, Thursday, Friday, free food, Starlink, Wi-Fi, open all day, chat with us. You want to jump on a podcast. You want to talk to some talk to Swix." I mean, you're a big draw here, too. Uh, walking through, people like, "Oh, is that Swix?" >> Yeah. Like, you just walk through and you feel important and you are important if you're hanging out here. So, it's it's working. So it's like the MVP of open frontiers. >> Okay. So that's that's Law Lounge. Open Frontier is a project that I've been</p>
<p>cooking on for a while in my brain, but really has come together in the last 3 weeks really quickly. It's inspired by the core premise of open research that happen that that is symbolized by Nurups, right? People publishing their ideas. And what's happened in the last year is western open science and research discourse has no has has lost the number one spot to China. So uh we if you ask Stanford and Berkeley PhD students which I have dozens of them and they're hanging around here I'm going to</p>
<p>introduce you where where are the best papers the most interesting papers about AI coming they'll say I read twice as many interesting papers by Chinese startups than I did by Americans because really just make an effort. >> Yep. Moonshot, Kimmy, Deep Seek, they're publishing really interesting stuff. They make an effort to talk about it. Whereas in the United States, since OpenAI closed their doors and stopped publishing, so did all the other labs. And by and large, you go and you don't get to publish. You are actually working on the frontier at those labs, but you're not talking about it. >> And so, >> so you need to open the frontier. >> The so the the openness is the key word.</p>
<p>Um, Open AI was called Open AI and they were open for a while and now they're not. We need something that fills in that gap to be a champion and bring together all open researchers to put forward a unified front at the so that we can operate at the frontier. And so that's what the goal of this thing is to bring together every single organization that is leading in open research. So Allen Institute, France law will be around for ARC prize foundation. We have Berkeley, Stanford, Skyab, Berkeley AI</p>
<p>research bearer. You got you know the like Yedjin's lab and Marin the project of Percy Leang who's kind of very senior at HAI and runs the center for uh >> foundation modelation models. >> He coined the term. >> Exactly. Yeah. Foundation model. So and Percy was in the lab with a lot of this is just my my network because Percy and I were in the same as lab with mate and all the other data bricks founders back in 2008 back at Berkeley. So it wasn't hard to go find these people. Everyone I've approached said yes. The idea is</p>
<p>like let's team up, get together one day in San Francisco in the next five months and get the hundred most influential open researchers together for a conference and for meetings where we share our road maps and we talk about common goals amongst the entire ecosystem. It's kind of surprising this hasn't happened yet. And the the the thing we're basically doing is making an open frontier lab effort starting with a</p>
<p><strong>[21:08]</strong></p>
<p>conference. >> Yeah. And it's live streamed so they'll actually open so that we actually can actually like embody the democratic principles that have gotten us to where we are today. Get an order of magnitude more people in the world watching the breakthroughs that are happening in these labs. All the continual learning, prompt optimization, BLM SG, breakthroughs in inference, breakthroughs in evaluations, terminal bench and a lot of other I just heard for the first time about this benchmark called impossible bench. It's a genius idea. I haven't talked about it with you until Yeah. just so um that that dissemination of what's actually</p>
<p>happening will capture the world's attention and in turn the goal is to with the world's attention help this become to gel into uh the a well enough funded and resourced collaboration team up of the ecosystem that we can once again become number one at open frontier research uh because that should be so hard it shouldn't you talked to like yeah it shouldn't And then I was like, "How about we just put this conference together?" They're like, "That's a good idea." Okay, we're doing it. >> Well, thanks for kicking it off. I'm</p>
<p>excited to support it in any way I can. And I'm see I'm sure you'll be there and I'm sure everybody will be like, "How do we get on with Swix?" >> Thank you so much. >> Yeah, my pleasure. [music] [music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=ZagdY6UJYL4</guid>
      <pubDate>Wed, 31 Dec 2025 20:18:01 +0000</pubDate>
    </item>
    <item>
      <title>Artificial Analysis: The Independent LLM Analysis House — with George Cameron and Micah Hill-Smith</title>
      <link>https://www.youtube.com/watch?v=v5mBjeX4TJ8</link>
      <description>don’t miss George’s AIE talk: https://www.youtube.com/watch?v=sRpqPgKeXNk
—-
From launching a side project in a Sydney basement to becoming the *independent gold standard for AI benchmarking*—trusted by developers, enterprises, and every major lab to navigate the exploding landscape of models, providers, and capabilities—*George Cameron* and *Micah Hill-Smith* have spent two years building *Artificial Analysis* into the platform that answers the questions no one else will: *Which model is actually best for your use case? What are the real speed-cost trade-offs? And how open is "open" really?*
We discuss:

* The *origin story:* built as a side project in 2023 while Micah was building a legal AI assistant, launched publicly in January 2024, and went viral after Swyx's retweet
* Why they *run evals themselves:* labs prompt models differently, cherry-pick chain-of-thought examples (Google Gemini 1.0 Ultra used 32-shot prompts to beat GPT-4 on MMLU), and self-report inflated numbers
* The *mystery shopper policy:* they register accounts not on their own domain and run intelligence + performance benchmarks incognito to prevent labs from serving different models on private endpoints
* How they make money: *enterprise benchmarking insights subscription* (standardized reports on model deployment, serverless vs. managed vs. leasing chips) and *private custom benchmarking* for AI companies (no one pays to be on the public leaderboard)
* The *Intelligence Index* (V3): synthesizes 10 eval datasets (MMLU, GPQA, agentic benchmarks, long-context reasoning) into a single score, with 95% confidence intervals via repeated runs
* *Omissions Index* (hallucination rate): scores models from -100 to +100 (penalizing incorrect answers, rewarding \"I don't know\"), and Claude models lead with the lowest hallucination rates despite not always being the smartest
* *GDP Val AA:* their version of OpenAI's GDP-bench (44 white-collar tasks with spreadsheets, PDFs, PowerPoints), run through their Stirrup agent harness (up to 100 turns, code execution, web search, file system), graded by Gemini 3 Pro as an LLM judge (tested extensively, no self-preference bias)
* The *Openness Index:* scores models 0-18 on transparency of pre-training data, post-training data, methodology, training code, and licensing (AI2 OLMo 2 leads, followed by Nous Hermes and NVIDIA Nemotron)
* The *smiling curve of AI costs:* GPT-4-level intelligence is 100-1000x cheaper than at launch (thanks to smaller models like Amazon Nova), but frontier reasoning models in agentic workflows cost more than ever (sparsity, long context, multi-turn agents)
* Why *sparsity might go way lower than 5%:* GPT-4.5 is ~5% active, Gemini models might be ~3%, and Omissions Index accuracy correlates with total parameters (not active), suggesting massive sparse models are the future
* *Token efficiency vs. turn efficiency:* GPT-5 costs more per token but solves Tau-bench in fewer turns (cheaper overall), and models are getting better at using more tokens only when needed (5.1 Codex has tighter token distributions)
* V4 of the Intelligence Index coming soon: adding GDP Val AA, Critical Point, hallucination rate, and dropping some saturated benchmarks (human-eval-style coding is now trivial for small models)

—
Artificial Analysis

* Website: https://artificialanalysis.ai (https://artificialanalysis.ai (\))
* George Cameron on X: https://x.com/grmcameron (https://x.com/grmcameron (\grmcameron\" (\)))
* Micah Hill-Smith on X: https://x.com/_micah_h (https://x.com/_micah_h (\_micah_h\" (\)))

00:00:00 Introduction: Full Circle Moment and Artificial Analysis Origins
00:01:08 Business Model: Independence and Revenue Streams
00:04:00 The Origin Story: From Legal AI to Benchmarking
00:07:00 Early Challenges: Cost, Methodology, and Independence
00:16:13 AI Grant and Moving to San Francisco
00:18:58 Evolution of the Intelligence Index: V1 to V3
00:27:55 New Benchmarks: Hallucination Rate and Omissions Index
00:33:19 Critical Point and Frontier Physics Problems
00:35:56 GDPVAL AA: Agentic Evaluation and Stirrup Harness
00:51:47 The Openness Index: Measuring Model Transparency
00:57:57 The Smiling Curve: Cost of Intelligence Paradox
01:04:00 Hardware Efficiency and Sparsity Trends
01:07:43 Reasoning vs Non-Reasoning: Token Efficiency Matters
01:10:47 Multimodal Benchmarking and Community Requests
01:14:50 Looking Ahead: V4 Intelligence Index and Beyond</description>
      <content:encoded><![CDATA[<p><strong>[00:06]</strong></p>
<p>This is kind of a full circle moment [music] for us in a way. Um cuz >> the like first time artificial analysis got mentioned on a podcast was you and Alysia on Lena Space. >> Amazing. >> Which was January 24. >> I I don't even remember doing that, but [laughter] yeah, it was it was very influential to me. Um, yeah, I'm looking at AI news for Jan 17 or Jan 16, 2024. Uh, I said this gem of a models and host comparison site was just launched. And, uh, and then I put in a few screenshots and I said, um, it's an independent</p>
<p>third party. It clearly outlines the quality versus throughput tradeoff and it breaks out by model and hosting provider. Uh, I did give you for missing fireworks and [laughter] uh, how do you have a model benchmarking thing without fireworks? But you had together, you had Perplexity, and uh I think we just started chatting there. Welcome George and Micah uh to Lyn Space. You I've been following your progress. Congrats on an amazing year. You guys have really come together to be the presumptive new gardener of AI. Okay. How do I pay you [laughter]</p>
<p>>> and let's get right into that. How do you make money? >> Um well, very happy to talk about that. So, it's been a like big journey the last couple years. artificial analysis is going to be 2 years old in January 2026, which is pretty soon now. >> We first run like the website for free obviously and give away a ton of data to help developers and companies navigate AI and make decisions about models, providers, technologies across the AI stack for building stuff. We're very committed to doing that intend to keep</p>
<p>doing that. We have along the way built a business that is working out pretty sustainably. We've got just over 20 people now and two main customer groups. So we want to be who enterprise look to for data and insights on AI. So we want to help them with their decisions about models and technologies for building stuff. And then on the other side we do private benchmarking for companies throughout the AI stack who build AI stuff. So no one pays to be on the website. We've</p>
<p>been very clear about that from the very start because there's no use doing what we do unless it's independent AI benchmarking. >> Yeah. >> Um but turns out a bunch of our stuff can be pretty useful to companies building AI stuff. >> And is it like I'm a Fortune 500, I need advisor on objective analysis and I call you guys and you pull up a custom report for me. You come into my office and give me a workshop. What what what kind of engagement is that? So we have a benchmark and insight subscription which</p>
<p>looks like standardized reports that cover key topics or key challenges enterprises face when looking to understand AI and choose between all the technologies. And so for instance one of the report is a model deployment report. How to think about choosing between serverless inference, managed deployment solutions or leasing chips and running</p>
<p><strong>[03:08]</strong></p>
<p>inference yourself is is is an example kind of decision that big enterprises uh face and it's hard to hard to reason through like this AI stuff is is really new to to everybody and so we try and help with our reports and insight subscription companies [snorts] navigate that. We also do custom private benchmarking and um so that's very different from the public benchmarking um that we publicize and there's no commercial model around that for for private benchmarking we'll at times create benchmarks run benchmarks to</p>
<p>specs that enterprises want and we'll also do that sometimes for AI companies who have built things and we help them understand what they've built with private benchmarking um you know through the expertise mainly that we've developed through trying to support everybody uh publicly uh with our public benchmarks. >> Yeah. Let's talk about tech stack uh behind that. But okay, I'm going to rewind uh all the way to when you guys started this project. Uh you were all all the way in Sydney. >> Yeah. Well, Sydney, Australia for me. George was an SF, but he's Australian</p>
<p>but moved here already. >> Yeah. And u I remember I that Zoom call with you. Um what was the impetus for starting artificial analysis in the first place? You know, he started with public benchmarks. And so let's let's start there and we'll go to the private stuff. >> Yeah. Why don't we even go back a little bit to like why we, you know, thought that it was that it was needed. >> Yeah. >> The story kind of begins like in 2022, 2023, like both George and I have been into AI stuff for quite a while. In 2023 specifically, I was trying to build a</p>
<p>legal AI research assistant. So, it actually worked pretty well for for for its era, I would say. but was finding that the more you go into building something using LLMs, the more each bit of what you're doing ends up being a benchmarking problem. So had like this multi-stage algorithm thing trying to figure out what the minimum viable model for each bit was trying to optimize every bit of it. as you build that out, right? Like you're trying to think about accuracy, bunch of other metrics and</p>
<p>performance and cost and mostly just no one was doing anything to independently evaluate all the models and certainly not to look at the trade-offs for speed and cost. So we basically set out just to build a thing that developers could look at to see the trade-offs between all of those things measured independently across all the models and providers. Honestly, it was probably meant to be a side project when we first started doing it. Like we didn't like get together and say like, "Hey, like we're going to stop working on this</p>
<p>stuff and like this is going to be our main thing." >> When I first called you, I think uh you you hadn't decided on starting a company yet. >> That's actually true. I don't even think Paul's like like George quit his job. I hadn't quit working my legal AI thing. Like it it was genuinely a side project. >> Yeah, we built it because we needed it as people building in the space and thought, "Oh, other people might find it useful, too. So we'll buy a domain and link it to the versel deployment that that that we had [laughter]</p>
<p><strong>[06:09]</strong></p>
<p>and and and and and [clears throat] tweet about it and but very quickly it started getting attention. Thank you Swix for I think doing an initial retweet and spotlighting it there this project that that we released and then very quickly though it was useful to others but very quickly it became more useful as the number of models uh released accelerated. Uh we had mixture late time 7B and it was a key >> that's a fun one. >> Yeah. Like a a open source model that really changed the landscape and opened up people's eyes to other serverless</p>
<p>inference providers and thinking about speed, thinking about cost and so it became more useful um quite quickly. >> Yeah. What I love talking to to people like you who sit across the ecosystem is well I have theories about what people want but you have data and that's obviously more more relevant. Uh but I want to stay on the origin story a little bit more. Um when you started out I would say I think the the status quo at the time was every paper would come out and they would report their numbers</p>
<p>versus competitor numbers and that's basically it. And uh I remember I did the leg work. I think I think everyone has some version of Excel sheet or Google sheet where you just like copy and paste the numbers from every paper and just post it up there and then sometimes they don't line up because they're independently run and so your numbers are going to look uh better uh than the or your reproductions of other people's numbers going to look worse cuz you don't hold their models correctly or whatever whatever the the excuse is. I think then Stanford Helm Percy Lang's project would also have some some of</p>
<p>these numbers and I don't know if there's any other source that you can site the way that if I were to start artificial at the same time you guys started I would have used the Luther AI's um eval framework uh harness. >> Yep. That was some cool stuff. Um, at the end of the day, right, running these emails, it's like, if it's a simple Q&A email, all you're doing is asking a list of questions and checking if the answers are right, which shouldn't be that crazy, but it turns out there are an enormous number of things that you've</p>
<p>got to control for. And I mean back when we started the website like one of the reasons why we were we realized that we had to run the evals ourselves and couldn't just take um results from the labs was just that they would all prompt the models differently and when you're competing over a few points then >> you can um put the answer into the model [laughter] that in the extreme and like you get crazy cases like back when um Google did Gemini 1.0 Ultra and needed a number that was better than GB4 and um like constructed um I think never published</p>
<p>like chain of thought examples 32 of them in every topic in MLU to run it to get the score like there are so many things that you >> they never shipped ultra right >> um >> that's on this one I mean it I'm sure it existed but yeah so we were pretty sure that we needed to run them ourselves and just run them in the same way across all the models and we were we were also dead certain from the start</p>
<p><strong>[09:09]</strong></p>
<p>You couldn't look at those in isolation. You needed to look at them alongside the cost and performance stuff. >> Yeah. Okay. A couple technical questions. I mean, so obviously I also thought about this and I didn't do it because cost. >> Did you did you not worry about cost? Were you funded already? Clearly not, but you know. >> No, we we well we definitely weren't at the start. So like um I mean we're paying for it personally at the start. So well the numbers weren't nearly as bad a couple years ago. So we like certainly um like incurred some costs but we were probably in the order of</p>
<p>like hundreds of dollars of spend across all the benchmarking that we were doing stuff. It was like kind of fine. >> These days that's gone up an enormous amount for a bunch of reasons that we can talk about. Um but yeah, it it wasn't that bad cuz if you can also remember that like the number of models we were dealing with was hardly any and the complexity of the stuff that we wanted to do to evaluate them was a lot less like we were just asking some Q&A type questions and then one specific thing was for a lot of evals initially</p>
<p>we were just like sampling an answer directly without letting the models think. We weren't even doing chain of fault stuff initially and that was the most useful way to get some results initially. >> Yeah. And so for if for people who haven't done this work, literally parsing the responses is a whole thing, right? Like because sometimes the models the models can answer any way they feel fit. And sometimes they actually do have the right answer but they just return the wrong format and they will get a zero for that unless you work it into your parser and that involves more work.</p>
<p>And so there I mean but there's an open question whether you should give it points for not following your instructions on the format. So >> it depends what you're looking at, right? Because you can if you're trying to see whether or not it can solve a particular type of reasoning problem and you don't want to test it on its ability to do answer formatting at the same time, then you might want to use an LLM's answer extractor approach to make sure that you get the answer out no matter how it answered. But these days it's mostly less of a problem. Like if you instruct a model and give it examples of what the answers should look</p>
<p>like, it can get the answers um in your format and then you can do like a simple reax. >> Yeah. Yeah. And then uh there's other questions around I guess sometimes if you have a multiple choice question sometimes there's a bias towards the first answer. So you have to randomize the responses all these nuances you like once you dig into benchmarks you're like I don't know how anyone believes the numbers on all [laughter] these things because it's it's so it's so dark magic. You've also yeah got like the um uh different degrees of variance and different benchmarks, right? So if if you if you run four question multi-</p>
<p>choice on a modern reasoning model at the temperature suggested by the labs for their own models, the variance that you can see on a four question multi- choice eval is pretty enormous if you only do a single run of it and has a small number of questions especially. So like one of the things that we do is run an enormous number of all of our emails when we're developing new ones and doing upgrades to our intelligence index to bring in new things so that we can dial in the right number of repeats so that</p>
<p><strong>[12:10]</strong></p>
<p>we can get to the 95% confidence intervals that we're comfortable with so that when we pull that together we can be confident in intelligence index to at least as tight as like a plus or minus one at a 95% confidence. >> Yeah. Again, that just adds a straight multiple to the cost. And >> yeah, so that's one of many reasons that cost has gone up uh a lot more than linearly over the last couple years. >> We um we report a cost to run the artificial analysis intelligence index on our website and currently that's assuming one repeat. Okay. um in terms</p>
<p>of how we report it because we want to reflect a bit about the waiting of the index but um but our cost is actually a lot higher than what we report there because of the repeats. [laughter] >> Yeah. Yeah. Yeah. And probably this is true but just checking they you don't have any special deals with the labs. They don't they don't discount it. You just pay out of pocket or out of your your sort of customer funds. >> Oh there is there is a mix. So we >> so so the the the issue is that sometimes they may give you a special endpoint which >> 100%. Yeah. Yeah. Yeah. Exactly. So we</p>
<p>are um laser focused like on everything we do on having the best independent metrics and making sure that no one can manipulate them in any way. There are quite a lot of processes we've developed over the last couple of years to make that true for like one you bring up like right here of the fact that if we're working with a lab, if they're giving us a private endpoint to evaluate a model that it is totally possible that what's sitting behind that black box is not the same as they serve on a public endpoint. We're very aware of that. We have what</p>
<p>we call a mystery shopper policy and so and we're totally transparent with all the labs we work with about this that we will register accounts not on our own domain and run both intelligence emails and performance benchmarks without them being able to identify it. And no one's ever had a problem with that because like a thing that turns out to actually be quite a good factor in the industry is that they all want to believe that none of their competitors could manipulate what we're doing either.</p>
<p>>> That's true. I never thought about that. I've been in a database data industry prior and there's a lot of shenanigans around benchmarkings, right? So, I'm just kind of going through the mental laundry list. Did I miss anything else in that in this category of shenanigans? [laughter] >> I mean, okay. the the the biggest one like that I'll bring up like is more of a conceptual one actually than like direct shenanigans. It's that the things that get measured become things that get targeted by what they're trying to build. Right. Exactly. So that doesn't mean anything that we should really call</p>
<p>shenanigans. Like I'm not talking about trading on test set but if you know that you're going to be great at a particular thing. If you're a researcher, there are a whole bunch of things that you can do to try to get better at that thing that preferably are going to be helpful for a wide range of how actual users want to use the thing that you're building, but will not necessarily do that. So, for instance, the models are exceptional now at answering competition maths problems.</p>
<p><strong>[15:11]</strong></p>
<p>[laughter] There is some uh relevance of that type of reasoning, that type of work um to like how we might use modern coding agents and stuff um but it's clearly not one for one. So the thing that we have to be aware of is that once an eval becomes the thing that everyone's looking at, the scores can get better on it without there being a reflection of overall generalized intelligence of these models getting better. That has been true for the last couple of years, it'll be true for the next couple of years. There's no silver bullet to defeat that other than building new</p>
<p>stuff to stay relevant and measure the capabilities that matter most to real users. >> Yeah. And we we'll cover we'll cover some of the new stuff that you guys are building as well. Uh which is cool. Like you used to just run other people's evals, but now you're coming up with your own. And I think obviously that is a a necessary path once you're at the frontier. You've exhausted all the existing one-on-one ones. I think the next point in history that I have for you is AI Grant that you guys decided to to join and and move here. >> What's what was it like? I think you</p>
<p>were in like batch two. >> Batch four. >> Batch four. Okay. >> I mean it was great. Nat and Daniel are obviously great and it's a really cool group of companies that we were in AI grant alongside. It was really great to get Nat and Daniel on board. Obviously, they've done a whole lot of great work in the space with a lot of leading companies and were extremely aligned with the mission of what we were trying to do. Like we're not quite typical of like a lot of the other AI startups that they've invested in and they were very much here for the mission of what we</p>
<p>want to do. Did they say any advice that really affected you in some way or like were one of the events very impactful? >> That's an interesting question. I I mean I remember fondly a bunch of the um speakers who came into fireside chats at AI Grant >> which is also like a crazy list. >> Yeah. Oh totally. Yeah. Yeah. Yeah. I I I there was something about um you know speaking to that and Daniel about the challenges of of working through a startup and just working through the questions that don't have like clear answers and how to to work through those kind of methodically um and just like</p>
<p>work through the hard decisions uh and they've been great mentors to to us as we built artificial analysis. Another benefit for us was that other companies in the batch and other companies in AI grant are pushing the capabilities of of what AI can do at this time. And so being in contact with them, making sure that artificial analysis is is useful to them has been fantastic for for supporting us in working out how should we build out artificial analysis to continue to being uh useful to those</p>
<p>like you know building on AI. I think to some extent I'm mixed opinion on that one because to some extent your target audience is not people in AI grants who are obviously at the frontier >> to some to some extent but then so [laughter] a lot of what the AI grant companies are doing is um taking capabilities coming out of the labs and trying to push the</p>
<p><strong>[18:11]</strong></p>
<p>limits of what they can do across the entire stack for building great applications which actually makes some of them pretty archetypical power users of artificial analysis. some of the people with the strongest opinions about what we're doing well and what we're not doing well and what they want to see next from us. Cuz when you're building any kind of AI application now, chances are you're using a whole bunch of different models, you're maybe switching reasonably frequently for different models and different parts of your application to optimize what you're able to to do with them at an accuracy level and to get</p>
<p>better speed and cost characteristics. So for many of them, no, they're um like not commercial customers of ours. Like we don't charge for all that data on the website, but they are absolutely some of our power users. >> So let's talk about just the the the evals as well, right? Like you start out from the the general like MMLU and and GPQA stuff. Um what's next? How do you how do you sort of build up to um the overall index was in V1 and how did you evolve it? Okay, so first just like background like we're talking about the</p>
<p>artificial analysis intelligence index which is our synthesis metric that we pull together currently from 10 different email data sets to give what we're pretty confident is the best single number to look at for how smart the models are. Obviously doesn't tell the whole story. That's why we published the whole website of all the charts to dive into every part of it and look at the trade-offs. But best single number. So right now it's got in it a bunch of Q&A type data sets that have been very important to the industry like a couple that you just mentioned. It's also got a</p>
<p>couple of agentic data sets. It's got our own long context reasoning data set and some other use case focused stuff. As time goes on the things that we're most interested in that are going to be important to the capabilities that are becoming more important of AI, what developers are caring about are going to be first around agenda capabilities. So surprise surprise, we're all loving our coding agents and how the model is going to perform like that and then do similar things for different types of work are really important to us. The linking to</p>
<p>use cases to economically valuable use cases are extremely important to us. And then we've got some of these um things that the models still struggle with like working really well over long contexts that are not going to go away as specific capabilities and use cases that we need to keep evaluating. >> Mhm. And but I guess one thing I was driving was like the V1 versus the V2 and how bad it was over time >> like how like how we've changed the index to where we are. >> Yeah, I think that reflects on well the</p>
<p>change in the industry, right? So that's a nice way to tell that story. >> Well, V1 would be completely saturated right now [laughter] by almost every model coming out because doing things like writing the Python functions in human eval is now pretty trivial. It's easy to forget actually I think how much progress has been made in the last two years. Like we we obviously play the game constantly of like the</p>
<p><strong>[21:13]</strong></p>
<p>today's version versus last week's version and the week before and all of the small changes in the horse race between the current frontier and the who has the best like smaller than 10b model like right now this week right and that's very important to a lot of developers and people in especially in this particular city of San Francisco. But when you zoom out a couple of years ago, literally most of what we were doing to evaluate the models then would all be 100% solved by even pretty small models today. And that's been one of the key things, by the way, that's driven down the cost of um intelligence at every tier of intelligence. We can talk</p>
<p>about more in a bit. So V1, V2, V3, we made things harder. We covered a wider range of use cases and we tried to get closer to things developers care about as opposed to like just the Q&A type stuff that MMLU and GPQA represented. >> Yeah. I don't know if you have anything to add there. Uh or we could just go right into showing people the benchmark and like clicking around and ask asking questions about it. >> Yeah, let's do it. >> Okay. >> This would be a pretty good way to chat about a few of the new things we've</p>
<p>launched recently. >> Yeah. And I think a little bit about the direction that we want to take it and we want to push benchmarking. Currently the intelligence index and and and eval focus a lot on kind of raw intelligence but we kind of want to diversify how we think about intelligence and uh we can we can talk about it but kind of new evals that we've kind of built um and partnered on focus on topics like hallucination and we've got a lot of topics that I think are not covered by the current eval set</p>
<p>that should be uh and and so we want to bring that forth but before we get into that So, so for listeners just as a time stamp right now on number one is Gemini 3 Pro High, then followed by Claude Opus at 70. Uh just 5.1 high. You don't have 5.2 yet. Uh and Kimmy K2 thinking. Wow. Still hanging in there. So those those are the top four. >> That will date this podcast quickly. [laughter] >> Yeah. Yeah. I mean I I love it. I love it. >> This time next year and go how cute. >> Like totally a quick view of that is</p>
<p>okay. There's a lot I love this chart. This is such a favorite, right? And almost every conferences and stuff like we always put this one up first to just talk about situating where we are in this moment in history. This I think is the the visual version of what I was saying before about the zooming out and remembering how much progress there's been. If we go back to just over a year ago, before 01, before Claude Sonet 3.5, we didn't have reasoning models or coding agents as a</p>
<p>thing and the game was very, very different. If we go back even a little bit before then, we're in the era where when you look at this chart, like Open AI was untouchable for well over a year. And I mean you would remember that time period well of like there being very open questions about whether or not AI was going to be competitive like full stop whether or not open AI would just run away with it whether we would have</p>
<p><strong>[24:13]</strong></p>
<p>the a few frontier labs and no one else would really be able to do anything other than consume their APIs. I am quite happy overall that the world that we have ended up in is one where >> multimodel >> absolutely and strictly more competitive every quarter over the last few years. Yeah, this year has been insane. >> Yeah, you can see it. Uh, this chart with everything added is hard hard to read currently. Um, there's so many dots on it, but I think it reflects a little bit, you know, what what we felt like how crazy it's been. >> Why 14 uh as the default? Is that a</p>
<p>manual choice? >> Cuz you got service now and there that are, you know, less less traditional names. >> Yeah, it's models that we're kind of highlighting by default in our charts in our intelligence index. Okay. is where this >> you just have a manually cured list of stuff. >> Yeah, that's right. But um something that I actually don't think every artificial analysis user knows is that you can customize our charts and and choose what >> uh what modelsated. >> Um and so if we you know take off a few names, it gets a little easier to >> Yeah. Yeah. A little easier to read.</p>
<p>>> Yeah. But you can uh I love that you can see the 01 jump. Look at that. September 2024 >> and the Deep Seek jump [laughter] >> which got close to OpenAI's leadership. >> They were so close >> I think. Yeah, we we remember that moment around this time uh last year actually. >> Yeah. Yeah. Yeah. Well, couple of weeks. It was it was Boxing Day in New Zealand uh when when Deepseek V3 came out and I like we'd been tracking Deepseek and a bunch of the other global players that</p>
<p>were less known over like the second half of 2024 and had runs on the earlier ones and stuff. I I very distinctly remember Boxing Day in New Zealand. I cuz I was with family for Christmas and stuff running and getting back result by result on Deep Seek V3. Um, so this was like the the first of their V3 architecture, the 671B MOE. Um, and we were very very impressed. Like that was the moment where we were sure that DeepS was no longer just one of many players,</p>
<p>but had jumped up to be a thing. Um, the world really noticed when they followed that up with the RL working on top of E3 and R1 succeeding like a few weeks later. Um but the groundwork that absolutely was laid with a like just extremely strong base model completely open weights that we had as the best open weights model on Boxing Day last year. >> Yep. >> Boxing Day is the the day after Christmas for for those not [laughter] >> uh I mean I'm from Singapore. A lot of us remember Boxing Day uh for for a different reason for the tsunami that</p>
<p>happened. >> Of course. Yeah. So I was Yeah. Yeah. But that was a long time ago. So yeah. So this is the the rough uh pitch of AQI uh or is it AAQI or A II? >> I I so good good memory though. So once upon a time we did call it quality index um and we would talk about um quality performance and price but um we changed it to intelligence. >> There's been a few naming changes. We added hardware benchmarking to the site</p>
<p><strong>[27:15]</strong></p>
<p>and um so have benchmarks at a at a kind of system level and so then we changed our throughput metric to we now call it output speed and then [laughter] throughput makes sense at a system level. So >> got it got it. Took that name. >> Uh take me through more charts like what should what should people know? You know obviously the way you look at the site is probably different than how a beginner might look at it. >> Yeah that that's fair. We can [laughter] there's a lot of fun stuff to dive into. Um maybe so we can hit past all the like we like we have lots and lots of emails and stuff. Um the interesting ones to talk about today that' be great to bring up like a few of our recent things I think um that probably not many people</p>
<p>be familiar with yet. So first one of those is our omniscience index. So this one is a little bit different to most of the intelligence evos that we run. We built it specifically to look at the embedded knowledge in the models and to test um hallucination by looking at when the model doesn't know the answer. So we're not able to get it correct, what's its probability of saying I don't know or giving an incorrect answer. So the</p>
<p>metric that we use for omniscience goes from negative 100 to positive 100 because we're simply taking off a point if you give an incorrect answer to the question. We're pretty convinced that this is an example of where it makes most sense to do that because it's strictly more helpful to say I don't know instead of giving a wrong answer to factual knowledge question. And one of our goals is to shift the incentive that EVELs create for um models and the labs</p>
<p>creating them [clears throat] to get higher scores. And almost every email across all of AI up until this point, it's been graded by simple percentage correct as the main metric, the main thing that gets hyped. And so you should take a shot at everything. There's no incentive to say I don't know. So we did that for this one here. [clears throat] >> I think there's a general uh field of calibration as well like the confidence in your answer versus the rightness of the answer.</p>
<p>>> Yeah, completely agree. Yeah. Yeah. >> On that and one reason that we didn't do that is be uh or put that into this uh index is that we think that the the way to do that is not to ask the models how confident they are. >> I don't know maybe >> it might be though >> you put it give it a JSON field say say confidence and maybe it spits out something. >> Yeah. You know, we have done a few eval podcast over the over the years and we did one with Clementine of Hugging Face who maintains the open source leaderboard and this was one of her top requests which is some kind of hallucination/ lack of confidence</p>
<p>calibration thing >> and so hey this is one of them and I mean like anything that we do it's not a perfect metric or the whole story of everything that you think about as hallucination um but yeah it's pretty useful and has some interesting results like one of the that we saw in the hallucination rate is that anthropics claude models at the the very left hand side here with the lowest hallucination</p>
<p><strong>[30:17]</strong></p>
<p>rates out of the models that we've evaluated emissions on. That is an interesting fact. I think it probably correlates with a lot of the previously not really measured vibes stuff that people like about some of the claude models. >> Is the data set public or what's is it is there a held out set? >> There's a hel set for this one. Um so it we we have published a public test set but we we've only published 10% of it. The reason is that for this one here specifically it would be very very easy to um like have data contamination</p>
<p>because it is just factual knowledge questions. Um we will update it over time to also um prevent that but we've yeah kept most of it held out so that we can keep it reliable for a long time. It leads us to a bunch of really cool things including breakdown quite granularly by topic and so we've got some of that disclosed on the website publicly right now and there's lots more coming in terms of our ability to break out very specific topics. >> Yeah, I would be interested. Let's let's dwell a little bit on this hallucination one. I noticed that haiku hallucin hallucinates less than sauna</p>
<p>hallucinates less than opus and would that be the other way around in a normal capability environment? I don't know what's what do you make of that? One interesting aspect is that we've found that there's not really a not a strong correlation between intelligence and hallucination rate. That's to say that the smarter the the models are in a generalist sense isn't correlated with their ability to when they don't know something say that they don't know. It's interesting that Gemini 3 Pro preview was a big leap over here Gemini 2.5</p>
<p>flash and and and 2.5 Pro. But and if I add pro quickly here, >> I bet pro is really good. Uh actually, no. So I me I meant uh the GPT pros. >> Oh yeah, >> cuz GPT pros are rumored. We don't know for a fact that it's like eight runs and then with the LM judge on top. >> Yeah. So we saw a big jump in this is accuracy. So this is just percent that they get uh correct. And Gemini 3 Pro knew a lot more than the other models. And so big jump in accuracy, but</p>
<p>relatively no change between the Google Gemini models between releases >> and the hallucination rate. >> Exactly. And so it's likely due to just kind of different post training recipe between the the claw models. >> Yeah. >> Um that's driven this. >> Yeah. You can uh you can partially blame us and how we define intelligence having until now not defined hallucination as u negative in the way that we think about intelligence. And so that's what we're changing. Uh I know many smart people who are confidently incorrect.</p>
<p>[laughter] >> Look look that that that is very human very true >> and there's times and a place for that. I think our view is that hallucination rate makes sense in this context where it's around knowledge but in many uh cases people want the models to hallucinate to have a go. Often that's the case in coding or when you're trying to generate newer ideas. One eval that we added to artificial analysis is is is</p>
<p><strong>[33:18]</strong></p>
<p>critical point and it's really hard uh physics problems. >> Okay. >> And is it sort of like a human eval type or something different or like a frontier math type? >> It's not dissimilar to frontier frontier math. So these are kind of research questions that kind of academics in the physics physics world would be able to answer but models really struggle to answer. So the top score here is 9%. And when the people that that created this like Minway and and actually Afia who was kind of behind Sweeten >> what organization is this or is this</p>
<p>it's Princeton >> kind of range of academics from from different academic institutions really smart people they talked about how they turn the models up in terms of the temperature [laughter] as high a temperature as they can when they're trying to explore kind of new ideas in physics as a as a thought partner just because they they want the models to hallucinate um >> yeah sometimes maybe get something newact Um so not right in every situation but um think it makes sense you know to test hallucination scenarios where it makes sense. Well, so the obvious question is uh this is one of many that there is</p>
<p>there every lab has a system card that shows some kind of hallucination number and you've chosen to not endorse that and you've made your own and I think that's a that's a choice. Um totally in some sense the rest of artificial analysis is public benchmarks that other people can independently rerun. you provided us a service here. you have to fight the well who are we to to like do this and your answer is that we have a lot of customers and you know but like I guess how do you converge the industry on one number that actually everyone</p>
<p>agrees is is the rate right cuz you have your numbers they have their numbers never the the two shall meet >> I mean I think I think for hallucination specifically there are a bunch of different things that you might care about reasonably and that you'd measure quite differently like we've called this a nissian hallucination rate not trying to declare the legacy. >> Humanity's last hallucination, [laughter] >> you could uh you could have some interesting naming conventions and all this stuff. Um the biggest picture answer to that and something that I actually wanted to mention just as George was explaining critical point as</p>
<p>well is so as we go forward we are building evals internally. We're partnering with academia and partnering with AI companies to build great evals. We have pretty strong views on in various ways for different parts of the AI stack where there are things that are not being measured well or things that developers care about that should be measured more and better and we intend to be doing that. We're not obsessed necessarily with that everything we do we have to do entirely within our own team. Critical point is a cool example of where we were launch partner fraud</p>
<p>working with academia. We've got some partnerships coming up with a couple of leading companies. Those ones obviously we have to be careful on some of the independent stuff but with the right disclosure like we're completely comfortable with that. A lot of the labs have released great data sets in the past that we've used to create success independently and so it's between all of those techniques we're going to be releasing more stuff in the future. >> Cool. Let's cover the the last couple and then we'll I want to talk about your</p>
<p><strong>[36:19]</strong></p>
<p>trends analysis stuff you know. >> Totally. Before that actually I have one like little factoid on if you go back up to accuracy on omniscience. An interesting thing about this accuracy metric is that it tracks more closely than anything else that we measure the total parameter count of models. >> Oo, >> makes a lot of sense intuitively, right? Because this is a knowledge eval. This is the pure knowledge metric. We're not looking at the index and the hallucination rate stuff that we think is much more about how the models are trained. This is just what facts did they recall and yeah, it tracks</p>
<p>parameter count extremely closely. Okay. What's the rumored size of Jupit 3 Pro? [laughter] And to be clear, not confirmed for any official source, just just rumors, but rumors do fly around. Rumors >> I get I hear all sorts of numbers. I don't know what to trust. >> So if you if you draw the line on emissian's accuracy versus total parameters, we've got all the open ways models. You can squint and see that likely the leading frontier models right now are quite a lot bigger than the one trillion parameters that the open</p>
<p>weights models cap out at. And the ones that we're looking at here, there's an interesting extra um data point that Elon Musk revealed recently about XAI that Grock 3 trillion parameters for Grock 3 and 4, 6 trillion for Gro 5, but that's not out yet. Take those together, >> have a look, you might reasonably form a view that there's a pretty good chance that Gemini 3 Pro is bigger than that, that it could be in the 5 to 10 trillion parameter range. To be clear, I have absolutely no idea. But just based on</p>
<p>this chart, like that's where you would you would land if you have a look at it. >> Yeah. And to some extent, I actually kind of discourage people from guessing too much because what does it really matter? Like as long as they can serve it as a sustainable cost, that's about it. Like >> yeah, totally. They've also got different incentives in play compared to like open weights models who are thinking to supporting others in self- deployment for the labs who are doing inference at scale. It's I think less about total parameters in many cases when think about inference costs and and more around number of active parameters</p>
<p>and so there's a bit of an incentive towards larger sparer models. >> Agreed. Understood. >> Yeah. Great. >> I mean obviously if you're a developer or company using these things not exactly as you say it doesn't matter. You should be looking at all the different ways that we measure intelligence. You should be looking at cost to run index number and the different ways of thinking about token efficiency and cost efficiency based on the list prices cuz that's all that matters. >> It's not as good for the content creator rumor mill where I can say well GPT4 is this small circle. Look at GPT5 is this big circle and that it used to be a</p>
<p>thing for a while. [laughter] >> I mean but that that that is like a on its own actually very interesting one right that >> is it? [laughter] Well, no, just purely that chances are the last couple years haven't seen a dramatic scaling up in the total size of these models and so there's a lot of room to go up probably and total size of the models especially with the upcoming hardware generations. >> Yes. So, um you know taking off my</p>
<p><strong>[39:21]</strong></p>
<p>posting phase for a minute. [laughter] Uh yes. Yes. At the same time, I I do feel like, you know, especially coming back from Europe, people do feel like Ilia is probably right that the paradigm is doesn't have many more orders of magnitude to scale out more and therefore we need to start exploring at least a different path. >> GDP val I think it's like only like a month or so old. Um I was also very positive when it first came out. I actually talked to Tedel uh who was the the lead researcher on that. Oh >> and uh you have your own version. >> It's a fantastic data set. >> Yeah. And >> maybe I will recap for people who are</p>
<p>still out of it. It's like 44 tasks that based on some kind of GDP cutoff that's like meant to represent broad white color work that is not just coding. >> Yeah. >> Yeah. >> Each of the tasks have a whole bunch of detailed instructions, some input files for a lot of them. It's I within the 44 is divided into like 220 22 to5 maybe um subtasks that are the the level of that we run through the agents and yeah, they're really interesting. I will say that it doesn't necessarily capture like all the stuff that people do at work. No avail is perfect. There's always going</p>
<p>to be more things to look at largely because in order to make the tasks well enough to find that you can run them, they need to only have a handful of files and very specific instructions for that task. And so I think the easiest way to think about them are that they're like quite hard take-home exam tasks that you might do in an interview process. >> Yeah. Yeah, for listeners is not no longer like a long prompt. It is like well here's a zip file with like a spreadsheet or a PowerPoint deck or a PDF and go nuts and answer this question.</p>
<p>>> Yeah, OpenAI released a great data set and they released a good paper which looks at performance across the different you know web chat bots on the data set. It's a great paper encourage people to read it. What we've done is taken that data set and turned it into an EVEL that can be run on any model. So we created a reference agentic harness that can run the models on the data set and then we developed a valuator approach to compare outputs that's kind of a AI enabled. So it uses Gemini 3 pro</p>
<p>preview to compare results which we tested pretty comprehensively to ensure that it's aligned to to human human preferences. One data point there is that um even as the as as an evaluator Gemini 3 Pro interestingly doesn't do actually that well in GDP val aa >> yeah the thing that you have to watch out for with OM judge is selfreference that models usually prefer their own output uh and in this case it was not >> totally I think the the way that we're</p>
<p>that we're thinking about the places where it makes sense to use an LLM's judge approach now like quite different to some of the early LLM's judge stuff a couple of years ago because some of that and MTV was a great project that was a good example of some of this a while ago was about judging conversations and like a lot of style type stuff. Here we've got the task that the grader grading</p>
<p><strong>[42:22]</strong></p>
<p>model is doing is quite different to the task of taking the test. When you're taking the test, you've got all of the agentic tools. You're working with the code interpreter and web search the file system to go through many many turns to try to create the documents. Then on the other side when we're grainy it we're running it through a pipeline to extract visual and text versions of the files and be able to provide that to Gemini and we're providing the criteria for the task and getting it to pick which one more effectively meets the criteria of the task out of two potential outcomes. It turns out that we prove that it's just very very good at getting that</p>
<p>right matched with human preference a lot of the time because it's I think it's got the raw intelligence but it's combined with the correct representation of the outputs. the fact that the outputs were created with an agentic task that is quite different to the way the grading model works and we're comparing it against criteria not just kind of zero shot trying to ask the model to pick which one is better. >> Got it. Why is this an ELO and not a percentage like GDP val? >> So the outputs look like documents and there's video outputs or audio outputs</p>
<p>from some of the tasks and >> so he has to make a video. >> Yeah, >> for some of the tasks. What task is that? [laughter] >> I mean, it's in it's in the data set. >> Maybe a YouTuber. >> It's a marketing video. >> Oh. >> Uh, what? >> Like model has to go find clips on the internet and try to put it together. The models are not that good at doing that one for now. To be clear, it's pretty it's pretty hard to do that with the code interpreter. [laughter] Um, and the computer stuff doesn't work quite well enough and so on and so on. But, um, yeah. >> And so there's no kind of ground truth necessarily to compare against to work</p>
<p>out percentage correct. it's hard to come up with correct or in in incorrect there and so it's on a relative basis and so we use an ELO approach to compare outputs from uh each of the models um between between the task >> you know what you should do you should you should pay a contractor human to do the same task and then give it an ELO >> and then so you have you have human there so I think what's helpful about GDP val the open eye one is that 50% is meant to be normal human and and and</p>
<p>maybe domain expert is higher than that but 50% was the the bar for like well if you've crossed 50 you are super human. >> Yeah. So we like haven't grounded this score in that. Exactly. I agree that it can be helpful but we wanted to generalize this to a very large number of models. It's one of the reasons that presenting as law is quite helpful and allows us to add models and it'll stay relevant for quite a long time. I also think it it it can be tricky looking at these exact tasks compared to the human performance cuz the way that you would</p>
<p>go about it as a human is quite different to how the models would go about it. >> Yeah. [snorts] Uh I also like that you included Llama for Maverick in there. Is that like just one last like [laughter] >> No, no, no, no, no, no. It is the it is the best model released by Meta and [laughter] so it makes it into the homepage default set still for now. Other inclusion</p>
<p><strong>[45:23]</strong></p>
<p>that's quite interesting is we also ran it across the latest versions of the web chat bots and so we have >> Oh, that's right. Oh, sorry. I Yeah, I completely missed that. Okay. >> No, not at all. So that that which has a checkered pattern. >> So So that is their harness, not yours is what you're saying. >> Exactly. And what's really interesting is that if you compare for instance Claude 4.5 Opus using the Claude web chatbot, it performs worse than the model in our agentic harness. >> And so in every case, the model performs better in our agentic harness than its</p>
<p>web chatbot counterpart, the harness that they created. >> Oh, my backwards explanation for that would be that well, it's meant for consumer use cases. And here you're pushing it for something. >> The conraints are different and the amount of freedom you can give the model is different. Also, you like have a cost goal. We Yeah. Let the models work as long as they want basically. Yeah. >> You copy paste manually into the chatbot. >> Yeah. >> Yeah. >> That's >> that was how we got the chatbot reference. Yeah. >> We're not going to be keeping those updated at like quite the same scale as on the [laughter] hundreds of models on</p>
<p>>> So, I don't know. Talk to browser base. They'll they'll automate it for you, you know, like >> true. Yeah, we should >> uh I have thought about like well we should turn these chatbot versions into an API because they are legitimately different agents in themselves. >> Yes. Yeah. And that's grown a huge amount over the last year, right? Like the tools that are available have actually diverged in my opinion a fair bit across the major chatbot apps and the amount of data sources that you can connect them to have gone up a lot. Meaning that your experience and the way you're using the model is more different</p>
<p>than ever. what tools and what data connections come to mind when you say what's interesting what what what's notable work that people have done. >> Ah okay. So my favorite example on this is that until very recently I would argue that it was basically impossible to get an LLM to draft an email for me in any useful way because most times you're sending an email. You're not just writing something for the sake of writing it. Chances are context required is a whole bunch of historical emails. Maybe it's notes that you've made. Maybe it's meeting notes. Maybe it's um</p>
<p>pulling something from your um any of like wherever you at work store stuff. So for me like Google Drive, one drive um in our superbase databases if we need to do some analysis on some data or something preferably model can be plugged into all of those things and can go do some useful work based on it. The things that like I find most impressive currently that I am somewhat surprised work really well in late 2025 are that I can have models use superbase MCP to >> query</p>
<p>>> read only of course run a whole bunch of SQL queries to do pretty significant data analysis and make charts and stuff and can read my Gmail and my notion >> and okay you actually use that that's good that's that's good is that a cloud thing >> to various degrees supported both JBD and claude right now I would say that this stuff like barely works in fairness right now. [laughter] Um like</p>
<p><strong>[48:24]</strong></p>
<p>>> because people are actually going to try this after they hear. >> If you get an email from Mic, odds are it wasn't written by a chatbot. >> No. So yeah, I think it is true that I have never actually sent anyone an email drafted by a chatbot yet. Um and so >> but [laughter] you can you can feel it, right? And this time this time next year we'll come back and see where it's going. >> Totally. >> Um Superbase shout out another famous Kiwi. Uh I don't know if you've you've any conversations with him about anything in particular on on AI building and AI infra. >> We have had Twitter DMs um with with him because we're quite big uh Superbase</p>
<p>users and and power users and we probably do some things more manually than we should in in Superbase. [laughter] >> So he's just the support line because you're you're >> a little bit yeah been being super friendly. [laughter] One extra um point regarding um GDP val AA is that on the basis of the overperformance of the models compared to the the chat bots. Turns out we realized that oh like our reference harness that we built actually works quite well on like g generalist agentic</p>
<p>tasks. >> This proves it in a sense. And so the agent harness is very minimalist. I think it follows some of the ideas that are in clawed code. And we all that we give it is context management capabilities a web search web browsing uh tool uh code execution uh environment anything else >> I mean we can equip it with more tools but like by default yeah that's we we give it for GDPL tool to uh view an image specifically um because the models</p>
<p>you know can just use a terminal to pull stuff in text form into context but to pull visual stuff into context we had to give them a custom tool but yeah exactly you to explain an expert. >> No, so it's it it we turned out that we created a good generalist agentic harness and so we um released that on on GitHub yesterday. It's called Stirrup. So if people want to check it out and and it's a great um you know base for you know generalist building a generalist agent. >> It is kind of >> for more specific tasks. I'd say the best way to use it is get clone and then</p>
<p>have your favorite coding agent make changes to it to do whatever you want because it's not that many lines of code and the coding agents can work with it super well. >> Well, that's nice for the the community to explore and share and hack on it. I think maybe in in in other similar environments the terminal bench guys have done uh start harbor uh and so it's it's a bundle of well we need our minimal harness which for them is terminus. Yep. And we also need the RL environment or docker deployment thing to to run independently. So I don't know</p>
<p>if you've looked into hardware at all. Is that is that like a standard that people want to adopt? >> Yeah, we've looked at it from a eval um perspective and we love terminal bench and and host benchmarks of of terminal bench on on artificial analysis. Um we've looked at it from a from a coding agent um perspective but could see it being a great um basis for any kind of</p>
<p><strong>[51:26]</strong></p>
<p>agents. I think where we're getting to is that these models have gotten smart enough, they gotten better better at tools that they can perform better when just given a minimalist set of tools and and let them run. Let the model control the the agentic workflow rather than using another framework that's a bit more built out that tries to dictate the dictate the flow. >> Awesome. Let's cover openness index and then let's go into the report stuff. Uh so that's the that's the last of the proprietary numbers I guess. I don't know how you sort of classify all these. >> Yeah. Or call call it let's call it the</p>
<p>last of like the the three new things that we're talking about from like the last few weeks. Um because I mean there's a we do a mix of stuff that where we're using open source where we open source and what we do and um proprietary stuff that we don't always open source like long context reasoning data said last year we did open source um and then all of the work on performance benchmarks across the site. Some of them we looking to open source but some of them like we're constantly iterating on and so on and so on. There's a huge mix I would say just of like stuff that is open source and not across the side.</p>
<p>>> So that's a LCR for people. >> Yeah. Yeah. >> But let's talk about open >> let's talk about open syntax. This here is call it like a new way to think about how open models are. We for a long time have tracked where the models are open weights and what the licenses on them are. And that's like pretty useful that tells you what you're allowed to do with the weights of a model. But there is this whole other dimension to how open models are that is pretty important that we haven't tracked until now. And that's how much is disclosed about how it was</p>
<p>made. So transparency about data, pre-training data and post- training data and whether you're allowed to use that data and transparency about methodology and training code. >> Mhm. >> So basically those are the components. We bring them together to score an openness index for models so that you can in one place get this full picture of how open models are. >> I feel like I've seen a couple other people try to do this but it they're not maintained. I I do think this does matter. I don't know what the numbers</p>
<p>mean apart from is there a max number? Is this out of 20? >> It's out of 18 currently. And so we've got an openness index um page but essentially these are points. So you get points for being more open across these different categories and the the maximum you can achieve is 18. So AI 2 with their extremely open 32B think model is a leader in a sense >> with hugging face. >> Oh with their with their smaller model. Um it's coming soon. I think we need to run we need to get the intelligence benchmarks run on the side.</p>
<p>>> We can't have an open index and not include hugging face. >> We love hugging face. We'll have that have that up very soon. >> I mean uh you know the refined web and all all that stuff. It's uh it's amazing. Or is it called fine web? Fine web. >> Fine web. Yeah. Yeah. Totally. Yeah. One of the reasons this is cool, right, is that if you're trying to understand the holistic picture of the models and what you can do with all the stuff the company's contributing, this gives you that picture. And so we are going to</p>
<p><strong>[54:26]</strong></p>
<p>keep it up to date alongside all the models that we do intelligent settings on on the site. And it's just an extra view to understand. >> Can you scroll down to the the trade-offs chart? Yeah. Yeah, that one. Yeah. This this really matters, right? Obviously, cuz you can be super open but dumb. >> I mean, [laughter] >> the slant obviously goes the wrong way here, right? And >> a lot of people would like to see labs hill climb on the and target the open. This is the access to to hill climb. Yeah. >> Unfortunately, it might be fundamentally true that the the slum will always go this direction because once you open</p>
<p>something up, then everyone else can get to the level of what you open up. >> Well, so let me let me tweak your point system, right? like you have these like numbers on the point system and it go up to 18, you know, but like just because I have a little bit of open data doesn't mean I'm necessarily that much better in someone who put a lot of effort into their open ways that is smarter. So I might I might just mess with the point system to make sure that like I'm accurately representing the the contribution to the open so openness. It is hard to wait for the materiality of the contribution to to open source like</p>
<p>it's we tried to make it so that it is quite well defined and no one can disagree about like which um category things should be and so we're not saying like this was a big contribution or a small contribution in terms of um impact on the industry or anything. It's just like how much of your data did you release. I would say that it is still valid to say that we train a model that's not that smart, maybe even not at the frontier for a particular size category, but we chose to open up all the data, all the training code. That is a very useful exercise for the industry</p>
<p>and we want to recognize that even if the smartest model in the category >> Yeah. Yeah. And also special shout out to Nvidia Neotron which doesn't get enough credit for the amount of stuff that they do and honestly it's a sales enablement for Nvidia as well. like the fact that they can do this as a side project. >> Totally. [laughter] But I mean, but it is true that Nvidia have actually put an enormous amount of effort over the last year especially into the neatron models and >> and so many people actually use it for like synthetic data and stuff like it's it's uh it's a pretty interesting secret of the industry that uh Nvidia holds up all these guys. [laughter]</p>
<p>>> I mean it's in their interest for there to be more AI. Uh so obviously I think you want to push openness as as having an index every index that you push like has encodes some kind of opinion or value. >> Yes. >> I think one of the openest questions from this year was people messing with the the the license. And so Llama had this like if you have 700 million daily active users you're not allowed to use our model or you have to talk to us something like that. So basically like what are your customers telling you about the kind of licensing worries that</p>
<p>they have right because obviously most people will never hit 700 million users. >> We have like a detailed breakdown of that in the openness index and that was actually one of the initial questions like took us down the route of wanting to do this. Um [laughter] cuz yeah the simplest thing that like our opinion is is that there is a lot of advantage to having like an official OSI license like MIT or Apache 2 because</p>
<p><strong>[57:28]</strong></p>
<p>then the box is just checked. You don't even need to read it because it's just Apache 2 and you can do whatever you want and it's fine. There are often very good reasons that companies don't want to release language models with those completely open licenses. The index tells you. So if you get the top category, it's one of those licenses, you're totally good. And then we've got um some lower categories for when attribution is required. Um and then when commercial use is not allowed. Yeah, they're there. >> So that's the open openness index. Thank you for doing all those all those works. Let's talk a little bit or at least end</p>
<p>the pod on just the the trend reports that you guys do which is kind of a bit of the bread and butter how you make money. Highly encourage everyone to see George's talk at World's Fair which gives a little bit of a preview and you're you were very excited about talking about the smiling curve or I don't know what you call it. >> Yeah. Yeah. Yeah. Yeah. Yeah. Let's talk about that one. >> Let's explain it for people and and I might I might actually put put it up um because I don't have it >> got to copy the slide. That'll be that'll be excellent. >> It's important for people to have in their head because Yeah. People only get the marketing message from the labs that oh we're cutting cost all the time.</p>
<p>>> Yeah. Yeah. But it's it's true. It's just that it's [laughter] not the whole picture. So okay, a couple of like the big trends that we track at artificial analysis over time and that like we're always showing charts of on the trends page and these reports and stuff. One that the cost of intelligence has been falling dramatically over the last couple of years. The best way to think about that is that the cost for each terror of intelligence has been dropping. The like one fact on that is that you can get intelligence at the level of GPD4 for over 100 times cheaper than GPD4 was at launch right now.</p>
<p>>> I think my number is a thousand actually >> if you look at um the Amazon Nova models which are very very cheap. >> Yeah. like my my conservative uh statement is normally like but in fairness this slide like I we were actually saying before the podcast right is like maybe six months old now and it's conceptually still correct but like could actually probably do on the exact numbers because like the market's moving so quickly. >> Yeah, feel free to kick it off. I I mean we'll have to try I told people to watch the world's fair talk but let's let's introduce what context makes you make</p>
<p>something like this. There are two trends that seem to not make sense together. Both of which we talk a lot about at artificial analysis and are very important to developers building stuff in AI. The first is that the cost of intelligence for each level of intelligence has been dropping dramatically over the last couple of years. We track the cost to run artificial analysis intelligence index for each bucket of intelligence index scores. and each bucket you just see the line go down really really quickly and</p>
<p>actually go down more quickly for each new level of intelligence that's been achieved over the last couple of years. So the rate of that cost plan has actually been going up. So we've got that being true and yet it is clearly possible to spend quite a lot more on AI inference now than it was a couple of years ago. Nvidia stock go up. [laughter] >> It's going it's going really up. Uh I just heard from a friend's startup that</p>
<p><strong>[60:30]</strong></p>
<p>just went to the age of zero. They're spending $5,000 per employee on coding agents spend alone. >> That's ridiculous. >> That's an impressive number. We need to get our numbers up. We're uh we're not going to be hitting. >> I was like, it's so high that I'm like, are you doing something wrong? >> Yeah, [laughter] cuz there are some efficiency questions along the way, but like you can make AI inference useful to that level in a bunch of ways that I can imagine, right? Um I I don't think that's that nuts. Um but basically the reason we made this slide to answer the question right is to show that the crazy thing is that it is</p>
<p>actually true that we've had this 100x to a,000x decline in the cost of GP4 level intelligence on the left hand side and yet on the right hand side because the multipliers are so big for the fact that even though small models can do GB4 level now we still want to use big models and probably bigger than ever models to um do frontier level intelligence. We've got reasoning models using tokens and then we're throwing them these them in these agentic workflows where they're consuming enormous numbers of input tokens and making enormous numbers of output tokens working for a really long time. Those</p>
<p>two things taken together get you back to we can spend enormously more today than we could a couple years ago. >> Yep, I think that's right. There's a number of drivers at play and we kind of outline kind of six key ones here. Um but you know as complex it's changing quickly. All of these have changed very dramatically in the last uh in the last 12 months. Let's pick on hardware efficiency since you you also have you also track hardware stuff and I think the general assertion or the the message is that the efficiency from nextg Nvidia chips is actually not 4x or you have</p>
<p>what 3x or 4x you have 3x in here and it's it's like 2x maybe or it's more of like a power story rather than like a a sheer sort of compute tokens efficiency story but yeah what's going on in in hardware >> okay so the the the unfortunately uh is it depends and it just depends massively on like so many things across a bunch of different types of workloads and ways to think about it. So one of the simplest ways to think about this is to take single relevant model to think about</p>
<p>serving it at speeds that are realistic for what you actually might want to hit and can afford to hit and then think about the throughput per GPU that you can achieve serving the model at those speeds. One of the reasons that's important is that there's a trade-off between the throughput per GPU that you can achieve and the per user speed that you can achieve. And as in it costs more to serve stuff fast to users. When you run all of that for especially big sparse models, you can get a lot better than 2 or 3x gain going from Hopper to Blackwell generation Nvidia. I am, this</p>
<p>shouldn't be too controversial a statement, but like I'm pretty confident that Blackwell has delivered pretty enormous gains and that the next couple of years of Nividia's road map are going to continue to deliver quite enormous gains and that those will actually come through as lower total cost per token to the companies that are running models on</p>
<p><strong>[63:32]</strong></p>
<p>them and will allow bigger models, will allow way more tokens to be made for lower cost and that that's going to continue these things also stack on all of the software and model improvements being made. So basically like my prediction across like both sides of that like smile chart uh that we're going to see the left hand side continue to be true and probably like for another order of magnitude and the right hand side continue to be true for another order of magnitude and that going to enable a whole lot of things. >> Okay. Well, I'll push on let's go back to the the small chart. I'll push back</p>
<p>on sparity, right? Uh we've gone a long way on sparity. Deepseek was a major pusher of fine grain experts let's call it right I have a mental number of sparsity in terms of let's say active params versus total params and that number went from >> 25% let's say down to like 15 right you obviously can't really go below I don't know five >> so there's a lower limit to to sparity is what I'm saying >> I don't know that that's obvious actually >> all right</p>
<p>>> um there must be a limit somewhere right >> yeah exactly But we've got numbers in the wild that are quite a lot lower than that right now. So the GBD OSS models like the big ones at about 5% um active. Kimmy K2 is at like 3% active. Oh, okay. >> I think pretty sure I I've looked at those numbers. I've calculated them. I don't remember. Yeah. But I I remember thinking like this must be it. your your 5% is is exactly is like around the ballpark for the for the open weights</p>
<p>models of of what's released today. I think one interesting that gives me kind of pause when thinking that it won't go the sparity won't go higher or the number of percentage of active parameters lower is that we in our benchmark see a lot of performance to correlated more with total parameters than active and not that correlated with how sparse like the models are. our accuracy benchmark as part of AA omniscience. It's very correlated with total. It's not correlated with with active uh parameters which I think is</p>
<p>very at all which is very very interesting and so I think yeah there could there could be quite a bit um to go here. >> Awesome. Well, we don't have that much time but I I did want to leave some room to cover reasoning and non-reasoning models and token efficiency. >> Let's do that. So at a at a super high level, people have to classify this binary thing of reasoning versus non-reasoning. >> People who are insider have some discomfort with that because basically you just have the think tag or no think tag. >> How have you guys decided to approach this? And also how does that laid out in over the course of the year where we</p>
<p>have things like GT5 which is a model router? >> Let's say GVD5 and CH GVD the consumer experience is a model router. when you're hitting the API like we can you can pick the different versions and you can pick reasoning strength of the different versions but that that goes to why this is now such a complex thing. So earlier this year and probably when you and George last spoke for the AI engineers World's Fair, we had this</p>
<p><strong>[66:33]</strong></p>
<p>great slide that was super easy where we would show that the average reasoning model is using 10 times the number of tokens per query in our intelligence index as the average non-reasoning model. And there was this moment where that was a pretty clear distinction and extremely useful to look at it just like that. definitely no longer the case. Not least because you can think about reasoning strength for a bunch of these different models, but particularly because different models have wildly different token efficiency now, more than an order of magnitude in difference. That means that the way that you probably need to think about cost</p>
<p>for any application is to use something like our cost to run intelligence index metric as the starting point for what it's going to look like for these different models, these different reasoning strengths and this continuous spectrum from non-reasoning to reasoning. That's basically like where we're at. We will still show reasoning and unresing and define reasoning as when there is that separated chain of thought that you're getting in a different parameter in an API normally. But it doesn't necessarily anymore mean that that model is actually going to have longer end to end latency that it's</p>
<p>going to use more tokens than something that is branded on a non-reasoning model for the same task. >> That's true. I think 5.1 was it and then 5. Codeex had these this chart which is super nice of this like let's say bottom 10 percentile query being faster but top 10 percentile being longer and that's a kind of the efficiency chart you want to see right? >> Yep. That so so that is a an extra thing. Let's say [laughter] let's say let's say that we've got that's a really important extra thing though, right?</p>
<p>That you've got not just the average number of tokens being used by the model which we cover really well right now. But the behavior that you want in the model is it to use more tokens when it needs more tokens and not to use more tokens when it doesn't need more tokens. So that's what OpenAI were basically claiming that 5.1 CEX is better at. We don't actually publish anything on this right now but have tracked it a bunch internally in our internal analytics on evals across all the models that we run where we look at the difficulty the questions and the correlation between token usage and difficulty and net net surprise surprise like models have got</p>
<p>better at doing that over the course of this year I think going into next year that's going to be really important especially as you multiply it by the number of steps in an agentic workflow that a model has to take to get to an answer we are going to care a lot about token efficiency and number of turns efficiency for getting to what we want. >> Which would you rather have token efficiency or number of turns efficiency? >> Um >> or [snorts] like which is more important to work on? >> Like it depends on the application and both are going to be really important. >> Uh yeah, retail tobench airline.</p>
<p>>> Yeah. Interestingly in TA um T2 bench telecom run you know on a per token basis more expensive models like a GBD5 compared to some smaller open source models because the um some of the GB5 for instance got to the answer faster and so it was able to resolve the</p>
<p><strong>[69:34]</strong></p>
<p>customer's query faster and fewer turns and maybe it used more tokens per turn but it certainly cost more per token so you would always rather use 5 in in in that scenario. And so I think that's what that's where we're getting to. I think number of turns is is going to be a metric that we're going to be talking about a lot more and uh I think it'll be something that people want to really start to think about uh a lot more. >> There's a trade-off in benchmarking here where most benchmarks need to be one turn to be autonomous, to be parallelized and all that, but most a lot of real life use cases need to be multi-turn and especially like quick</p>
<p>multi-turns so you can align. Yeah. >> Yeah. I mean I I would say that historically benchmarks have been single turn but I wouldn't say they need to be at all into the future right like we have a couple of agentic benchmarks in the index right now and GDP val that we were talking about we let the models do up to 100 turns in um our stirrup agents to do that eval and we're going to build similar stuff like that in the future it definitely is hard and you've got whole kinds of infrastructure problems to run that and exactly as you say parallelize it because we need to run that on</p>
<p>hundreds of models and we want to do that really fast when new models come out and when labs want us to run it on their models, but you can do it. We're putting in the work to build that stuff and it's going to be great. >> Okay, so we've covered I mean there's a lot more to cover and you haven't even touched on multimodal which is huge. >> We also do speech benchmarking, image benchmarking, uh video benchmarking hardware. >> I like the way that you've done it because very smart which is uh video takes a long time. >> So you pre-generate, right? So then people just pick their preferences and</p>
<p>you can see the the overall arena results and you also avoid like any sensitivity issues around like unsafe content that that is being generated. >> Yeah. And you can see it as a good good thing or bad thing depending on what your view is. But it means that we have a quite active creative direction approach to trying to understand what creative professionals and users want to do with those image and video models and so that we can be directing the arenas and our categories toward gathering votes on what people care about. One</p>
<p>call out actually to listeners like if you are using our arenas is that you can submit requests to us for things that we should cover. I didn't know that >> underststudied categories, areas that you think the models are bad at and the labs don't focus on enough. Like if you want something solved, one of the levers that you have is send us a couple of prompts on it. We might be able to get a category going on it. And this thing that we were talking about earlier, right, that once things get measured, they can get targeted. You can make that work for you. >> For me as a content creator,</p>
<p>infographics >> very needed. I took the latest Deepseek paper and uh I you know they had some descriptions of their search agents and their coding agents and I put it in I created an infographic and um I I I just think like that's an industrial use case that doesn't require a lot of I guess design taste but just requires some like you need to conform to some preset references which is something that uh that is increasingly important especially in like the nano banana</p>
<p><strong>[72:35]</strong></p>
<p>series but um yeah I think like open is releasing image 2 soon which uh is going to have So I I think like it's it's all like of a kind where people need to incentivize like workhorse use cases and not just art. I don't know. >> Totally. Yeah. >> What are we going to be talking about next year? Like what's what's like what's emerging that you're seeing and like maybe not in the discussion. >> The first answer that I'll give to that as the the boring answer is that on most of our charts the lines go in a particular direction and our overall prediction is the lines are going to keep going that direction. We're going</p>
<p>to do a lot and do a lot to be as useful as possible to developers and companies to measure what's important on every one of those and along those lines. But I think we're going to talk about similar stuff. It's just that we're going to have continued on this trajectory for another year and things are going to feel pretty different because of that happening. I know this is the boring answer to that question. >> No, no, I mean I'm a fan of things uh that truths that don't change because you can build and plan for that. And I think in media in general, in the podcast business, newsletter business, Twitter business, people are addicted to</p>
<p>change. Like, oh, everything's breaking, everything's No, like there's some truths that are just constants that you can plan on and build. And yeah, >> I I think one of the truths is that the demand for AI intelligence and smarter AI intelligence is going to be insatiable. Um, some people disagree that okay, once we reach certain thresholds, then you don't need more intelligence. I think to that I ask people, have they ever worked with or managed someone in a work environment and wouldn't press the button that they</p>
<p>were smarter to make them smarter or better at their job or or would they never press that for themselves? And I'm not sure that that's that's the case. But I think for artificial analysis, we'll keep benchmarking uh raw intelligence, but we also want to think about it and explore models more deeply across other axes as well. I think hallucination's the start of that, but we're getting into wanting to support people in understanding, okay, the behavior, the person personalities of the models to help people make more nuanced uh decisions.</p>
<p>>> You're going to have a personality bench. >> Maybe [laughter] that is a direction that Chaji Open is leaning into a lot. Uh so if you manage to solve that, you should definitely talk to uh Fiji and run. Oh, okay. Um, yeah. So, what is going to be included in let's say like a V3 of the intelligence index because obviously you're going to saturate in March. >> Why don't we break it now? >> How soon does the podcast come out? >> Whenever you want. Okay. So, so we're at V3 right now. So, so the the the version that we that's going inside is is V3. V4 is what we're going to call the next,</p>
<p>you know, major update. Surprise, surprise. We're going to be adding several of the things that we've actually talked about today that we've launched over the last few weeks. So it's not that's not going to be wildly shocking, but some of the things that are most exciting is that adding GDP [snorts] is going to give us this general agentic performance in a really strong way in intelligence index. Adding critical point the um physics eval was talking about similar to frontier math that gives us completely new view with a</p>
<p><strong>[75:37]</strong></p>
<p>brand new data set of very very hard research problems. We are going to be using omniscience and we are going to be using hallucination rate. The exact ways that all of those are going to come together. Um >> the waiting is going to be hard cuz the numbers are different. Yeah, we're going to make sure that we don't do anything to cause odd distortions and stuff that could be misleading. >> But every time you version it, you have a onetime reset of the >> Exactly. you know. >> Yeah. >> Yep. That's exactly how we think about it. We will make sure that within each version number that there's no um drift in any of the scores so that people can rely on them and reference them. Um you just have to watch out for that version</p>
<p>number. Once it's v4.1, those numbers won't be compatible with v4. >> Of course, uh there there is a little bit of debate over the the accuracy of bench. I don't know if you you're clued in to what's going on. Apparently like a very high number of top chests are impossible. >> Potentially for the earlier versions T2 bench telecom we're pretty convinced is pretty good. If anything the only issue there is that models have got very good at doing it and so like anything >> top three. >> Yeah.</p>
<p>>> Yeah. >> On we go. >> Yeah. On we go. Okay. Well, thank you so much for uh providing such a great service to the industry. I'm I'm glad to at least know you guys as before you got famous and now now you are famous. [laughter] So >> Oh, look our pleasure and we really appreciate your support along the way. Like I I wasn't kidding at the start, right? That it was a um quite material moment for us like when artificial analysis was covered on later in space. >> Some random guy in San Francisco mentions you and >> No. So I I I was a fan of late in space for like a year before you mentioned us. So I'd been I'd been I'd been listening. Um I don't think I was um like familiar with um like you personally yet at that</p>
<p>point but like I was I listened to your voice probably for many many [laughter] hours and so once like you mentioned and then like got to get to know you and like meet you for the first time nearly a couple years ago like it was really cool honestly. So yeah it's great to be here >> and thanks for you know being such a great member of the community and kind of spotlighting you know projects which don't don't have attention and and bringing them to your to your audience. >> Yeah. Uh well actually so it wasn't me right uh someone in the discord dropped it in our in our discord and that's I I rely on our community and it kind of feeds itself right so uh so someone</p>
<p>brought it to my attention I don't know who we should probably go back and check but once I saw it I was like this is this looks good this is something I always wanted I I I wanted to build it I I was too shy or dumb or lazy to build it and you guys did and um not now it's a whole thing so thank you >> built some really cool other stuff like like this pod [laughter] >> yeah totally >> so thank you >> that's Great. Cool. Thank you. [music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=v5mBjeX4TJ8</guid>
      <pubDate>Fri, 09 Jan 2026 00:18:47 +0000</pubDate>
    </item>
    <item>
      <title>Brex’s AI Hail Mary — With CTO James Reggio (acquired for $5B by Capital One!)</title>
      <link>https://www.youtube.com/watch?v=BKLvySNVBtM</link>
      <description>From building internal AI labs to becoming CTO of Brex, James Reggio has helped lead one of the most disciplined AI transformations inside a real financial institution where compliance, auditability, and customer trust actually matter.
We sat down with Reggio to unpack Brex’s three-pillar AI strategy (corporate, operational, and product AI) [https://www.brex.com/journal/brex-ai-native-operations], how SOP-driven agents beat overengineered RL in ops, why Brex lets employees “build their own AI stack” instead of picking winners [https://www.conductorone.com/customers/brex/], and how a small, founder-heavy AI team is shipping production agents to 40,000+ companies. Reggio also goes deep on Brex’s multi-agent “network” architecture, evals for multi-turn systems, agentic coding’s second-order effects on codebase understanding, and why the future of finance software looks less like dashboards and more like executive assistants coordinating specialist agents behind the scenes.
We discuss:

* Brex’s three-pillar AI strategy: corporate AI for 10x employee workflows, operational AI for cost and compliance leverage, and product AI that lets customers justify Brex as part of _their_ AI strategy to the board
* Why SOP-driven agents beat overengineered RL in finance ops, and how breaking work into auditable, repeatable steps unlocked faster automation in KYC, underwriting, fraud, and disputes
* Building an internal AI platform early: LLM gateways, prompt/version management, evals, cost observability, and why platform work quietly became the force multiplier behind everything else
* Multi-agent “networks” vs single-agent tools: why Brex’s EA-style assistant coordinates specialist agents (policy, travel, reimbursements) through multi-turn conversations instead of one-shot tool calls
* The audit agent pattern: separating detection, judgment, and follow-up into different agents to reduce false negatives without overwhelming finance teams
* Centralized AI teams without resentment: how Brex avoided “AI envy” by tying work to business impact and letting anyone transfer in if they cared deeply enough
* Letting employees build their own AI stack: ChatGPT vs Claude vs Gemini, Cursor vs Windsurf, and why Brex refuses to pick winners in fast-moving tool races
* Measuring adoption without vanity metrics: why “% of code written by AI” is the wrong KPI and what second-order effects (slop, drift, code ownership) actually matter
* Evals in the real world: regression tests from ops QA, LLM-as-judge for multi-turn agents, and why integration-style evals break faster than you expect
* Teaching AI fluency at scale: the user → advocate → builder → native framework, ops-led training, spot bonuses, and avoiding fear-based adoption
* Re-interviewing the entire engineering org: using agentic coding interviews internally to force hands-on skill upgrades without formal performance scoring
* Headcount in the age of agents: why Brex grew the business without growing engineering, and why AI amplifies bad architecture as fast as good decisions
* The future of finance software: why dashboards fade, assistants take over, and agent-to-agent collaboration becomes the real UI

—
James Reggio

* X: https://x.com/jamesreggio
* LinkedIn: https://www.linkedin.com/in/jamesreggio/

Where to find Latent Space

* X: https://x.com/latentspacepod
* Substack: https://www.latent.space/

00:00:00 Introduction
00:01:24 From Mobile Engineer to CTO: The Founder's Path
00:03:00 Quitters Welcome: Building a Founder-Friendly Culture
00:05:13 The AI Team Structure: 10-Person Startup Within Brex
00:11:55 Building the Brex Agent Platform: Multi-Agent Networks
00:13:45 Tech Stack Decisions: TypeScript, Mastra, and MCP
00:24:32 Operational AI: Automating Underwriting, KYC, and Fraud
00:16:40 The Brex Assistant: Executive Assistant for Every Employee
00:40:26 Evaluation Strategy: From Simple SOPs to Multi-Turn Evals
00:37:11 Agentic Coding Adoption: Cursor, Windsurf, and the Engineering Interview
00:58:51 AI Fluency Levels: From User to Native
01:09:14 The Audit Agent Network: Finance Team Agents in Action
01:03:33 The Future of Engineering Headcount and AI Leverage</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>We have like three pillars for our AI strategy. We have our corporate AI strategy which [music] is how are we going to adopt uh and like buy AI tooling um across the business and basically every single function to be able to 10x uh our workflows. Then we have our operational AI strategy which is how are we going to buy and build [music] uh solutions that enable us to lower our cost of operations as a financial institution. And then the final pillar is the product AI pillar which is like [music] are we going to introduce new features uh that um enable Preex to [music] be a part of the</p>
<p>corporate AI pillar of our customers. It's like we want to build features and be a solution that somebody else is saying to their board, hey we we adopted Rex and this is part of our corporate AI strategy. Hey everyone, welcome to the living space podcast. This is Allesio, founder of Colonel Labs and I'm joined by Swixs, editor of Blade in Space. >> Hey, hey, hey. And we're here with Jio at Brex. Welcome. >> Hey, thank you for having me. >> Thanks for visiting uh from up in Seattle where uh I I've been a little</p>
<p>bit. It's cold up there, huh? >> Yeah. And we have an atmospheric river hitting the the the city right now. So, a lot of Yeah. Well, yeah, it's uh we're getting we're getting the full-on winter effect right now. >> Well, you're you're here. We're talking about the sort of AI transformation within Brex. There's a lot of um interesting tidbits that we're going to draw from your article but also your background. You have got a wide array of experience from stripe to um banter to convoy and uh I think also mostly I'm interested in your journey as as one of the rare people that have transitioned</p>
<p>from like a mobile engineering leader to a CTO which I think is also a bit more rare. I used to have this comment in the past where there's a career ceiling for people who work on client only things where usually they don't hit CTO whereas they typically promote the the backend people the backend clouding for people the CTO. >> Yeah. You know it's it's something that I I hear fairly fairly frequently because um there aren't that many folks with a front-end background who who reach this level leadership and it's exciting for me to be able to represent that group. I I'll say that even though</p>
<p>my resume kind of reflects that I've been more on the the front end of things, it's probably more my experience as a founder a couple times over that actually helped me get to this this level of my career working for somebody else. Becoming CTO was very much like a leadership and and like general business role as much as it is a technical role. And so I think it was more the skills that I built from starting companies and and trying to build those up made me a decent fit and enabled me to get the nod from from Pedro to take this on as my predecessor left about 2 years ago.</p>
<p>>> Yeah. One thing I'm curious you guys commentary this is a little bit broad unscheduled but a lot of startups are bragging about how many ex-founders they have and yes to some extent you want people with the founder mentality and agency which is what what you did to be your employees and to to take initiative in the company but also I wonder if it's becoming anti-signal sometimes I don't know if you've thought about this</p>
<p><strong>[03:00]</strong></p>
<p>>> I think it's more about the turn for me especially when people are hiring exfounders is like if you're truly of the founder gene. It's kind of hard to just stay somewhere. It's like an IC for too long >> and then it's like all right, I joined this thing and then in one year I'm back to being a founder. I'm curious for you. What was your I'm sure you thought about leaving and like doing another company instead. >> In fact, that was that was the the alternative. I was considering even at the time that I got the phone call where they made me the offer to become CTO, I was thinking about leaving to go start a company. And uh you know I think what's interesting about it we we actually</p>
<p>launched um sort of like a new recruiting and employee value proposition for Brex a couple months ago called quitters welcome where we actually intentionally are leaning into this idea that uh we have a disproportionate number of folks who go on to become founders or like heads of a department when they leave our company and and we celebrate that. It's actually something that uh I'm very proud of and that means that like we um we welcome in people who want to get a different experience. I think that there's certainly like uh a lot of founders who don't make it uh don't scale their own</p>
<p>businesses to this to the scale that we've achieved at Brex. So there's something to be learned when they come in. And then we're very happy to like uh support people on their way out. And so I I actually really like hiring former founders or future founders. Uh the one value proposition I find that's most relevant because a lot of the folks we're hiring as AI engineers are kind of folks that are either like winding down their companies or or are considering um maybe running AI startup. The thing that resonates the most with them is that we often times can give them problems to solve that are interesting problems that may maybe they even want to want to like</p>
<p>build their own startup around but with instant distribution right like that that is the that is the allure is it's like you can come into this business and build uh like financial AI applications and instantly have that deployed to roughly 40,000 uh customers across uh you know the Fortune 100 down to you know tens of thousands of startups. So that that's what is I think appealing the founders but but the uh the challenge then is making sure that we set them up for success in an environment that still feels a little bit like the startup that they might build themselves versus like something that's too corporate.</p>
<p>>> Yeah. Instead of doing your own company and then coming to you and be like can I integrate into Brex get all the data. >> How's the engineering team structure? >> Yeah. So we have about uh 300 people in engineering like 350 uh total across EPD and uh for the most part we structure around um our product domains and so this means that Brex is a corporate card uh it's also corporate bank account it's expense management travel and accounting and so we we actually have sort of full stack uh product domains uh that are roughly like 30 40 people for each of</p>
<p>those that um have everything from like the low-level infrastructure up to the the web and mobile experiences. That's generally like the structure of of our of our engineering um organization. And then we have uh naturally like a organization that focuses on infrastructure security um it uh and then there are two uh additional centers of excellence that we've kind of built that kind of violate that org</p>
<p><strong>[06:01]</strong></p>
<p>design uh where we've felt the need to uh to put more focus or like operate slightly differently. And AI is one of those areas where we have another team of just roughly about 10 people um who are focused prim primarily on LLM applications. And we wanted to create a bit of a separation there because the way that we were thinking about this and this is actually something we did this summer is we we paused and asked ourselves on on our AI journey towards like infusing our product uh with AI and generating customer value. We asked ourselves like what would a company that was founded today to disrupt Brex look</p>
<p>like? And then we try to basically use the answer to that question to form this team uh internally. So it's a little bit off to the side. Ideally everybody kind of comes up to speed and um contributes uh you know LLM features but uh but we have this sort of off on the side right now in a centralized manner. >> What's the difference in AI adoption for those teams? So like are the people on the LLM team like much bigger cursor users, clock users or like do you see similar diffusion? It's actually fairly fairly uniform across the entire</p>
<p>engineering uh department. It's actually kind of funny like one of our our largest cursor users is actually an engineering manager. So like and I I think that this also just like speaks to our core value of operate at all levels where we want all of our ems and everybody in leadership to still basically do the job uh that they're managing manage the work. So it actually is I I I think the journey of getting everybody into using agentic coding was not sort of exclusive to like the the AI group. >> Yeah. Um I in fact I think this podcast</p>
<p>was actually set up because I cold outreach to Pedro >> because he he tweeted this I I I assume this interex. He says I started a new company inside to build the future of aentic finance. No BS just builders building 96 and pushing production grade agents to 30,000 finance teams now 40,000. Um, and then he actually has like a little job description which I think is really interesting. Uh, but I'll skip that and go straight to Brexit accelerated to grow 5x and cut burn 99% in the past 18 months. I assume that's a mix of internal AI automation and other</p>
<p>stuff. >> Um, but we're basically I wanted to put some headline numbers up front to impress people before we dig into the details. >> Yeah, absolutely. And you're you're correct. That's the that's the team that we have this uh like AI team. You're actually what was that? >> Very young team. >> Yeah, it's very young. I mean it's and it's been really interesting the the composition of the team is like very young like AI native like 20-year-olds who basically grew up with the tech um kind of paired off with more like staff level software engineers that have been at a little while who can kind of navigate like the existing code bases</p>
<p>and like understand uh the product and the customer deeply like we've formed these really uh couple of tight tightnit pods in the AI or where it's like three people generally somebody who has like more of a product to customer focused background that like staff engineer who knows uh where the skeletons are and then like a much younger like AI native engineer who um can just do things with uh with agents that like the rest of us uh dinosaurs maybe uh don't don't uh can't either dream of or like or where</p>
<p><strong>[09:03]</strong></p>
<p>our I think I think part of it is like sometimes the too much experience or too much knowledge of how to solve a problem can actually be an impediment to thinking uh differently about it and thinking about it from like an AI first lens. But yes, we we've been we've been slowly growing that team just in the same way that like a preede startup. You want to be very very careful about talent density and like very deliberate like only hire when you absolutely need it. And so yeah, at this point it's just about 10 people and I think it was probably four or five people. Uh I think everybody was actually in the photo that was attached to that tweet uh when Pedro put that out a couple months ago.</p>
<p>>> Yeah, we'll put it up. It's a photo at 1:20 a.m. in a on a Friday. >> Yes. Oh, yeah. Yeah. cuz we we always do uh we always do like Friday Friday demos and and like that's a time for everybody to get like kind of exec review time and so uh >> everyone's in Seattle. >> Um those folks were all in Seattle. Uh but they're actually ge geographically distributed. Uh we have a couple folks here, a couple in S. Paulo, a couple in Seattle. >> How addressible we have this like AI center of excellence which are basically the people running these teams across companies. >> Yep. >> How do you make the other engineers not feel like you're not special? I think</p>
<p>that's something that I hear a lot is like, hey, you know, why aren't these people working on all the Google LM things and like I'm stuck working on, you know, the KYC integration with whatever. Yeah. You know what I mean? It's like, how do you build that culture? >> You know, it's interesting. I I thought that that would be more of a problem, but the benefit of having really optimized our engineering culture around business impact actually causes it to cut in the other direction where where folks some folks don't want to work on the AI products because doesn't have as much clear direct like business impact</p>
<p>right now. Doesn't doesn't impact revenue as directly. And so I uh I think folks for the most part uh we've we've enabled folks who have a strong desire to work on on um AI products to to join that team like somebody somebody transferred out of our expense management organization to come over there because they're really passionate about taking like their knowledge of like policy evaluation and and bringing it into the the AI uh uh team. But for the most part, I think everybody understands like how their work uh ladders up and maybe there's some like friendly rivalry because like the folks who say work on a card product, they</p>
<p>they drive 60% of our direct revenue and so they they're pretty happy with that and uh and they don't feel like they're being left out. Uh and I will also say u as you probably saw in this this piece that we we uh put out with uh first round, there is a lot of smaller applications of LLM peppered throughout all of our product and operations teams. is just some of the more novel like agentic layer that sits on top of BS that has been put together like in this in this sort of isolated team. So it's not like folks aren't getting to to</p>
<p>build with LLMs or use LLMs on a daily basis. >> Yeah. Maybe run people through the BS agent platform. We'll put the diagram in the video where you had the LLM gateway. You have like the whole MCP layer. We just had David the creator of MCP right before you. So this is very timely. >> Um yeah, how did you start building that? What's the architecture? Yeah, the architecture, you know, I think simple is uh is elegant and we we've had basically an LL gateway and and a basic canrolled platform uh from the very</p>
<p><strong>[12:04]</strong></p>
<p>early days. In fact, right before being tapped to become CTO, I was leading uh like a AI uh labs team internally uh in the wake of like the announcement of chat GPT, you know, everybody saw this through technology and said, "Hey, what are we going to do with it?" And so one of the first things that we did um I think January 2023 that would have been uh was try to put together some internal infrastructure that made it possible for us to deploy pro deploy manage version and eval prompts uh and then be able to manage uh like data egress and model routing and uh have some very basic like</p>
<p>observability and cost monitoring uh in an LLM gateway. So that's that's infrastructure that we stood up and it still continues to power a lot of those smaller uh more let's say like precise applications of LLM. So like for instance, we've uh we set up a completely automated uh pipeline for um evaluating uh customer applications to get them onboarded instantly to Brex, which is something that used to require um human intervention either for underwriting or KYC, but now we basically have a series of of agents and um and particularly like research agents</p>
<p>that will go and do the work that humans would normally do. And so that's running on top of this uh this handrolled uh framework. And then for the agents on BREs that we announced in our fall release, which is like this agentic layer that we're building that sort of sits on top of BS and can embody workflows that a finance team would normally uh hire humans for. We've actually uh started using Mastra for that as like the kind of primary >> primary framework for for accelerating us. They actually have built everything in Typescript um which is another like</p>
<p>technology choice that's uh answers the question of like what would we do if we started Brex today but isn't the case for all of our existing backend code which is either cotlin or elixir and then we have uh we have a mix of PG vector pine cone and like I think what we've seen is we're always we're always re-evaluating the tech and framework choices as we go uh because the halflife of code has declined so significantly with agent coding it's actually quite uh easy for us and for anyone else to to kind of try on for size a variety of</p>
<p>different pieces of tech to to figure out what is going to be most ergonomic for solving the problem. >> Double click on Mastro that's a new choice an interesting one. Yeah, I mean I think that the main the main reason that we adopted Mastro is that it provided the ergonomics that we were actually uh that the ergonomics of Master are quite similar to the internal um LLM framework that we built 2 and a half years ago. Uh whereas like Langchain was available at the time 2 and a half 3 years ago. uh it didn't</p>
<p>quite feel uh right to us when we were trying to um it it kind of addressed the things that weren't the the pieces that we we needed to address which was like being able to have really simple um observability and and um logging tracing >> lang chain didn't do it >> it I mean at that time it didn't I think it was really I think it was >> well they fixed that >> yeah no they certainly they certainly did but but but so like we we did I'm</p>
<p><strong>[15:06]</strong></p>
<p>trying to remember because this is now ancient history uh we evaluated link chain turned off of it, built our own thing, and then as we were looking, we kind of want to deprecate this internal framework that we built because at the end of the day, it's not leveraged for us to maintain that. Uh, and Master ended up fitting the bill for um or the the feature set that we were looking for. And I think what what's been interesting is about half of the the applications that we're we're building right now um on the the agent layer are running on Mastra and then the other half are actually still running on like</p>
<p>yet another internally developed framework which is a framework that's focused more on networks of agents. So sort of multi- aent orchestration versus more like strict uh like you know single turn or uh like workflows which are easier to use like either langraph or mastra. tell us about your multi-agent framework. I mean it's what are the design considerations? Um why why is this the first we're hearing about it? >> Yeah. Yeah. So it's funny a big big reason why we haven't written more about this is that it continues to evolve quite a bit and I I feel like we we</p>
<p>actually had a blog post that we were going to put out in conjunction with the fall release uh talking about how we built this and by the time that we finished uh you know the blog post and had all the package ready it was already like halfway outdated. And so the way that this has started to emerge is this multi- aent network approach to implementation was when we were trying to scale up our um sort of consumer grade Bs assistant. So if you think about like Brex uh and our customers uh there's really like two very broad personas that we serve. We serve members of a finance team who are generally like</p>
<p>going to be doing like in roles like accountant or controller or head of T& for those folks. uh they are uh going to be interacting with agents that are much more specific to their roles. But then the other broad cohort of of users we have are like employees of companies that have deployed Brex. So you know you go join a new company that company uses Bre you get your Brex card. And our goal for employees is for Brex to completely disappear. Like the best UI UX for Brex is just the card. like every single thing that you have to do in the</p>
<p>software beyond just swiping the card is like an opportunity for AI to uh to eliminate some work for you. And so what we thought was the right approach to solving that for that was to um was to embody like an executive assistant uh for every employee because I as an executive at Brex I have an EA and she knows enough about me. She has access to my calendar, my email, has all the context on uh when I'm traveling and for what business purposes. And so she's basically able to do everything that I would be obligated to do in Brex, be it</p>
<p>like booking travel or like doing expense documentation. And so what we wanted to do is we wanted to build like that EA connected to the same data sources and see if we couldn't simulate that behavior so that um you know you basically your interface to Bra's uh SMS in the card. And when we started building that out, uh you know, the most naive like architecture for that would be to have um an agent with a variety of tools and maybe maybe do some some rag</p>
<p><strong>[18:07]</strong></p>
<p>to ensure that it has like appropriate context for the conversation. But what we were finding is that um the wide range of different product lines that exist on Brex made it difficult for one uh like agent to perform well u being responsible from everything from like expense management to finding and booking travel to answering policy and procurement questions. And so that's when we started breaking down the problem uh and into into a variety of sub agents that sit behind an orchestrator. And obviously this isn't something that can be implemented using langraph or master even has the notion</p>
<p>of these as like network switches and beta. But what we found is that it was easier for us um when it came to being able to build evals for the system. uh we we kind of just hit the eject button and built our own framework which is one in which um we have agents that are able to uh to basically DM with other agents and have multi-turn conversations amongst themselves to coordinate to um to complete a task to uh or like to complete an objective. And um what's what's been nice about that is it means that like you can have your Brex</p>
<p>assistant there's like one single one single like point of contact uh between you as an employee and the Brex product. Then behind your assistant um if the company has like expense management turned on you have that. If they have reimbursements there's another agent for that. if they have travel attached to the red agent for that. It actually also then facilitates like our conception here is that um you know it's like generally like software encapsulation patterns taking like sort of projected into the agent space. It also makes it easier for us to have like the team that owns and understands travel like be the ones to go and iterate on that without</p>
<p>needing to worry about like redressing the total system um or needing like one team to own every single possible um act action you could take as an employee. And I'll say that like I'm still of the mindset that somebody will build a great framework and we they ultimately migrate to it but or it might be us that we ultimately open source this right like but um but for us like this is uh this has worked out quite well in like l of like a couple other approaches that we we tried along the way that just didn't perform well which was to you know overload the the the agent with a</p>
<p>variety of tools or contextual like context switching where we try to say oh this conversation looks like it's more about reimbursement so let's like update the prompt with more reimbursement context like that was that was another approach that we took that didn't perform as well is actually having a reimbursement agent that it would collaborate with. >> What about MCPS as like sub agents? Oh yeah, that's another pattern. >> The key thing there is that we there's actually a lot of value in having like multi-turn um uh conversations from like the orchestrator or the assistant to</p>
<p>like the sub agent whereas like you know a tool call is basically just like one RPC. And so oftentimes what will happen is um you know let's say let's say the the the user reaches out to their Rex assistant and says hey like am I allowed like how much am I allowed to expense per person for dinner tonight? I'm taking my team out. Uh and the the uh you know your assistant's going to then reach out to the policy agent. Maybe the</p>
<p><strong>[21:08]</strong></p>
<p>policy agent needs to know in order to answer that question. Maybe it needs to know like whether this was um uh was like a customer event, a team event or whether you're traveling. And so it may actually send instead of it can't just answer the question. So it's going to reply back to the the assistant and say hey I need you to ask uh this clarifying question. And so then the assistant will return to the user ask clarifying question and then they'll basically have this sort of multi multi-turn conversation across multiple agents versus it just being encapsulated in like a single uh call and response tool call. And so there are still like all</p>
<p>the all the sub aents have a ton of tools, but I I think of like the MCP and and tool usage as being like the interface to all of our conventional uh imperative systems, not not the the AI space. >> Yeah, that's the conversation we were having earlier whether or not it should be an agent to agent protocol as well or like >> Yeah, there should be like a chat back. >> Exactly. Exactly. And that's the thing is like Okay. And one of the ways that we actually grafted this into Astro before we we built our own framework was to was to make every sub agent a tool. Uh and then the input was just natural</p>
<p>language, the output was natural language and the if you needed to have multi multi-turn um you would basically just put the full like conversation and as you kept calling calling the sub agent as a tool and it's just like at that point you're like okay the ergonomics are kind of the framework framework is fighting me on this. It's actually helpful for us to basically conceive of it as an org chart and like it's the agent or chart with uh with um you know my EA is DMing other specialists uh and having brief conversations to support me as their</p>
<p>client. >> Yep. >> That was a really good uh deep dive. Thanks for indulging. I feel like you guys are not afraid to make your own tech which I think is a competitive advantage. I really like that culture. Maybe I we should go a bit breath first as well. Of course, I think we also deep dive a little bit too much in in one area. There's um and we'll we'll put up the chart, but I'm also very interested in like the sort of internal agent stuff, the operational stuff and just the general platform scope. So, please feel free to just like go into your</p>
<p>spiel on it. >> Yeah, of course. So, one of the things that I was trying to do at the beginning of the year, uh as CTO, you know, I think it really fell to me to articulate what our AI strategy was as a business. you know, every every board of director was, you know, or every me every member of our board was like, "Hey, what's your AI strategy?" And while we were doing a lot of duties, we literally go, "He's got it." >> Well, yeah. [laughter] Uh Yeah. And and if I didn't, I uh I'd be in trouble. I think he also was counting on me, given that I was doing the AI uh organization before CTO to to have That's true. But but a big part of</p>
<p>it was like we we were doing a lot with with LLMs. um it was more like these little one-off features and you know, hey, like maybe mix in some suggestions here or maybe do a little bit of ops automation over here. But it wasn't um it wasn't easy to to kind of create like a verbal framework uh um of all of these investments and without that framework then we weren't able to like set a set a</p>
<p><strong>[24:08]</strong></p>
<p>vision or a road map for for investments. What we did at the beginning of the year is we took everything that was going on as well as all of our ambitions, all of the good ideas, as well as like the problems we were trying to tackle as a business this year, throw it all on the table and see if there were some ways to cluster it into a framework that made sense to the business, to our board, uh to ourselves. And we came up with, I think this is not particularly novel, but has helped us quite a bit. We have like three pillars for our AI strategy. We have our corporate AI strategy which is how are we going to adopt and like buy AI tooling um across the business in</p>
<p>basically every single function to be able to 10x uh our workflows. Then we have our operational AI strategy which is how are we going to buy and build uh solutions that enable us to lower our cost of operations as a financial institution because I think it it's fairly intuitive like financial institutions like ours face a lot of regulatory expectations and there's just like a high ops burden for running our business and so it's sort of like a lot of kind of internal use cases like being able to do like fraud detection</p>
<p>underwriting KYC um be able to handle dispute automation on car transactions. Those those types of operational investments or our ops AI pillar. And then the final pillar is the product AI pillar which is like are we going to introduce new features uh that um enable Pre to be a part of the corporate AI pillar of our customers. It's like we want to build features and be a solution that somebody else is saying to their board, hey, we we adopted Rex and this is a part of our corporate AI strategy. Yeah. And so it's it's kind of has this nice little feedback loop and we we</p>
<p>basically within the company split uh you know did a little bit of divide and conquer where uh folks in um IT and on our people team were more or less uh spending more of the effort driving on corporate AI really like looking for um making the procurement decisions like creating a culture of experimentation where we spotlight and incentivize people for trying uh to sort of improve their personal workflows using AI. And then the the pieces that I've been more involved in have been operational and product and we were just talking about products here which is like the agents</p>
<p>on Brex and stuff but I think that the operational AI investments have been some of the the most sort of immediately impactful uh to the business because we have hundreds of people who work in our operations organization and it's actually something that differentiates us because our uh seesat and the quality of our our support and service is very very high uh something we're very proud of and so trying to figure out how can automate significant portion of this and uh and use LLMs in a way that doesn't degrade the customer experience and then</p>
<p>also kind of addresses like what is the future of the roles of the people who we already have working full-time for us. So this is where um uh Camila uh our COO who kind of co-wrote the the piece uh with First Round uh with me uh she's been leaning really aggressively to help every member of the operations organization um start rethinking their role as being uh not people who kind of</p>
<p><strong>[27:09]</strong></p>
<p>execute against an SOP but are people who are going to like build prompts, build evals and like be become more AI native and like the way that they do work. And so a lot of the engineering we've done has been to enable folks say in um in fraud and risk to be able to uh to refine prompts and uh and add additional automation to their workflows. >> Yeah. And this secret force pillar, the the platform. >> Yeah. Yeah. Exactly. That is the that is the thing that ties it all together. Exactly. Is is the is the platform. And I think what's been really nice is that um even though the platform is kind of a</p>
<p>loose uh loose loose term because it consists of a wide variety of technologies as I said like we haven't been too religious or dogmatic about everybody needing to be on one particular thing. What we've seen is that um by making a variety of sort of ergonomic uh options for building with loss available, it like really has made it easier for for us to make a quick leap forward on operational AI. Like we as soon as we put our mind to it and we said like look no we want to hit 80% automated acceptance rate for all um all startup and commercial businesses that</p>
<p>apply for Brex like we want a decision within 60 seconds that's fully touchless no humans involved. We were able to break that down and then actually build the uh build the agents build the tools um on top of that platform really quickly and and a lot of those tools are the same tools that our product AI uh agents use as well. >> I was pretty sold on the conductor. I don't know if this is under exactly the bucket, the conductor one >> uh provisioning command. I was like, "Yep, I want that." >> Yeah, that was actually I'd love to talk about that. So, that's that's actually on the corporate side. And I think that this goes back to maybe another</p>
<p>intuitive but but I'd say like bold decision that we made, which is that we're not going to we're not going to try to pick winners in the horse race between the foundational model providers or the the agent coding tools or like basically anywhere where there's there's an active horse race. What we do in instead of like trying to pick a single solution is we will procure like a a small number of seats like multiple solutions and then we'll give employees the ability to pick whatever one they want to use. And so for instance like we</p>
<p>allow employees to basically go to in Slack and use conductor one to uh get a chat GPT a claude or a Gemini license and basically you can just like build your own stack where you pick your um you pick your like chat chat provider uh as a as a dev you can pick um you know between like cursor wind surf cloud code credits like and and you can basically craft your your stack to your preference and easily switch between them. And what that does for us too is when we're going to like obviously we have sort of enterprise agreements in place for all of them for the sake of like the you</p>
<p>know the the privacy and non-training guarantees. But it's fun because when we go to renew these contracts um it it we can basically resist the need to like do a wall-to-wall deployment. We can say hey look like usage trends it our our employees are voting with their fee. They're voting with their dollars and you know maybe uh maybe your tool isn't as uh as hot as it was a year ago. Does it give you a dashboard of what people are choosing? >> Yeah, actually we look at that. Um we</p>
<p><strong>[30:10]</strong></p>
<p>were looking at that as we're going into budgeting over next year. Very interesting. >> I would love to see that those what what's you know anything that's like really up anything that's really down. >> It's fascinating how how different the landscape is every every three three months. And I think one of the one of the interesting challenges we had early on was uh getting folks to just like try um these tools, try to incorporate like a coding. Yeah. I like early on I say like 12 to 18 months ago now like get folks to to just take the time to try a new workflow. And now at this point I think</p>
<p>what we're seeing is like even if you know uh a new model hits the same like um when when Codeex uh came out and everybody's like oh Codeex is is better at at uh codegen but it's a little bit slower. like I find fewer folks are like kicking the tires on new things because like the they're just so comfortable with the ergonomics of their current workflow that that um you know uh some folks are just like I want to just stick with Cloud Code cuz I know it now. I've been working with it for like 9 months. So I don't need to to keep uh keep switching. I don't need I don't feel the</p>
<p>incessant need to keep trying new things because I've I've gotten I'm an iPhone person and I'm just like going to stay with an iPhone even, you know, even though there's some really sexy Android hardware out there. Do you have one of the big numbers like 80% of all of our code is written by AI or but how do you measure it internally? >> Yeah. No, not really. We we I mean I what we do is we'll we'll measure like the attributions on the the number of commits that that have the like co- co-authored with um and we pull some of those stats but I don't index heav like in fact I don't index on those at all. I</p>
<p>don't and honestly like I >> I don't know how I in honest like honestly calculate that number. I agree. >> Yeah. And so so I the thing that the thing that we're we're really just you know we're at the point now with like our AI agentic coding journey where now we're trying to solve the second order effects of like a little bit too much slop maybe a little uh not enough um yeah exactly not enough like rigor and code reviews. Um we're trying to the adoption is there and now we have to</p>
<p>figure out like how to mature in our usage of these tools uh so that we you know quality or like long-term maintainability doesn't suffer as well as like maybe one of the other fac facets of being able to generate a lot more code more quickly is like the the drift between team members as far as like understanding of the the code that's in their services increases is like everybody's moving faster and more independently. Uh it that is another sort of risk that we're starting to see like you know an incident response where</p>
<p>folks don't know they don't know a service as well as they they used to because it's changed so much in the past couple months because everybody's moving more quickly. >> Yeah, this has been a major topic for me this year on codebased understanding and slop because obviously it's so much easier to generate code but then now we have to review it and to some extent you can't really fight AI with more AI. You can't just be like oh just throw throw an AI reviewer on their AI code and you solved it. Uh and so so you do need to</p>
<p><strong>[33:12]</strong></p>
<p>just scale human attention. And I think that's something I've been pushing a little bit in terms of like well you're you're just going to like every engineer is just going to own more code >> period and and be parachuted in and be expected to ramp up and be be productive and also fix bugs and if you're on you know pager duty or whatever to just because I mean everyone's going to trying to be more efficient and you're supposed to see ROI productivity because if you don't then what's the whole point of this? >> Exactly. Exactly. And I and I think it's funny going back to the point of, you know, you could you could add AI on top</p>
<p>to solve the problems that AI introduces and but you just keep you that's like an endless chain. Uh and so >> well no I mean that the the the code rabbits of the world, the the graphites of the world would say yes actually you can >> and so that's the little bit of the the tension there. >> Yeah. And you know, I I uh I've been thinking a lot about how the craft of of engineering is evolving and and I will say that I feel further away from being able to predict what what it looks like than I I did this past summer when I spent a bunch of time. Um I actually basically went on leave for a month and</p>
<p>joined the um join the the the team that uh the the AI team that we were building just to go and build alongside them. I felt like it was really important for me to deeply understand uh the problems in the tech but and so that was me. I was I was you know writing pushing code um effectively 996 and uh and I I went through so many different moments of realization of like oh my god this is going to change everything to oh my god this is just amplifying all the good and the bad in the industry uh to oh my god engineers are not going to have a job</p>
<p>anymore to you know it's like and so I I don't have any pred like I felt like I had all the predictions back then and at this point now I'm just very interested to watch the the phenomenon continue to unfold in front of us and uh I will say I was chatting with a bunch of really bright uh you know college juniors and seniors at a dinner we hosted last night and um well these folks are about to enter the industry basically having kind of come up in the the era of agentic development and LLM and I asked them like so what is your workflow when you're like building uh like building a</p>
<p>project uh how do you how do you use agents versus like when do you decide you're going to actually just write code by hand and I was surprised to hear the consensus was that most people there were using agents to collaborate on like building a design document like collaborating on the architecture of the solution that they want to build and then maybe asking it to like emit you know a doc or an implementation plan but then they'll go and write a lot of the code themselves still. Uh so it's a little bit more of the the uh the rubber duck co-architect uh uh use case that</p>
<p>was most prevalent in that group. I I was very surprised by that. >> I'm impressed the kid the kids are all right. Yeah, I know. Then they still want to they still want to actually write the code themselves. It's interesting. >> Yeah. What we hear from like the Gen Z's at open, they they just yolo everything into code and [laughter] >> yeah, I would say most of the code I generate is like Yeah. But but I spend a lot of time on the dock. It's curious</p>
<p><strong>[36:13]</strong></p>
<p>like when you're like younger in your careers like you don't really have all the mental models of the different patterns to instruct. So I feel like there's like over reliance especially if you're doing the design doc you know. I I feel like most of the senior engineers will spend more time on that. It's like even things like you know what columns should you index >> depending on you know what queries we usually run on this table and things like that. It's hard for any AI to know that right >> you know and it's like I feel like the the role of like the more senior engineer should actually be more of this. It's like spending time teaching</p>
<p>the AI and then the AI can teach the junior people in a way. >> Yeah. Yeah. And it it everything everything looks like mentorship and management [laughter] >> at the end of the day, right? It's like you're you're breaking down tasks, you're uh you're supervising work, you're giving feedback. Like it's, you know, it's basically management >> except that there's agents are really bad at memory still. Like they basically have zero memory and and it's it's in Seattle 2025. What's going on? >> Yeah. [laughter] >> Yeah. What's your internal stack for like a uh preferences? There's like kind</p>
<p>of like you know explicit preference you can use with uh you know agents MD and all that stuff. Uh there's implicit preference with lint rules and things like that in a way where it's like it just happens you don't have to tell it. How do you structure that? >> Oh are you talking about for aentic coding or memory like platform? >> Yeah. Yeah. For like the coding specifically it's like and then we can kind of talk about you know the whole bre platform. >> Yeah. Just just nothing nothing special just a lot of um explicit rules >> MD files. >> Yeah. And then we have uh and we um</p>
<p>linting. We still have like traditional llinters in place for the couple of different language tool chains. And then we're we're big fans of creptile and we use them for basically all of sort of the um smarter than linting uh like a code review. Uh that's been the one solution that we've aligned around that has served us extremely well. >> Yeah. Reportile. >> Yeah. No, we're we're huge fans. They're they've built something really impressive. And I think the thing that constantly blows my mind about it is um the way that they're able to just have a really impressive signal to to noise</p>
<p>ratio. Like the the comments that it leaves are very very high signal. Uh like never I never regret going through all like 65 comments it leaves on my on my diffs because it catches so many things. >> Yeah, I found the codeex review to be really good. I don't use codeex for code generation but like the review product Yeah. is like very good for some reason. Um, I used to have when I was working in Rails, there was like this project called danger systems. It was kind of like a semantic llinter. >> Exactly. >> I feel like there should be more of that now. It's kind of like the rules are one</p>
<p>thing a generation, but I want something in my CI that is like enforce these rules and call out where they're broken and then I can just copy paste that uh in an agent. But >> yeah, when we when we started building this this new agent um codebase like cuz as we was saying like we were answering the question, what would you do if you built you know a Brex disruptor today? And it's like it wouldn't be to pick Cotlin and Elixir as the back end. And uh and so we actually went with the full</p>
<p><strong>[39:14]</strong></p>
<p>like TypeScript stack and and we we were building on all like public interfaces and um really trying to make sure that this agent layer was uh like arms length from from the the good and the bad of of the core of our product. And um and one thing I think what we did early on and I don't actually know if this is true because again the team keeps sort of iterating uh but we we're having good uh good luck using um cloud code like in a GitHub action to basically go and do uh do more of that dangerous style like code review. So have a a prompt for it that went through all of the different</p>
<p>facets that were more conceptual versus like rigidly enforceable by a llinter uh and have it leave a big comment at the end with u your conformance to the ediomatic coding patterns of the of the new repo. I wanted to spend some time. You said you wanted to dive on and operational agents uh customer support, onboarding, KYC, fraud, delinquent account disputes. This is I imagine the bulk of it of of the work. >> Anywhere where there's a good story about maybe um when you started out it was going to be this way and then you</p>
<p>discovered through building or through customer contacts that it had to go a different direction. And so that difference in beliefs is something that people can learn from. The thing that immediately comes to mind is that we uh we believed at the beginning that using RL for credit decisions would actually be a like would be the way that we would end up or like credit and underwriting like how much of a of a limit should we give to this business? Um that reinforcement learning would be the way that we would go about um building a</p>
<p>model that effectively would decision in the way that um a human underwriter would. And it turns out that it was we made this big investment. We were working with some outside uh like a company uh that specializes in this and the performance we ended up getting was inferior to just building a like a web research agent. And so so I think what what we took away what what has been most evident in operational AI is that in operations you need to be able to</p>
<p>break down problems really granularly and be able to form SOPs that humans can repeatably follow and thus can be audited. uh because so much of the the responsibilities in operations is to uh is to have auditable repeatable processes that help to ensure that we're operating in a compliant manner and that actually translates just so cleanly to LLM that we haven't needed to use too many sophisticated techniques in in operational AI. uh it's been it's been</p>
<p>relatively simple like new tool uh like agents or maybe even a lot of problems can be solved with just like a single turn uh chat completion and so the fact that we didn't when we did one one sort of attempt to overengineer and use more sophisticated techniques uh and we we we discovered uh that in fact the solutions are a bit more uh more plain and and less technically sophisticated. The</p>
<p><strong>[42:14]</strong></p>
<p>challenge is really articulating and refining prompts to reflect reflect the execution of the SOP and like reflect all the sort of institutional knowledge that isn't written down uh so that agents can properly replace like the the humans or the contractors we would have making these decisions. How do you decide what is worth like spending a lot of time building versus what you think some of these models are just because some of these tasks are so generic. They're not really about bre. Yep. like you can assume the models will be good at it versus some of them are like very specific to you. We kind of prioritize like the the tasks that are most common</p>
<p>for the broadest number of customers. And the uh some of them are are are fairly um fairly intuitive like being able to research uh a customer to look to assess like legitimacy of the business and whether that business would fit our ideal customer profile for for onboarding because there's certain types of businesses that we either legally cannot serve or we are not comfortable being able to serve. So that's the type of really kind of basic research um and</p>
<p>like a a relatively straightforward problem uh that isn't hyperre specific. The things that are a little bit more specific who to us or or companies in our sector would be preparing documentation for a network card dispute. Like if if you go and dispute a transaction on on your your personal card, you will provide evidence to your card issuer. or the card issuer then has to put together like a three or four page word document that goes to uh the card network and then eventually goes to</p>
<p>the acquiring bank and and all of that is like much more specific to our business. It's a huge operational overhead for us and that's something that we we decided to automate later because it's not as uh it's not on the critical path of like serving the vast number of our customers like disputes are expensive but not very common operational process and so they're lower on the stack and uh I think we're we're getting there right now but this year has basically been us just kind of like looking at every single process just kind of stack ranking and um I will say</p>
<p>like the thing that got us started down this path um was we wanted to expand our ideal customer profile to support more business like a wider variety of commercial businesses which tend to be businesses that aren't growing as quickly. Uh so they're not like tech startups which have a lot of growth and they're not usually like they're not enterprises which also tend to have a lot of growth. It's more like a lawyer's uh a law firm or a dentist office. U these types of like solid businesses that we should be able to serve and underwrite, but the cost to to onboard</p>
<p>them and the cost to serve if you have all uh all the humans in the loop make them ROI negative. And so that was the first sort of use case of of AI within our ops uh ops organization that then led to us really understanding we could automate much more than that. >> Is this BS going back into SMBs? >> Ah that's a good question. Yeah. Yeah. So, never never let let that die. Uh, you know, know we um I think</p>
<p><strong>[45:15]</strong></p>
<p>the way we've thought about this is we want to always like offer our product to customers where we believe we have a like a an offering that is well suited to the to the needs of those businesses. And I I would say that still for very small uh businesses, our offering isn't it's not built for that. It's built for it's built for companies that have some degree of scale, typically have at least sort of one person if not a couple people in the their finance team. And so we consider these to be more like the the commercial segment. And so it rhymes</p>
<p>with with SB, but our approach back then was uh was a little bit more naive. And I would say we also we were just going for a volumes like a volume game there. Uh our our internal controls were not as strong. we didn't have as much experience like underwriting those businesses. And so it it was really ended up being a huge uh burden for the business uh almost existential uh for us to have those tens of thousands of customers that all were uh ROI negative.</p>
<p>Uh so we're trying to basically scale to serve more businesses outside of tech and outside of like the OPM market segment but um but do it thoughtfully. So, I think right now our um our minimum threshold is is uh like a million dollars a year in recur in in annual revenue or um or like $10,000 or more per month in in card transactions is kind of being like the low end of our ICP, which is obviously not what you would think when you think of a small business. Like small businesses tend to still be smaller than that. >> Oh, wow. That's really small. Okay.</p>
<p>>> Yeah. >> Yeah. Mid-market. >> Yeah. Exactly. And it's funny. It's just like the the the names of these segments. Um, you know, it's like it I don't know. >> Yeah. No, I think I like that's that's like Yeah. It's like lower midmarket and it's funny though because when what we call enterprise may be another, you know, what sales what we call enterprise is a business that Salesforce might call a mid-market, right? Like it's just it depends on the scale of yourself as a business when you use these terms. >> And all of these things are built in the Bra agent platform like all these automations that people build.</p>
<p>>> Yes. Exactly. Yeah. Yeah. And in fact the um most of the operational AI is running on that original platform that we have and we we built it. One element of it that I didn't mention is that it um it also most of the UI UX for this platform is uh built in retool. And so like you you can basically go into retool and uh there's like a a prompt manager, a tool manager, an email manager. Um and that's sort of where much of this was built. And the goal with that was again to make it more accessible, more ergonomic to to get started. But what a secondary effect of</p>
<p>having a more like visual set of tools for this is it's enabled members of the apps organization to go and do prompt refinement themselves. So you don't need engineers to go and and refine the prompts or um or even like uh test new foundational models when they come out. I think that that's another fun thing when like a new uh when a new model drops uh folks will go into the the platform and basically run the evals on the new um on the new model and kind of</p>
<p><strong>[48:15]</strong></p>
<p>see like can we get better performance here or does this have different uh different latency or different uh like cost characteristics. >> Yeah, you want the domain experts or the people directly using the tool not the engineers who are sort of somewhat removed from the tool. Y >> uh yeah I I I I do want to highlight to listeners that a lot of the BERS agent platform are just things that every company should have basically uh pro management system which we talked about where the domain experts are doing it multimodel testing evaluation and benchmarking frameworks API integrations for automated workflows MCP based architecture shared with Brexit's external AI products this one is</p>
<p>obviously very Brex specific uh one thing I did want to highlight that I was semi-impressed by because nobody people very few rarely talk about this is knowledge case for understanding Brexit's business. >> Yeah. >> So, do you want to expand on that? >> Yeah. And this is an area where we've only scratched the surface here, but but a big a big challenge that that we face is that the world knowledge or the knowledge that's built into the model about uh about, you know, what GPT 5 thinks BS does and how it thinks our business operates is actually quite</p>
<p>different from what our business offers today or how our product works. And so we've had to to work on building a corpus of sort of product documentation, process documentation and like curate this set of information to basically ground a variety of our LLM applications including like that Brex assistant which is like the you know the assistant that employees uh will will talk to. It's like we don't want it to to hallucinate features that we don't have or like give give wrong information there. And similarly like uh some of the</p>
<p>operational um uh agents need to be grounded on um like what our ICP is because if you ask uh you know CHP5 right now like what types of businesses does BRS uh on board or like what types of businesses does BR serve it might not give an accurate uh explanation to that to that question. It might it might say hey we're a corporate car for startups which is what we did you know seven years ago. It might say we're only we only serve enterprises. And so that has been an interesting challenge. I think we're what we've been trying to do there</p>
<p>is I'm actually going to be spending time with with folks uh talking about this next week internally about like can we refresh our strategy and kind of unify it because we have a lot of product documentation that's internal for like our operations and go to market teams. We have a bunch of product documentation that's external for our customers. We have a lot of uh go to market uh sort of enablement material that's more uh sales pitchy and um we have documentation that is put into Sierra which is the you know the the chat assistant that we use for um uh for</p>
<p>frontline um support like all of this ideally could draw from the same source but right now it's uh right now it's a little bit fragmented. It's just something that we're trying to invest in though because I think at the end of the day the duplication of of efforts is just like is is is wasteful and it's absolutely necessary to to get this right. >> Uh just to dduplicate uh Sierra meaning</p>
<p><strong>[51:15]</strong></p>
<p>the Brett Taylor startup. >> Yes, exactly. >> Yeah. [clears throat] >> I would expect that you have you built so many other agents that's that's one you can build yourself. >> That's like solving problems that are not differentiated enough. uh for us. I think what what's interesting about the the Sierra that has been really helpful is that again it's really easy for like the UI and UX of basically administering a Sierra agent is something that's really accessible for the ops uh and CX strategy team which are like it's much more low code and uh more sort of workflow and DAG oriented and we have</p>
<p>engineers kind of going and giving it tools to to take actions but for the most part like it's nice to not have to build build the UX for somebody to manage something like that. And I think the fact that Sierra speaks the language of of customers. Yeah, exactly. Speaks the language of CX that can do all the reporting and the telemetry and stuff that that um our you know VP of CX uh would like to see just you know it's just one fewer thing that we have to build. >> What about um evals? How do you build evals? Who manages them? >> Well, it depends on uh it depends on the</p>
<p>application. So on the on the operational AI side, um those eval are are basically baked into the into the platform around every um every prompt or every agent. And for the most part, I think most of these use cases kind of come online like the V1 of like our our um commercial underwriting agent or the V1 of our our startup KYC agent are co-developed between like a subject matter expert in Knops and like an engineer and they're going to kind of co-develop um an initial eval set. But then from there generally in ops you're</p>
<p>always doing QA be it like on humans or on uh on on the LLM uh decisions. And so whenever like as part of our QA feedback loop whenever there is uh a mistake that's usually almost always going to result in like another eval being written as like a regression test. Uh so all of that within ops AI is pretty pretty straightforwardly managed. on the product AI side, that's where it starts getting a little bit more challenging because the multi- multi- aent network um is quite challenging to evaluate. And</p>
<p>so what we do there is we try to adopt some of the state-of-the-art for multi-turn evals where we will um we'll basically have an agent embody the user and like you know have basically the the end user agent is given an objective and then we basically have it run a multi-turn um uh conversation and then use L misjudge at the end to do all of the different uh asset assessment. The one other thing that we do technique-wise that is interesting is sometimes you want to you don't want to</p>
<p>do like you know I think these multi-turn eval are kind of like integration tests they um they sometimes test more than than you what you want to to assess and so sometimes what we'll do is we'll also pre-an like an initial preamble to a conversation or maybe a couple turns will be handwritten and we'll basically set set the the um eval to start and we'll see if uh we're able to like isolate certain um certain</p>
<p><strong>[54:17]</strong></p>
<p>behaviors. So, uh it's it's still like a work in progress. And I say like at the end of the day a lot of the just um periodic human review and and like looking at um at cases where uh we've detected as we go to like summarize um like what we'll do is we'll reflect on a conversation after a certain amount of time has has passed where we'll summarize it like extract facets like did it seem like the user accomplished their their objective and we'll just manually when uh a lot of the cases when that's that's failed and decide to write another email for it. Mhm. Are all the</p>
<p>eval supposed to pass or do you have a set of evals that are like someday the model will be good enough and like how could it change over time? >> Yeah, it's interesting. Um I don't know if we have any that that are like oh someday I hope it'll be good enough to do this but it's like there there are the evals that are are blocking because they would indicate like a a regression an unacceptable regression. So these tend to be just accuracy related um evals, but then there are others that are more about like tone and uh coherency and these types of things</p>
<p>where they're they're more subjective and we we're just looking at those over time as a as a metric. >> Uh but the the team is actually interesting. I think we're going to get a big update on like how the team is thinking about eval uh our Friday review. So it's this is an area where I'd say the largest challenge like the the largest change we needed to make and how we were executing sort of as like a lab or an incubator back uh earlier this year to like where we are now where we've we've shipped and like</p>
<p>we're trying to to increase the rigor has been around uh like avoiding regressions and having more and more increasingly robust eval. Yeah, I work with a company called VI that does user simulations and I think like that's what's been interesting. Some of these things they just don't expect like the customer does not expect the model to do >> but they want to track the saturation of the model in a way if that makes sense. And I feel like most companies know what they don't want to happen but it's almost like they don't they cannot quite articulate oh I want in the future the</p>
<p>model to be able to do this. They can do it today but I I'll keep running this eval. That's actually really really interesting to me and I I'm going to take that away and and and start thinking about this because there are there are going to be certain I mean we already seen this where where uh users will ask the assistant for help with things that we don't support yet or we haven't implemented yet. It's like those are opportunities actually for us to build a like effectively write a test that's going to be fail like failing for for weeks or months and and eventually will go green, but is a way for us to</p>
<p>actually kind of show like the progression of sophistication of the assistant. I I really I really like that as an idea. >> Yeah. I wonder how you also catch hallucinations and of things that it doesn't have. That's usually the That's usually the problem is it it'll it'll it'll pretend like it can assist with something and it'll uh like one thing that is really annoying that has been tough to um to prevent is that the the assistant because it is used to speaking to other agents um that can support it</p>
<p><strong>[57:19]</strong></p>
<p>in like accomplishing various tasks. If you ask it to to help with a task that it thinks it probably should have an agent to uh uh to to work with, it'll just hallucinate that it, you know, it's like, oh yes, I'll I will like, you know, I'll reach out to the finance team on your behalf to uh to pass this question along, but it's not doing anything. There's like no finance team. There's no way for it to do that. This is something that comes up a lot. It's like, would you like me to ask the finance team? And there's no there's no actual tool guards for that. >> Yeah. Yeah. That that was something that we had to >> like a reax like >> Oh, no. But we don't I think we've been</p>
<p>a we've been able to just beat that up at system with a system prompt. But uh but the we don't have as many guard rails in place right now just around a couple of like potential uh like things that could get us into trouble. >> Really? Yeah. I just Yeah. It's surprising when I I guess two years ago was first kicking around the idea of all these things. I would have said that probably guardrails would be more prevalent, especially in finance use cases, but surprisingly they're not. >> Yeah. And that was actually part of what</p>
<p>we that was like a feature I believe we built in the L L1 gateway early on is like the the sort of last chance like uh um hard. >> Yeah, exactly. Here's some redes. Yeah, exactly. here just, you know, in the way that like if you go way a field on uh chatbt, you just get like the inline 500 error. It doesn't even tell you that it can't help. It just like craps out. Uh like I we kind of built a couple of those circuit breakers or like the ability to put those circuit breakers in and and I don't I don't believe we're using them for anything. >> One last thing I want to get your thoughts on was AI fluency levels.</p>
<p>>> Mhm. >> Which you guys have a framework of user advocate builder native and everyone goes through it including Camila. >> Mhm. >> And I just think it's interesting. I think it's a model that other people are thinking about adopting, but they're worried about rolling it out >> that everybody's going to be bad and then I >> Well, and also like how do you have like this in-house training course that you keep up to date? Just tell us more about it. >> Yeah, so in in the operations or uh they're actually more ahead of even</p>
<p>engineering on this front as far as like trying to create um create like learning pathways for this. Uh and I think that part of the reason why they're ahead of us is that in operations they're much more uh they have to be able to operate training at at scale. like training is a big very big part of um of how people build aptitude around their their job function within ops whereas like in EPD a lot of it is sort of uh getting hands-on building experience like going along getting mentored getting code review but uh it's been really neat</p>
<p>because I think we've really like we created an environment we managed to by by speaking openly uh about the the transformation that we saw would happen in this industry towards AI sort of displacing a lot of um a lot of the operations and CX roles and we were just honest about it and I think what what in the same breath that we said hey a lot of these job responsibilities will go</p>
<p><strong>[60:20]</strong></p>
<p>away we also said we don't anticipate that meaning that your job has to go away it's just that your job has to change and so the the fluency framework and then the the like the training and support and like the positive sort of culture where we celebrate people making progress has been really helpful for like avoiding a culture of fear or like oh you have to do this or you have you're going to get this is going to go in your performance evaluation. I think the >> it does >> it well it's not like as wrote as like oh like what is you know what is the like how much are you using AI and is it</p>
<p>is it enough it's it's more I think we've built a pretty like positively frame culture where we'll do like spot bonuses for for people who have like particularly novel uses uh of AI on in their day-to-day um in our company all hands every two weeks we'll do an AI spotlight and it's very rarely somebody in EPD for the most part it's folks in PTMs ops uh finance the people organiz ganization showing off like how they're building agents, you know, in chatbt or on glean or how they're they like just found some new use case that they thought was helpful. So, we're trying to</p>
<p>create create like um I think at the end of the day like we've hired a bunch of really smart people who like I have full confidence that that that this type of work is in within the reach of anybody who's motivated to like sort of challenge themselves. And so, we've we've done that. Then in engineering, there's one other thing that I want to call out because I think that this is kind of fun is that we adapted our interview loop to be more AI sort of aentic coding native. So instead of um we had uh like a coding and a system design question that we basically have revamped into um a project where we'll</p>
<p>give you like a brief before you come on site and then like an additional sort of spec when you do um when you start. You know, we expect you to use agentic coding to complete the the task. In fact, it's like kind of impossible to get all the way through it if you don't. And so, we're evaluating, you know, your knowledge. Like we're kind of watching how you work. We're evaluating whether you understand the code that's coming out. We we, you know, we're kind of probing at you as you go. But what we did in order to kind of maybe bootstrap the process of all of our existing engineers, like getting familiar with Agenta coding is that we as soon as we</p>
<p>had the interview um ready to ship, we started we said everybody in engineering, including all the managers are going to have to go through this interview. And so we reined everybody internally and it's like it's one of those things where it's like it's not a we didn't like keep a score like or like you know I don't have any data on like who passed or failed or what they what they scored but what we found is like as people would take it it would actually cause them to have moments of realization where it was like oh I I can uplevel my skills or sort of like I have like I want to be better at this and so we're trying to find like um a way like</p>
<p>a variety of techniques to kind of push the culture along. Um, and I think as I reflect on like the year, cuz this is the year where we really put all the effort into it, um, I'm really satisfied to see the extent to which everybody's leaning in on a on a daily basis. Going back to like even I I was shocked when we were looking at our cursor logs that</p>
<p><strong>[63:20]</strong></p>
<p>like the number one user is is an engineering manager on for org. It's like that that is super cool to me. It means that like folks have have uh have taken this to heart and found found ways of um doing their job differently. I guess my I I had a closing question or I guess a parting question and this is broadening out from Brex and this is just you interface with other engineering leaders all the time. >> Did we not cover anything that other CTO's are having as top of mind today like their number one problem is underscore?</p>
<p>>> The thing I find myself discussing with with folks that and I I don't want to shy away from like scary topics. Uh, in fact, we were just just kind of on one that was adjacent, which is like how do you evaluate somebody's like progression towards being more AI native. >> The the the the cousin to that question is it's like will we need as many people um to operate our businesses? Like are there layoffs coming? Are how are we how are we thinking about like um headcount growth? >> Junior versus senior. >> Junior versus senior. Yes, exactly. Like level mix. Um and I still have more</p>
<p>questions than I have answers there. I think I think what has been really interesting is that I view agentic development as being something that amplifies all the all the good just as much as it amplifies all the bad and amplifies uh uh sloppiness, poor architectural thinking, um uh misunderstanding of of the requirements like there are for all of the the acceleration of good outcomes, it also accelerates bad outcomes. And think what</p>
<p>has been interesting is that there's been when you sum that all together there's less of a obvious um like capacity increase. It's it's more it's more nuanced than that. And so I'm not looking at headcount planning uh as we think about it next year as as being something like oh well because AI is giving us so much more leverage we we don't need as many people. Um, we've actually, the thing I'm really proud of in in my tenure as CTO is that we we haven't grown engineering at all. What we've done is we we've grown the</p>
<p>business significantly, but we've been able to build like greater efficiencies and and how we execute, like how we how we uh we think about building, how we roadmap um what we choose to do and what not to do that we're able to uh to serve significantly more customers with more lines of business um without needing to grow engineering headcount. I think that that's kind of the way that we're going to just continue on this road is like I like having 300 engineers. Like I would love love love to just you know a year from now have 300 engineers but we're still you know 30 50 100% more efficient. that that that is the thing</p>
<p>that comes up with uh with other engineering leaders and the other part of that conversation is like how much is AI getting blamed for just sort of ordinary performanceoriented uh rims you know like if if Microsoft is letting go of like 4,000 people as a business what they have 150,000 employees I believe uh is that really like AI causing that or is it them just</p>
<p><strong>[66:21]</strong></p>
<p>using it as a way to uh to avoid some harder like perf management decisions I'm not entirely sure But I'm I'm listening more than I'm speaking on the on this topic because I every time I feel like I have a pretty firm point of view, some new uh anecdote or experience comes in that kind of challenges or invalidates it. >> Yeah. Well, you know, I I take these signals as it's my job to go find people who think they have answers and surface them. And you may or may not disagree, but at least you have something to use as a straw man in in your work. >> Exactly. Exactly. And I I think as an</p>
<p>industry we're just early innings on on on this transformation. So I'm looking forward to seeing uh uh you know listening to this this podcast episode a year from now and and and seeing you know what we got right, what we got wrong and what's different uh because so much changes uh quarter over quarter. >> Yeah, I do think AICOE is a very well established pattern. I think uh internal platform is very well established pattern and this uh fluency thing is something that people are figuring out that I think you guys are ahead on. I'm happy to hear it'll be my feedback.</p>
<p>>> Yeah. >> Any final call to action for things that you want to buy? Like what should people build for you like problems you're trying to solve that you would love people to reach out for to to help with? The call that I'd make is for folks who are interested in in multi- aent networks to to get in touch with us because I I do feel like this is something where where we're we're innovating in in service of of our customers and where I I feel like the the frameworks, the tooling um and the the research is is is there. There's actually quite a lot of like interesting</p>
<p>papers and things that we lean on. Uh but I would love to uh would love to see more of that like encoded in the um in the what's available at large in the industry because I feel like my intuition has been that trying to graft LLMs into deterministic workflows and DAGs is is kind of underelling like the power that they have to actually plan and execute in a more sophisticated like fluid way. and and I and I just want to see like the the industry lean in more um on uh on these agentto agent uh uh</p>
<p>interactions. >> Okay. So I'll I'll dive in a little bit here cuz I have a minor opinion. You keep using the word networks. Is that a reference to a specific paper or it's your term for it? >> It's just it's our term and I think that that is that's actually the term that master uses as well um for it. it we um yeah initially we used to call them agent run times uh internally and then we just yeah switched to networks. >> Uh and then I think the other thing I wanted to get a clarification on is is it mostly a full agent talking with a</p>
<p>full agent or is there kind of like a orchestrator boss agent talking to a sub agent and I think that does matter for a subset of people who are building all these things because when you say multi- aents people don't agree what that means. >> Yeah. So it's it's a tree more than it is a graph. So it is like yeah we have >> and when you say network it feels more of a graph. >> Yeah. >> But it seems more directional as a tree</p>
<p><strong>[69:23]</strong></p>
<p>like there there is a hierarchy. >> There's a hierarchy. Yeah. But there but there are some violations of that. Like one of the one of the interesting use cases uh and this is where like the power of of having an an assistant for every employee plus having agents that run and and embody members of the finance team is really powerful because there's this interesting use case that that we brought to market which is that um one of the finance team agents that we we uh launched is an audit agent where like an audit agent kind of</p>
<p>embodies is the work that a lot of larger finance teams will do to look for patterns of waste, fraud, or abuse or like systematic uh avoidance of policy that isn't as obvious with a single expense. Like you can evaluate a single expense and the metadata around it to see if if it's um within policy or not, but um what if you start seeing an employee often make a large number of like $74 transactions when receipts are required at 75? Or what if you what if you see um certain things like oh okay there's actually a fair number of like</p>
<p>Door Dash expenses during business hours from this individual like on on days that an office launch is provided or maybe you see like ride share patterns that are are um where you have to look at broader context. Um so we built this audit agent that can like ingest your SOP and and look also ingest your >> this is a Pikes customers SOP. >> Exactly. Yep. And uh and what it does then is it's it's basically always looking for potential violations. And what it does is it it is extremely zealous like it it wants to have a minimum number of false negatives. So it</p>
<p>will raise a large number of potential violations and then a separate agent a review agent will then apply wisdom the wisdom of like is this important enough to follow up on? Is the dollar amount in question high enough? Does this user seem to have like a high compliance behavior? More generally, it makes a judgment call about whether it's worthy enough to take that violation and make it into a case. Then once it's made into a case, generally what happens is that you need to get more information from the individual. So if humans were doing</p>
<p>this, there there'd be some outsourced team that's like looking for all the potential violations. Then you have some full-time employee on the finance team who who's looking at all the violations. Oh, these are the ones that are important. We need to follow up on it. Then what they do is they hand it off to somebody who will go and slack that employee and be like, "Hey, what's going on here?" And so what we have is like the audit agent looks for violations, the review agent decides whether it's worthy enough to turn into a case. And then from there uh when the case is filed the that that will trigger an event to the Brexit assistant for that employee and like any additional</p>
<p>information about like the business justification um can be collected or maybe the assistant already knows because it in its conversation history of the employee knew something about why this this expense looked out of out of policy. And so you start having the the network becomes interesting when you have the finance team agents communicating with uh the assistant for various employees and then behind there</p>
<p><strong>[72:24]</strong></p>
<p>you have other other sub aents and so then you start seeing like more of a graph uh emerge but when you look at just what serves the employee it looks more like a tree. >> Amazing. Wow. I didn't know you were going to go into that level of detail. Yeah. Sorry about that. No no no. I'm actually really glad I asked. That is very impressive and uh I hope you uh do more content about that. >> Yeah, absolutely. We're we're really excited about it. I think uh it's it's been it's been good to finally figure out uh a use for for agents and have the technology be as uh like as sort of robust as it is to start realizing this vision cuz it's something that we we</p>
<p>kind of dreamt of a couple years ago and the tech like to your earlier point the tech just wasn't there when we were trying to make the make the a similar concept work with GPT 3.5. It was like, "No, we were hallucinating tool calls in uh back in that day." >> Um, awesome, man. Thanks so much for joining us. This was fun. >> I really enjoyed it. Happy holidays, guys. Thank you for having me. >> Thank you. >> [music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=BKLvySNVBtM</guid>
      <pubDate>Sat, 17 Jan 2026 01:38:03 +0000</pubDate>
    </item>
    <item>
      <title>Captaining IMO Gold, Deep Think, On-Policy RL, Feeling the AGI in Singapore — Yi Tay</title>
      <link>https://www.youtube.com/watch?v=unUeI7e-iVs</link>
      <description>From shipping *Gemini Deep Think* and *IMO Gold* to launching the *Reasoning and AGI team in Singapore,* *Yi Tay* has spent the last 18 months living through the full arc of Google DeepMind's pivot from architecture research to RL-driven reasoning—watching his team go from a dozen researchers to 300+, training models that solve International Math Olympiad problems in a live competition, and building the infrastructure to scale deep thinking across every domain, and driving Gemini to the top of the leaderboards across every category. Yi Returns to dig into the inside story of the IMO effort and more!
We discuss:

* Yi's path: *Brain → Reka → Google DeepMind → Reasoning and AGI team Singapore,* leading model training for Gemini Deep Think and IMO Gold
* The *IMO Gold story:* four co-captains (Yi in Singapore, Jonathan in London, Jordan in Mountain View, and Tong leading the overall effort), training the checkpoint in ~1 week, live competition in Australia with professors punching in problems as they came out, and the tension of not knowing if they'd hit Gold until the human scores came in (because the Gold threshold is a percentile, not a fixed number)
* Why they *threw away AlphaProof:* "If one model can't do it, can we get to AGI?" The decision to abandon symbolic systems and bet on end-to-end Gemini with RL was bold and non-consensus
* *On-policy vs. off-policy RL:* off-policy is imitation learning (copying someone else's trajectory), on-policy is the model generating its own outputs, getting rewarded, and training on its own experience—"humans learn by making mistakes, not by copying"
* Why *self-consistency and parallel thinking* are fundamental: sampling multiple times, majority voting, LM judges, and internal verification are all forms of self-consistency that unlock reasoning beyond single-shot inference
* The *data efficiency frontier:* humans learn from 8 orders of magnitude less data than models, so where's the bug? Is it the architecture, the learning algorithm, backprop, off-policyness, or something else?
* Three schools of thought on *world models:* (1) Genie/spatial intelligence (video-based world models), (2) Yann LeCun's JEPA + FAIR's code world models (modeling internal execution state), (3) the amorphous "resolution of possible worlds" paradigm (curve-fitting to find the world model that best explains the data)
* Why *AI coding crossed the threshold:* Yi now runs a job, gets a bug, pastes it into Gemini, and relaunches without even reading the fix—"the model is better than me at this"
* The *Pokémon benchmark:* can models complete Pokédex by searching the web, synthesizing guides, and applying knowledge in a visual game state? "Efficient search of novel idea space is interesting, but we're not even at the point where models can consistently apply knowledge they look up"
* *DSI and generative retrieval:* re-imagining search as predicting document identifiers with semantic tokens, now deployed at YouTube (symmetric IDs for RecSys) and Spotify
* Why *RecSys and IR feel like a different universe:* "modeling dynamics are strange, like gravity is different—you hit the shuttlecock and hear glass shatter, cause and effect are too far apart"
* The *closed lab advantage is increasing:* the gap between frontier labs and open source is growing because ideas compound over time, and researchers keep finding new tricks that play well with everything built before
* Why *ideas still matter:* "the last five years weren't just blind scaling—transformers, pre-training, RL, self-consistency, all had to play well together to get us here"
* *Gemini Singapore:* hiring for RL and reasoning researchers, looking for track record in RL or exceptional achievement in coding competitions, and building a small, talent-dense team close to the frontier

—
Yi Tay

* Google DeepMind: https://deepmind.google
* X: https://x.com/YiTayML

00:00:00 Introduction: Returning to Google DeepMind and the Singapore AGI Team
00:04:52 The Philosophy of On-Policy RL: Learning from Your Own Mistakes
00:12:00 IMO Gold Medal: The Journey from AlphaProof to End-to-End Gemini
00:21:33 Training IMO Cat: Four Captains Across Three Time Zones
00:26:19 Pokemon and Long-Horizon Reasoning: Beyond Academic Benchmarks
00:36:29 AI Coding Assistants: From Lazy to Actually Useful
00:32:59 Reasoning, Chain of Thought, and Latent Thinking
00:44:46 Is Attention All You Need? Architecture, Learning, and the Local Minima
00:55:04 Data Efficiency and World Models: The Next Frontier
01:08:12 DSI and Generative Retrieval: Reimagining Search with Semantic IDs
01:17:59 Building GDM Singapore: Geography, Talent, and the Symposium
01:24:18 Hiring Philosophy: High Stats, Research Taste, and Student Budgets
01:28:49 Health, HRV, and Research Performance: The 23kg Journey</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>The thing that I find the most useful about like these models in general is like when I have this big spreadsheets of a lot of results and I understand plots of it. I think models can quite use a screenshot and make a plot of this. I hate making this mplot like stuff about it's so annoying. There were so many moments this year where AI suddenly cross that like that immersion thing. I think AI coding is one of them where we just discuss. I think like nano also got to the point where I usually like you make these images is just like for fun. They just troll your friend or something like that. But like actually really got so good.</p>
<p>>> Welcome back. How are you? >> Yeah, I'm good. I'm good. Great to be back. It's been one one and a half years. >> Yeah, it's been one and a half. >> Feels like a long time. So last time we talked, you were at Rika. >> Yeah. And then you joined GDM again working for Quark again. >> Yeah. >> And more recently, you've started GDM Singapore. >> Yeah. >> Is it GDM Singapore? Gemini Singapore, dude. I don't know if you've named the team. >> Oh, I I I think we have a Germanite team in Singapore. Yeah, team in Singapore. It's called reasoning and AGI.</p>
<p>>> Yeah, reasoning AGI. >> Is it important to have AGI in the name? >> It was like a vit thing that we put AGI in. Yeah, I think that like one reason why we work on these models is that we want to get to AGI and this was a V thing that we added AGI to the job posting. Yeah, there's no like formal name of the team yet, but it's basically the Gemini team Singapore. Yeah, I mean I think people are like trying to triangulate Amazon has an AGI team, you guys have an AGI team and then let's say Meta now has a super intelligence team.</p>
<p>What are people signaling when they choose these names for their teams? Do they have oh we have a plan or is it just vibes? >> You try to fish some hot takes on the no >> you have officially AGI in your job title. >> No, it's not a thing name is is not Yeah. It's just you know we just want to signal the north style of we bringing these models to AGI. >> Yeah. Yeah. No, I wasn't really fishing politics. >> Okay. So, you rejoined GDM. >> Yeah. >> And I think last time we talked about I listened back to the whole thing. It was amazing episode last time you were talking about how it's like externally</p>
<p>were in Brain and came out and now you're back in GDM. Yeah. I wonder what's your just your general reflections just plugging back into the Google infrastructure. >> Oh yeah. So I guess coming back it's very interesting because it felt and return to Google like everything including your LDAP your username is all the same. It's like you play Pokemon you you leave it aside and then you go back and you click continue save game. Yeah you save game and continue game. It's like that. Obviously the last 1.5 years while I was away many things have changed. Brain is now part of GDM and</p>
<p>stuff. So I think that obviously a lot of things have changed but I think overall the coming back has been pretty seamless. Obviously I I love Google infrastructure and I think debuts are great and stuff like that. Yeah. And I'm very glad to be back to to Google infra. Yeah. >> And was the intention always that you were going to work on deep think? >> No, not really. I think I miss research a lot like doing like research not like</p>
<p><strong>[03:01]</strong></p>
<p>super fundamental research but like close to model research, right? But I really miss being at the frontier and trying to go beyond that, right? So I really missed that a lot and I think when I came back big thing wasn't a thing and I don't think there was any plans actually it was just like I'm just going to work on research and see what happens. Yeah I'm sure I guess there was some inclination that reasoning is the next frontier and that's like obviously the most rewarding research path especially this year. Yeah, I think reasoning these days reasoning and RL is like probably quite is RL reasoning</p>
<p>comes I spend a lot of my past life I call it the past art working on like architectures and pre-training but I think now I more I like transition more into RL research. I'm not like old school RL but the games RL and old school RL and and to be honest I had almost no RL background coming back but I think like like RL is the main means of modeling these days and yeah so I think it was pretty easy to jump back in and I think a lot of fundamental skills in research is general purpose and universal and and it's it's quite easy</p>
<p>to innovate even in a tool set that you're not super used to and yeah so I think RL is basically the main modeling tool set like we play around with yeah these superficially I see some you know in your UL2 and fan T5 work um some some overlap of like you know the the focus on objectives and the focus on the stuff that you're trying to incentivize so I would have maybe guessed there was more overlap than you are saying right now which is interesting</p>
<p>>> but I know I understand it's very >> super shift is objective and they have some like overlap right yeah I think it's just mainly like the on policy and off policiness of designing in these things that change how like also the learning algorithm itself right >> let's just introduce this kind of terminology to people if they're not that familiar with the sort of RL policy I do think that a lot of people are like trying to understand what is working about this generation of RL research anyway so Jason had this interesting post which I think you were co-signing</p>
<p>which is basically you always want to be on policy instead of mimicking other people's successful trajectories to your own actions and learn from the reward given by the environment basically correct your own path instead of trying to imitate other people's path. >> Yeah. Yeah. And first of all, he writes really well and I wish that more people wrote like him. But I don't know what's your reflection on that or your addition on top of that. >> Yeah. So I think like the biggest analogy of the own policy and off policy is basically all policy is basically like when you SFT something is odd policy. Basically you take some other model larger model stuff and then it's basically like this off somebody else's</p>
<p>generated outputs trajectories and whatever. I think odd policy is mainly like the core idea of like modern LM RL where you like generate and then you reward the model based on its own generations and then the model trains on its own generations. Yeah. So it's more it's a bit like selfisolation to some extent. You the model generate it own output and then you reward it and then trains on it own output. So I think on policy is basically this idea of like</p>
<p><strong>[06:02]</strong></p>
<p>model training on its own outputs and letting the model like generate own trajectories and then let letting some reward verify it and then the model train it own outputs. I think this is more generalizable in general. I think there's still a lot of like science out still to be done about the gap between SFT and and RL itself. But I think basically on policy and off policy right and I think bring this analogy back to real like life. I mean we this on policiness is more like like humans we are more on policy because we go around the world we make mistakes and then we ah okay this is but like imitation learning is supposedly somebody else</p>
<p>>> not first principle >> it just tells you what to do and they just copy. >> So I think yeah this philosophy bringing back philosophy to life is quite like powerful like when I like now I have a kid and everything like want my kid to try stuff and then you tell them like okay this is like where this went wrong where this went right and stuff rather than okay you just copy everything somebody else does. Yeah, there's a Montasuri schooling is mostly that right like very unstructured learning like you discover your own path and we just give you a safe environment to do it. >> Yeah. Yeah. Yeah. Yeah. What is the point in which you should transition</p>
<p>from imitation to on policy? >> I do bounce back and forth >> with humans, right? Not models, right? I >> I would say in models it seems like there mostly has been a very concrete like first you imitate then that's pre-training and then you are at the end. >> Technically SF is still imitation. But I think for humans alo a little bit of this right because if you basically like like sports right when you play sports you start off by imitating like hardcore imitating but you cannot imitate forever because you need to like imitation I don't know whe this is good energy but watching a lot of tutorials and stuff is</p>
<p>more like imitating you learn try to learn certain movements and stuff like that but then like on policy nurse is like going to the game itself and trying to get a reward signal from that right but so I think that humans do need some form of imitation learning but like I think everybody starts off by imitating thing but then again the human and model kind of is not it's just fun to have analogies but we shouldn't like take things like super literally and stuff like that >> I actually I'm a quite a serious taker of machine learning insights into human learning >> that's what we learn from models now</p>
<p>>> yeah because I think like machine learning is the most scientific way we have ever studied learning just in general >> that's true that's where we have to invent curriculum from like scratch >> yeah that's and things like learning rate. If your learning rate is too high, learning rate is too low. Like wait, do humans even have a learning rate? So I do tell people to to keep an idea of their own learning rate and to be wary of it being too low. So for example, if you've been wrong once, you should ask where else have I been wrong? And typically</p>
<p>usually, let's say learn, you know what I mean? People usually update slower than they should when they have been wrong. >> Where is it? Stubbornness. It could be stubbornness. Um I don't know is that is that the right word for it? It could be like they they're too Beijian when actually is like their prior assumptions are wrong and they need to completely throw out their previous assumptions because one counter example invalidates</p>
<p><strong>[09:05]</strong></p>
<p>all prior experience. Your entire world model is wrong. Throw it away. So Beijian actually wrong. Let's say you live for 10 years under some assumptions and you have one example that breaks your narrative. Okay? You shouldn't be like, "Okay, now I have 2% update." No, actually it should be like, "Oh, like something's really freaking changed. Everything I've assumed for the last 10 years is probably wrong. What else am I wrong in?" And update 20%, update 50%, not 2%. You know what I mean? That's your that's a learning rate thing for me. So my my direct example is the whole getting into AI stuff. I was watching</p>
<p>GANs for 10 years. >> Yeah. >> Has it been 10 years? Uh 2012, 2013. >> Time flies. Yeah. I was watching GANs and I was like okay this is cool it's getting more detail not that impressive then all of a sudden stable diffusion came out and you can run it on your laptop and that was my learning rate okay like [ __ ] like my mental model of generative image images did not include this and so I was like okay like I am very wrong and I need to pivot everything and that's how I started later space >> so will this mean that like your your learning rate is high</p>
<p>>> yes I yeah I will nudge it up I I schedule my learning phase because a role model has been violated Okay. I think it's a good it's a good strategy. I think also this brings a little bit to like when new paradigms happen like how fast people are to adopt it or like to invalidate their understanding of things. I think as scientists we definitely a lot of times we do have to keep as the few progress we do have to keep like invalidating our own world model. It could be like a certain ways the way to do like something all along and suddenly something comes along and invalidates it. Yeah.</p>
<p>>> Yeah. You can be very proud of your priors until it's like becomes your prison. >> Yeah. Yeah, I know. Yeah, that's actually very dangerous. >> Yes, it is. >> Yeah. Okay, that was a bit of a tangent. I don't know how we got there. You did highlight Denny's LM reasoning lectures where he got traced the intellectual history of reasoning in LMS train of thought to to RLFT. And then the one part that I was going to prompt you a little bit was also self-consistency, right? Which I think people roughly know. I think is more crudely implemented with OpenAI than with you guys where it is straight up they have eight inferences and they judge or</p>
<p>whatever. But I do think that also is relevant to on policy distillation where it's like literally you have eight different powers and they're all from the same model. So checking my intuition there. >> Basically the stuff that you're saying about why on policy is important and using let's say an external verifier to to improve your reasoning. You can also do that with parallel reasoning. >> Oh yeah. Yeah. I mean like when we train RM models they sample multiple times. So yeah to some extent there's some form of yeah self-conidence</p>
<p>>> is that directly the let's talk right >> yeah self quency is a little bit more the more nuanced version of if you talk to Danny will tell is not majority voting for sure but it's more I agree >> yeah it's more nuanced version of that but I think parallel thinking definitely is related to self-consistency >> yeah yeah I think for those open also actually put out some interesting papers on majority voting versus other forms of like multiple output consensus basically</p>
<p><strong>[12:06]</strong></p>
<p>like you at the highest level is actual an actual LM judge that decides like this is actually a worthwhile trajectory that is more valid based on some internal consistency or just like inspecting the chain of thought which is very cool that we can train models to do that. >> Yeah, for sure. >> Yeah. >> Yeah. Self consistency is a big like a big fundamental idea. I mean chain of thought itself was alo a big idea and then self consistency was also like a big fundamental idea in in in uh in modern like literature. >> Yeah. Amazing. Okay. So let's bring it to I guess the one of the headlines of this podcast is going to be about diving</p>
<p>into the IMO world. >> So this was around about May March March in July. July you guys announced Oh, this very nice photo here. This is the photo I was looking at. This is in London I believe where you had this room. >> Yeah, this room. >> Oh, you got to be at a like you got to be at a photo taking to to get the credit. >> That's [ __ ] right? What the [ __ ] >> No, no, I'm just kidding. >> The contributor list is bigger than this. >> Yeah. Yeah. But like they were like say like oh okay you should go to third >> to in order to get a literal gold medal. >> No no no no like like to get the the</p>
<p>credit for being the IMO at first. So it's just a joke. It's just a joke. Yeah. >> But anyway, okay. Could you tell the story of studying this IMO thing? Apparently it was done in one week. >> So let me like be a bit more clarify a lot of things right. So the IMO effort has been like very longstanding. So Tang and basically and co has been working on this even last year, right? last year because like I was not very bad at Google at the time like they had the alpha geometry stuff and then they were like alpha proof and stuff. So it's it's a very long extending effort but I think this year was the we wanted to try to like use actually use Gemini as a end to</p>
<p>end model basically no >> no no second system in text out model and even that was a not intuitive thing I covered the silver result from last year >> and I was like okay it's pretty close like it's one point off from silver just try be harder you'll get gold >> that decision to abandon it I think was pretty bold >> I don't know >> I personally was believe always believe in if we're not like in retrospect it's easy to say this but it's a bit like if the model can't get to IMO goal then can</p>
<p>we get to AGI basically so it's basically at some point we have to use these models to to try this Olympic competitions and I think that one of the goals this year was like okay we're going to do an end to end like tech index model >> that's where like my involvement came in so basically I was not like involved in the IMO effort only do the model training part. So I have to say that that tongue did most of the IMO thing. I just trained the model with a bunch of other what does that work involve? What the surface? So so basically we we just</p>
<p>prepared the model checkpoint for the actual IMO itself, right? So that that's also something that's easily overlooked about the IMO thing was that many times you you want to chase benchmarks or stuff like that is always like thing that you can kind of keep running and running and he climbing until you get there and then you but like the IMO was a live completion like some members of the team were in Australia for the thing and there was like this happening thing</p>
<p><strong>[15:06]</strong></p>
<p>was happening live was unfolding live. >> Oh it's a very alpha goal. You receive the thing you like punch it into your system and then like >> Yeah. Yeah. Yeah. So, so like some of the professors from Kung's team were like in when they went to the IMO itself and stuff like that the conference I don't even know where the IMO is a conference but it feels like there were people there like in Australia and then so it was a live thing and there were people who actually the job was to run inference on on on this IMO P1 the P6 that came out and they also came out on like different days so it's like</p>
<p>different sets like one day one day two something like that so the fun part is that I knew nothing about IMO like at all. I'm not like a kid that took part in IMO like too down for that. You're a piano player >> and yeah, I have a piano but what I only knew was that okay we delivered the checkpoint and that checkpoint was used to do the IMO but then like there was somehow a week in London where everybody gathered that. So everybody was flying to London and then this photo was taken there and then you get to see how all the different parts like come together</p>
<p>and like also being in the other rooms in the rooms with the other like co- captains and then it felt a little bit like a hackathon thing. So yeah, I think this was like the training process of this IMO model itself was like maybe a week or so. Not the actual like the whole like basically. >> Yeah. Yeah. I think that the question is I'm still not over the decision to throw away alpha proof. >> Okay. Okay. Yeah. Basically, I think it's very major and I understand that you have this goal of AGI obviously like at some point one model should do it to</p>
<p>do all of it, right? But I think you pointed a gun at me and said in 2024, what do you need to do IMO and IOI and and CPC and all the other stuff that you guys did was you need an LM reasoning system that knows how to operate a computer and knows how to write lean and run lean verifier and all these. But basically you rot the lean verifier into the chain of thought. >> Is that okay? So basically like it is not obvious that you can do that.</p>
<p>>> Okay. at all because I think >> so okay so I think what you mean is that like some in some way system encoded in of the model somehow >> yes >> yeah I mean it's just whether at the end of the day you just believe in like this like connection is one one model lots of parameter I mean there's also two use right there also two use which you know and stuff like that but I think to some extent the model we like I I think we should be able to get to a point where</p>
<p>to like in the past when the LM first started the model could not even be a calculator. Now it can somewhat be a calculator. So technically like a tool like a calculator is somewhat encoded in the parameters of the model. So I think we we we will eventually get a point where whether there's things that cannot be expressed in the parameters of the model is like an open question. We we don't know where is that limit but I think we will keep pushing and pushing this limit. So whether like uh some something like a lean system or like</p>
<p><strong>[18:08]</strong></p>
<p>some other things to solve other like physics engine or something where it's still we still continue to push that that that boundary. Yeah. But I actually don't know like whether there were a lot of debates about symbolic system versus >> yes that's the word I was training. >> I actually don't really know whether there was like to me I was just like oh let's train a model and then some someone told me to train the model and I train the model. Basically there was like overarching like I people like the IMO effort that that decided this and I also think that because the basically these specialized systems are very like</p>
<p>oneoff systems that are like you could create like a chemistry engineer create math engineer could create the thing right but at the end of the day you want one model for everything so yeah so I think this kind of fits that direction a little bit more where you have one model for and then this model was also like launched as Gemini dip thing as a general purpose Gemini dip thing so it's basically unchanged but with maybe some config toned down a bit. >> Yeah. Yeah. So the the the inference time config was like the one served to most people is different but and the full IMO like inference config was</p>
<p>present like shipped to some mathematicians just because of the inference cost right but that was good enough to be a general purpose model. I think my take is that this intuition was what led to the trying to go towards one model instead of cuz this specialized systems there's no end right you can create many specialized systems yes the most I can see in the future is there'll be a model then that is there's something that really cannot be subsumed by a model then you just use a tool or something right but my prediction is that I think most things can be can be</p>
<p>subsumed by the model I think yeah I mean researchers are quite good at he climbing Wait, >> history would say that you you have a lot of evidence backing you up. Is this the model output that this is it right? This is what >> Yeah, I think this is the model output. Yeah. >> What do you see when you look at this? You just see obviously it looks like a well-ritten problem. It looks like something a real human mathematician would do. I people did compare yours versus the openi one where openi is a lot more raw or have to clean up their versions. We don't have to talk about</p>
<p>openi but I just I think what is interesting to you when you saw this kind of output. I want to give a little bit like a special disclaimer is that I know nothing about the app. Right. So I think the wonderful thing about this era of LM is that like you can be like a AI researcher engineer and you don't have any domain knowledge and you can still yeah source get a gold medal universal tool >> that you don't know anything about. I can't pass this at all like this is foreign to me. But like maybe a proof is a particular kind of chain of thought. >> But I would say that the other</p>
<p>interesting thing that some of the some of your collaborators were talking about it was like oh this is the first example of reasoning in a non-verifiable domain which to me isn't proof by definition verifiable. I just want to give you things to riff on or debates that are might be worth digging into. >> So I think that's good. There's a lot of aside from proofs there's a lot of domains that are like nonverifiable and I think not not easy to verify. So it's</p>
<p><strong>[21:09]</strong></p>
<p>like when people mean non-verify, but it's like non-trivial to verify or like just not as not as easy as like the solution of like a math problem because pools are long form and it's also that's why it's non-trivial to to verify it to lin and then you do all kinds of things, right? So I think there's a lot of work to be done in like this non-verifiable domains. Yeah, I'm getting into this territory where I'm not sure what I can say, what I cannot say. Okay, so yeah, sure. Well said. I think another thing that is an open topic of debate was how much domain specific work or post</p>
<p>training was done because you then went on to do the II and CBC stuff as well, right? The same model. >> I was not directly involved in the ICPC but I was related to some extent. That's all I can say. Yeah. >> Yeah. Any other interesting call outs maybe just on the team? you called out Jonathan as someone who is co- captain on on this effort and yeah basically how does the effort come together >> so I think there were four captains for the IMO two from London Jonathan was from Mountain View I was from Singapore</p>
<p>so I think four of us basically trained this model together >> and I think one I also trying to see what T was saying but I think one one interesting thing was that like we all in different time zones and we all and there's something also very interesting about like passing on the job there's no like already like fixed workflow how to work together between captains. So it's more like oh I'm going to board the plane now. I'll be AFK for 12 hours. So someone take just babysitting the run. >> Sometimes there are bugs and stuff. The job comes down sometimes. So basically it's very ad hoc and it's very very it's</p>
<p>really between the captains how we decide to like work together and yeah but I think it was a kind of interesting time also because we were all flying like I think the London folks were not having to fly but I had to fly and John had to fly and then like when you visit another country you have another like if you visit an office you have many meetings. So that was in and out meetings and it was a pretty like interesting and I also think that we nobody really knew whether we will get go at that time because IMO actually has hasn't happened. Yeah, it it was interesting, exciting and then I think</p>
<p>like the whole process of this getting verified by the IMO committee and you know like you know that was like okay but we're not going there but I had to learn a lot about how the IMO works right apparently the goal score is not even it's not even a fixed number it's like a bell curve on right >> so it was like a time where you just look at the score you're like >> like I was even like looking at the watching the human part participants and then seeing like what the scores were because whether Gemini will get gold depends on like how the humans do. So you like looking at oh if a certain percentage you like what's the do we go</p>
<p>>> yeah to some extent you don't have any control over that. So >> yeah but you just curious right because so I would say that is definitely more in like exciting like there's more adrenaline than like just running on a benchmark and getting a number was like a process that that that took some time. Yeah. Yeah. But I think overall if you have specific questions you can ask also but I think this whole thing has been a highlight for me. This IMO effort has</p>
<p><strong>[24:10]</strong></p>
<p>been >> Yeah. I I would say most people if you ask them maybe two years ago whether a model could get an IM model they would have said like impossible. >> Yeah. >> Then the silver helped right from last year but like the fact that you can throw that system completely away and then just take existing Gemini and scale up deep think and then just run it for IMO gold I think it's also like very non-conensus compared to last year. >> Yeah definitely to some extent I think researchers were also surprised. I I wouldn't say like surprise but like it was more like a pat of on the back kind</p>
<p>of surprise but we actually made a lot of progress in we as in collectively the all the engineers and researchers working on Gemini it's a lot of progress being made you just look at how much we went in one year yeah and I also think that just 5 years ago right not two years like five you just you just imagine the outcome like you just look at the state of AI now like just generally the IMO and the ICBC go and like also even like things like nano banana if you just look at the AI progress now and five years ago I think people would think that we already reached like AGI >> some form of AI >> to some form of AGI we're just moving</p>
<p>>> if you just traveled like you take this checkpoints and you traveled back into 5 years ago uh someone should make a drama about this but I think it's really quite impressive like how the field has moved so quickly >> yeah yeah >> the hard parts you would say were scaling inference >> in what like hard expensive >> hard as in maybe the most amount of brain power expended on the team I saw some comments where they were like actually the hardest This part was the inference optimization or like the very very long horizon inference that deep lang needed compared to normal Gemini</p>
<p>stuff like that. >> I didn't work on the inference time scales. I yeah I wouldn't know here >> that is mostly that oh and then there was this the code name was apparently IMOAT which you named after your desk. >> Okay that's not really like it was in the so I think I tweeted about it at point at some point right bring up the tweet. So the IM cat was basically like okay it's not like a official code name or something. It's just like the name that the config of the job was like I cat that's that's the</p>
<p>>> you just need some kind of name. >> Yeah. Yeah. I mean I just like you know I just I like cats and then Yeah. >> Yeah. That is mostly it on unless you want to bring out anything else. We have other sort of researchy topics, but beyond before I go into sort of researchy topics, I did want to maybe leave the floor to cover. What else should people know about the reasoning effort that's going on at GDM? >> Let let me think of where to start this. >> What do people need to know? >> Was really good. Yeah, that's what people need. >> Maybe an easy one to start with would be a lot of people were focusing on maybe</p>
<p>academic benchmarks two years ago. Last year maybe LM Marina this year Pokemon is very interesting reasoning visual reasoning and just general long horizon agent planning yeah benchmark and I I don't know you seem to focus on it a lot and I think Gemini did it very well so obviously I think it's something that is easy to talk about >> I think I probably should there's actually nothing specifically done for</p>
<p><strong>[27:11]</strong></p>
<p>Pokemon >> of course >> yeah of course there's nothing specifically done for Pokemon and I think that I think Logan had this tweet recently about the recent Gemini on Pokemon Crystal >> and Pokemon Crystal is >> so much more efficient. >> Yeah, I think Pokemon is like so I used to play a lot of Pokemon and I'm a big Pokemon fan in general and I think it's a it's a great like you say it's a great long horizon benchmark and stuff like that and I think it's a good to check in once in a while on these benchmarks that like almost never get contaminated or like people actually like don't spend</p>
<p>time to climb benchmarks. It's like kind of silly to like build like okay like what you working on some people oh I'm working on Amy I'm working on h I'm working on Pokemon maxing or something like that that's kind of like funny we did interview the clock Pokemon I think his name is David and it showed serious flaws in enthropics screen understanding vision capabilities couldn't literally couldn't tell I'm trying to like get past this wall but you keep just keep</p>
<p>running to it doesn't know if the wall is And so that doesn't have any spatial reasoning at all. >> I mean that some of it could be like a harness like the harness thing or like also whether the model has access to like game state information or is it complete visual? >> Yeah. Cloud's implementation is very game state heavy. They dumped effectively like all the the memory of what's going on in the emulator. >> Yeah. Yeah. I see. Yeah. >> I think for I don't know whether I'm jumping off tangent or something like that. I think solving Pokémon is going to be more of like how fast you solve</p>
<p>it. And then like the thing that I have not really seen so far is like whether the model can complete the Pokédex. >> Why is that? I know you need more challenging. >> No, complet is so hard. You need to plan. You need to like you need to search up like information like there's some things if you don't like go online and basically you need to like have a little bit of deep research in this. Yeah. There the model will just never know like that it needs to trade. Okay. If he's able to go online post on forums and then find someone to like hey co can I trade with you Pokemon to evolve? Some Pokemon needs to be traded to be evolved. Yes. Or me.</p>
<p>>> Yeah. >> Yeah. But anyway, I I don't know. I have not seen the model be able to complete a book that completely hard actually for models. So I was I think that's actually an interesting like like an interesting one. Yeah. >> Yeah. I wonder what the real world analogy would be once let's if let's say we we have a model that is capable of doing that what can we make it do that we cannot do today? Oh, there's a lot of planning involved. Just real deep research >> like planning. There's also a lot of planning involved and it's more I think Pokemon the Pokemon game is very linear,</p>
<p>right? The Poké Bax is involve a lot of like backtracking, research. >> Yeah. A lot of research and a lot of Yeah. So, it's a probably a different nature itself. >> Is that as interesting to you as for example a lot of other people in the AI for science world are trying to discover things that you cannot look up, >> right? >> No knowledge. >> Yeah. No knowledge. Because basically what you're saying is we're not even there yet. We're at the place where models cannot consistently apply</p>
<p><strong>[30:13]</strong></p>
<p>knowledge that they look up, right? Like you give Gemini access to a web search and you say, "Okay, go try to collect all the Pokemon in a Pokédex." You don't have high confidence that they'll do it. I don't know if someone's actually tried. Probably know, right? No, I I think the hard part is actually like trying to use the like synthesize the web knowledge and then apply in in in the game itself with all that visual state going on and stuff like that. It probably will be solved in one. It's not >> it is challenging. >> It's not like super interesting like you're basically just >> the task really is can you look up the guide to do it and then can you apply</p>
<p>the guide? That's it. You know what's even more intelligent than that? creating the guide like being the first to figure out how to create the guide which is what >> oh yeah yeah but then when it comes to this is mostly does an exhaustive search thing the model to try and try like humans guide for that yeah >> okay so that's actually less interesting to you interesting >> okay actually to be okay when you think over like not super super interesting but it's okay it's just have not seen a model to try to do this so it's >> yeah I think like efficient search of a novel idea space is interesting obviously you can brute force anything</p>
<p>but we're not talking about brute forcing we're talking about trying create an AI scientist >> but no knowledge is actually an interesting thing that that I think is going to be quite a big thing being able to generate no >> Google has done stuff there which I don't you're probably not that close to those teams that has done AI scientist work >> there's some things that have been it's like for example if you freeze the model weights at like 2015 the free time at 2015 >> and then you even with the current model let's say you have okay just assume there's no leaking of information somehow can if you ask the model what's</p>
<p>the best ML okay not 2015 like 2012 or something just tell you like SVMs are like the best right this is the way that machine learning works in general right then the question is can you invent the transformer right it might not be able to like even today models they might not even be able to invent the transformer like if you freeze the time at a certain time and even you bring the tech I mean the model is a transformer so I just say there's no assuming there's no leakage so like totally possible like so so I think there's still a lot of open questions about like whether the model can really you know innovate and generate like really No.</p>
<p>>> Yeah. >> Knowledge. Yeah. >> Yeah. One related question on that which I think is related to the Denny paper which is I think people have this sort of mythicism on what reasoning is and reasoning if you really demystify it a lot. It's whatever happens inside the chain of thought tags, >> right? And you post you you you're eliciting that reasoning behavior from some stuff that is already latent inside of the pre-trained corpus. is that that's one version of this</p>
<p>interpretation. >> I think these days like reasoning itself is very vague and it's very open. So it's like most mostly different people will have different definition of what reasoning is right. So I agree that like this chain of thought is like basically when people think of reasoning they associate with chain of thought and obviously is what happens in in in the thinking and okay reasoning right but I think these days is more like I said in earlier part it's like reasoning RL is</p>
<p><strong>[33:14]</strong></p>
<p>almost like the like basically it's anything that is like post training to elicit capabilities basically RL and post training to exist capab capabilities so I think the actual like technical definition of reasoning is making models better with thinking and post training Okay. Yeah. So basically like RLing the model to think better, right? And thinking is more like thinking traces and thought trajectories and stuff like that, right? There's also this line of work for like latent thinking and and stuff like that like whether latent thinking and discrete token thinking is like going to be the same thing or like something like that</p>
<p>is like a open question >> meaning adding extra tokens to cap that represent or >> there's all these I don't forot what's the name like this academy papers that do this loopy things or >> track tokens >> or like they basically instead of decoding discrete tokens you actually simulate this by doing this in latent space right so when you do chain of thought thinking and like reasoning you basically decode extra tokens hide it in the thinking tag >> and then you decode stuff but like lat is basically you just don't decode tokens you just don't bother buy it</p>
<p>>> it might start speaking the native language of thinking is numbers not passing you through some filter of English >> and sometimes you might start thinking Chinese or something else >> yeah yeah generally I'm not I'm not really I don't really believe that model thoughts have to be the same with human thoughts I'm actually like generally in ML I'm more of the school of thought of let do whatever he wants in general >> there also discussion. There's a latent representation hypothesis paper that I think you're maybe sympathetic to if you haven't already read it. To me, sounds obvious, but basically image models will have the same idea what a laptop is</p>
<p>versus a text model will have they converge on like the same latent. >> Yeah. And obviously, you can align them and you can do all those stuff with them. And so, it totally makes sense that their concept would be just a vector of numbers that represents laptop. That's the concept. Yeah. Yeah. And okay, maybe you have some numerical differences between one model's idea of what a laptop is versus another, but it mostly would be the same. Yeah. Very interesting. The question I was kind of leading into was that because there's now we are in this age where DLM text is in the corpus of like stuff that we</p>
<p>train on where it's a little bit of a recursive loop, right? Like the reasoning tokens are out there now. And so pre-trained models themselves, pre-trained based models are also capable of reasoning and they're increasingly so as more and more reasoning text goes into the corpus. Isn't that interesting or is that worrying? >> Do you actually see much reasoning trace on the internet? >> So I've never seen those though. >> I would say that hug your face. Yeah, people are publishing that specifically</p>
<p>as now as to whether or not people researchers are actually including that in their training corpuses. Who knows, right? Like that's their their choice. But I would say that percentage on common crawl that has coot tokens in there went from zero to 0.001%. >> And it will just go up over time because like people are publishing it. >> Yeah. But I think if the sources are</p>
<p><strong>[36:14]</strong></p>
<p>like quite clear, you can actually like filter away those because usually people put it on GitHub or put it on >> Do you want to filter it? Maybe you don't. >> There's a choice for the for the >> Yeah. quite quite literally the whole reason why I don't think we covered this in the previous part, but two years ago a lot of people were like, "Oh, you just include more coding tokens in your pre-trained corpus. It will be >> Wait, but the coding tokens are different from like coding tokens. >> It generally is just outside of code for reasoning." >> Oh, that was like >> You don't believe that? >> No, no, no. eyes in light. I don't know if it's still true today, but let's see. >> Yeah, that was just our general coverage</p>
<p>of reasoning. I would say that there's a lot of interesting work here and more to do. Maybe I'll cover one thing which I know that you have personal inputs on which is that you have started using AI coding. >> Oh yeah. Uh so I actually don't really use much AI coding in the past but I think we've reached a point where AI coding has started to become really useful like okay so before AI coding the most the thing that I find the most useful about like these models in general is like when I have this big spreadsheets of a lot of results and I</p>
<p>just made plots of it. I think models can quite good screenshot and make a plot of this. I hate making this mattplot like stuff about it's so annoying. Okay, but that's basically like one thing that I can remember about like how I use AI in the past. But I think AI coding has started to become the point where I run a job, I get a bug. I almost don't look at the bug. I place it into like anti-gravity and like I told it that fix the bug for me and then I relaunch the job like beyond like VIP coding. It's more like VIP training VIP ML or something. I don't know. I</p>
<p>would say it does pretty well most of the time and it's actually there's there are classes of problems that is just generally I know this is actually really good for and in fact maybe probably better than like I would have to spend like 20 minutes to find like figure out the issue and then fix the thing and then >> to be so yeah that's very interesting because I would say like level one vibe coding is you actually know what to do you're just too lazy yeah it's just ah just do it for me like I've done this a thousand times like just go fix it like I know exactly what to do here you're saying Is that kind of like the next</p>
<p>level? Well, you actually don't even know. It's it's investigating it for you. As long as like the answer looks right, you just strip it. >> At the start, I was a bit like I did check it, look at the thing. And then at some point, I'm like, okay, maybe the model looks better than me. So, I'm just going to let it do it stuff and then I will relaunch the job based on the fix that the model gave me. And I think the models will just keep getting better and better. Yeah. >> So, yeah, it's something that that I also think that I recently there's anti-gravity. I think I think also because these tools were not like that in Google infrastructure is not that</p>
<p>easy to you don't I'm not that familiar what is available outside and when I was at the start I didn't really I think the models were not like so good like one and a half years ago so it's also like a forcing function that like so people are like oh try gravity is a game changanger and stuff like okay so I just started using and yeah >> yeah you spent some time with verun recently what did what >> oh no I really say hi green okay</p>
<p><strong>[39:15]</strong></p>
<p>>> I guess you were telling me you're an AR researcher that doesn't was to use much AI and like now you actually like AI pill as a user. >> There were so many moments this year where AI suddenly crossed that like the emerging things. I think AI coding is one of them where we just discussed. I think like nano banana also got to the point where I usually like you make these images is just like for fun you just throw your friend or something like that but like nano actually really got so good that you can use it for >> that that you can use it for like basically yeah so it's getting really good and I think yeah this year the stuff like and even things like the past</p>
<p>many this will like hallucinate things a lot but now I just trust it like automatically I think we just I people are just enjoying the utility by by yeah by by by by these models. So now I'm like I was always AI field like AI is a good thing like I don't see how anybody can disagree with that but >> yeah but you actually using it for things that you are high expertise in which is your own ML work. >> Yeah. Yeah. Yeah. >> And just just to you you do you have a special version of Gemini that you use internally that we don't have access to or it's like public Gemini or</p>
<p>>> I do I think it's the public Gemini. Okay. I was just saying like it would be entirely reasonable to train Gemini internal for only your code base and your work. >> Oh, actually I'm not sure though. You see these things are just like rather for me. >> But obviously if it obviously improves improves your productivity by I don't know 10%. Yeah. Worth it, right? So I I think that's interesting and there's the interesting thing levels of how much do you trust it? How much of your jobs do you automate away and no longer need?</p>
<p>There's also the question I guess about how people come up and train in a field if they you no longer need juniors because the Gemini is your junior ML ML researcher. So I think this all interesting questions. >> I want to say one quick thing first. Right. So I think that when it comes to like whether a model can be like a junior suite or like something like that right so I think if you think of it this way of if a job from a one one x suite one time suite can be replaced by a model itself but let's say you are a manager right the objective that the</p>
<p>metric you track is like your time and then if you can have a model that saves you like the same amount of time as the work that your reports do but you don't actually like replace one person per se but you >> a little bit from everybody. Yeah, right. Then you can I definitely agree that okay like when you count the net time save there are times where the model can fix bugs that like would have cost me like one day right one day is huge and these things are definitely like if you I don't know whether anybody has done any like real like metric evaluation or these type of things but if you use time as a real metric and then not as in number of okay maybe</p>
<p>three hours like kind of metric right but these things are not like like going to replace one person as as it is but more like a passive aura that buffs everybody the game terms, right? Yeah. >> I often think of myself as a bard cuz I tell stories and I plus everybody around me. You know, that's that's a that's an ideal situation for me in a DND group. >> Oh, okay. Yeah. I don't play DND, but Okay.</p>
<p><strong>[42:15]</strong></p>
<p>I said they're the kings of support hero. >> Support spot. Yes. Yeah. Okay. AI support I think is very encouraging. I think like where is it still not working for you that you've tried and you're like, "Oh man, I expected it to be better." Oh, there are times when models get try to get lazy and try to fix something in a like they still rehab. They get lazy and then they try to like guess like me into thinking that like the bug is fixed. So there there are still classes of there are classes of problems that are like very easy for the model, very hard for humans. There are some things that are very easy for</p>
<p>humans, very hard for model paradox and and stuff. So it's still very hard to characterize this these things into this proper quadrants and stuff like that. So I would say that the capabilities on models these days are good enough to be like really helpful but like it's still you is a bit like it still has some but yeah but I think this will like this I don't think there's anything that to to be done to specifically like focus fire these things is more like general capability improvements the model just get better over time and then these things will just like go away >> you say that okay so yes I think obviously in the grand scheme of things</p>
<p>just trust the process keep scaling in every dimension and uh things will just fall away, things will emerge. But you've also said in the past, I can't remember the exact tweet where you were like each additional data set compounds over time. They're just small additions. And I would say that when you say things like focus fire on things that you would think humans, it's easy for humans, hard for machines. Those are easy wins where you can just add a data set that would focus fire on that. And >> isn't hill climbing just a sequence of doing that until you reach AGI? Okay. So</p>
<p>I I get your point. I think that it's true that sometimes a lot of progress on the whole is just a series of small incremental changes that Yeah. that push. I think that's accurate. That's true. There's also it also feels that there's also a lot of like small like seemingly minor for the lack of better word like that push AI to the state where it is today. So I definitely agree. So nothing like against people who like focus fire, but it's just that when I mean that like it might be not easy to focus fire on things that are like not very easy to characterize. So</p>
<p>it's just that like when he has something targeted, right? Okay, I want to improve this capability as some data. So I think like defining like if I was defining the problems and stuff like it's like characterizing it and if it can be characterized and then okay then fine. But I think it's like what I was trying to say with the coding is that these things are not even some of the class of problems are like I don't work on coding but like people maybe work on coding they know like they have like terminology for different types of failures but so maybe somewhere somebody is focusing fire on this then make the model better that's great for everybody.</p>
<p>>> Yeah I mean that's why it takes thousand people to like get all these things together. >> AI is definitely like a big collective effort these days. It's a big >> it's really crazy. Okay. So I just wanted to broaden out to general things people are talking about in the community on research >> which again I know that you are very locked in so you don't necessarily have read all the papers or anything but we</p>
<p><strong>[45:16]</strong></p>
<p>can just riff on ideas. You can obviously ask me what I think as well. >> Yeah. >> Is attention all you need? So attention and transformer has been is like a core idea in the recent times like pre-training and scale is the thing that made attention and transformers like actually shine right because without I think the first transformer paper was just like a machine translation thing and then basically like GPT and bird were the ones that like actually showed the full like big potential of this idea. So in terms of is antia like really really all we need like probably</p>
<p>no but I think it's like one of from the architectural point of view also may maybe no but it's not all you need is but you need it definitely >> what else are you thinking about on that same level are you talking about stuff what do you mean when you say it's not all you need >> definitely need like you definitely need like the skill pre-training you need or the tokens you need like RL >> I think when I say when people say I say it's attention you need is mostly view. >> Well, will transervers get us all the way to AGI? Right. I guess is the >> Oh, so so basically when you get to AGI</p>
<p>is the >> will still be will it still be a gnome architecture or like meaningfully different? You'll be a transformer I think >> really like like you like people it depends on what you call it but I think unless the paradigm shifts completely which is I mean as a scientist you cannot like like completely like say no to like like that this will never happen but my feeling is that it's been like what like almost 10 years since transform >> 2017 years since the transform I think we have not replace self attention like</p>
<p>it's some form of it like you could rename it you could name something else because sometimes you >> can do local globals. Yeah, it's still a transformer in the end and I think like that's not going like anyway unless the whole thing with like back prop like everything like goes like the whole thing just changes completely like then there's a different story that's a different conversation to have but if it's still within the same scope and mounts so I spent a lot of time thinking of about architectures and like whether there's alternate architectures and stuff I okay at the sequence processing</p>
<p>level like >> there is the ultimate >> yeah there is sequence to sequence transformer >> it's probably the self attention is there this whole big era which I was also involved in this era where people try to like undermine the attention as much as possible like they try to remove it simplify it make it efficient like this whole like like efficient attention era at the end of the day the outcome was always like oh remove all attention but we have one layer of self attention there and it still works like that's at the end like of the always a story which even known a character he published some</p>
<p>stuff about how he has some ratio or mixing of local and global attention Right. Like you basically still attention but modifying it quite a lot. >> I will consider local and global attention to be like still attention >> just like how much you're skipping. >> Yeah. Yeah. The the only question is that if the formulation changes too much your QV becomes like ABCD MG or something like that or some >> maybe I'll give you some motivating</p>
<p><strong>[48:16]</strong></p>
<p>constraints in order to do this. You guys are still charging 2x for over 180k token context for 240 something like that and the max his theoretical max is 2 million tokens right what if we need 200 million is there some point at which where even this concept of input token context is irrelevant because you are doing continual learning that kind of stuff where you're modeling it as okay the AGI will be achieved through a sequence to sequence transformation therefore And attention is the best</p>
<p>sequence to sequence model or architecture. Therefore, attention is all you need. But I think other people are like sequence to sequence doesn't actually capture intelligence. >> But that's not really about sequence to sequence. That's more about like the whole gradient design and back prop thing, right? It's not the architecture itself. That's that that's a problem that we that is more of like the learning paradigm itself rather than the architecture itself. I think the architecture is just basically like the interface between the learning algorithm and the tokens. I think it's more about the learning algorithm itself and this</p>
<p>like continue learning this like there's many ways to think about processing many like insanely large like context right like 200 million 1 billion tokens or something like that right like whether it's going to be like you have a new learning algorithm that every time you run inference you you learn on it right then you can technically have some kind of memory like human being is learning as I'm talking to you right so that that's also like one way the other way is like whether okay maybe somebody will say that okay the attention is like just too expensive for 200 million 1 billion context so we need new architecture or</p>
<p>some people will say that oh we just improve the chips like accelerators so I think many ways to interpret it but I think it's like if it's about there's a lot of fundamental things that if it's about continual learning and stuff like there's a lot of fundamental things about the learning algorithm and and stuff as well yeah that will have to change I think like the learning paradigm and architecture and stuff like that goes like hand in hand and I think as the field progress and ideas just stack on top one another right so there's also this thing about a idea that was proposed has to be compatible with all the work that have been done</p>
<p>before to to shine right it's a bit of variant of this hardware lottery like by by Sarah it's not the harry that I wrote about the tu failing but it's it's the original hardware lottery but it's more like a bit of lottery of like the things proposed have to play well with the things that were proposed before so it's a bit like going down this local minima to some extent so now we are like in this local minima of like transformers everything everything right Maybe it's not easy to like get totally out of this because also a lot of people's investation optimization have been have</p>
<p>been done. So the things that play well needs to play well with the ideas >> before and the way I see it now is it's very difficult to like >> yeah come out of it. >> Okay. I'm not entirely convinced. I see what you're saying but let's call it gen AI [ __ ] hate that term. It's still a very young field and so yes, there's been eight years of work on the transformer, but what's that in the</p>
<p><strong>[51:17]</strong></p>
<p>grand scheme of things? Maybe we're in a local minima and we got to notch ourselves out of it. I do want to leave that open-ended. I don't have an idea. I do think that people are in this call what I sus calling the age of research, right? Like where we're like, okay, we scaled up what we can scale up. There's we know what the next maybe one two orders of magnitude look like in scaling on every dimension that we know about but what is the next dimension to scale >> there's this like mis misunderstanding a little bit about like oh the last 5 years has just been like scaling things</p>
<p>scale >> okay please tell me more yes you made that joke about now we scale re researcher salaries >> okay let's not go there but but I think that ideas like matter and I think that there have been a lot of good years in the last 5 years. It's just that maybe it's just not so it's not been like blindly like if you took a MLP right just like without self attention and you just okay I'm going to throw like hundred trillion dollars on this and scale up that thing and the thing >> it's never going to work. >> It's never going to work. >> Yeah. Yeah. So there's no like there's part of it there's also like I think the</p>
<p>bitter lesson gets used too much in like too conveniently used around but actually there's also a little bit of uh not a bit there's also a sweet lesson where it's like ideas matter and I think even to till today right like people downplay ideas and stuff like that yeah do you think the rate of new without being specific about what ideas cuz obviously you can't share but like do you think the rate of ideas has increased or decreased because there's like kind of a law of diminishing returns are smaller >> I think the number idea is always proportional to the number of researchers working on a certain problem. So by definition by definition</p>
<p>it should increase but I think the number of ideas that actually work is not decreasing compared to the last like if you're not in the era of diminishing returns yet. >> Well so I think ideas are still very important and there's still very good ideas that are game changers that are being invented. >> Yeah. Yeah. And I think I know the answer to this but is the closed lab advantage increasing versus open source or decreasing? like the Chinese labs they say keep publishing open source models and some of the American labs as well publish</p>
<p>open source models would you say that the ideas that I see there Nvidia has Neotron open has GPOSS these are all basically checkpoints on what is publicly known about training models as of this year you know >> okay okay >> that it's declassified information cuz everyone okay yeah everyone does this >> I think that the gap is increasing. >> I don't think it's it's completely predictable from stuff that you've said before. >> I think the gap is definitely increasing. Yeah, >> I think that would make that justifies</p>
<p>researchers otherwise what's the point of having researchers if not finding new tricks that compound over time. >> Yeah. Um >> Yeah. Yeah. But but definitely I think it's is increasing. Yeah. >> Okay. I'll do a side tangent. I don't know if you have any comments on this. So then this is very related to Nvidia's recent purchase of Grock which I don't know if you have views because you're very TPUcentric but are we memory or</p>
<p><strong>[54:18]</strong></p>
<p>compute bound and this is relevant to the transformers discussion of like >> in terms of what like serving >> exactly I think the the classic view is that we're comput because we just need more comput for pre-train and RL and then inference. >> Yeah. But actually the counter argument I would make against this is I actually have these charts of Moore's laws. I wish I could just pull it up easily. Mo's laws of the scaling of compute versus scaling of memory versus scaling of network and bandwidth. And compute has a much higher slope of</p>
<p>scaling than the other two. Memory what the cheap memory like honestly I don't think about this this memory bound that much. So maybe it doesn't. Yeah. So I would disagree with it but I don't have high confidence in in >> and because you're mostly on the research side less on the inference side. >> Yeah. There's maybe the inference guys will be like oh >> yeah yeah yeah I don't think about I don't wake up when I think about serving. So yeah maybe I don't think about the inference that much. Yeah >> my my previous line of discussion here was like Nvidia is very forsighted by Milanox because it actually is the real</p>
<p>bottleneck in scaling because it has the lowest mor law >> and then the second one now is memory >> which is very interesting. >> Okay. Okay. But honestly, I don't think about I don't think about this that that much. Yeah, I understand. Okay. Data efficiency. So, this is a joke. But implicit in this is that there's some kind of maximum data exposure, right? And and so I so previously I would say that a lot of the training paradigms is like one epoch is all you need. That's the nimi title of this idea. I would say that the real number maybe is between</p>
<p>three to four epochs. And I do wonder what the theoretical limit of data efficiency of a model in terms of training and compression should be I don't know that means >> data efficiency and basically but you're asking the question in a way asking like how much repeats is tolerable is that >> one and tolerable is contingent on does it actually improve in meaningful it's not about like you actually want to do it for it for his own sake but I do think there's that and then there's also</p>
<p>just the sheer amount amount of stuff that we can learn with limited data. So you say let's say say like we you're not compute bound you're not memory bound but let's say you are data bound right last time that we were on the podcast we talked about chinchilla versus inference optimal training but now actually I think a lot of people are even talking about like data optimal training like given limited data set how well can you learn from it I think that's an interesting research direction that not enough people are talking about maybe is something that is common place in the labs but it seems very clear that we are</p>
<p>very unoptimized with regards to how much we we learn from our data. I'll just put it there. I think in general the like learning more like extracting more from every data point is definitely valuable but I think that also related to the fact that we're like running out of tokens in the world. So I don't work</p>
<p><strong>[57:21]</strong></p>
<p>on data for pre-training and I think things that I say would >> general state of industry not >> the general state right so I think that the I don't know even know whether data has like diverged like the way that these things are done it have diverged too much across this labs and no open >> there's a lot of cross crossollination for sure >> crossation okay yeah but I don't think about data that much like the pre20 data that much this >> yeah maybe earlier this the first half of this year I would have said that kind of pre-training is dead and that everyone's like just funneling all their</p>
<p>work towards RL and you we had this like grog chart which is very interesting where we're sending the same amount of compute on as to you think it's a scout >> no I don't know I had no idea yeah >> I think that people are taking it seriously they are like yeah okay whatever especially in the agent labs like cognition they're taking the open- source models from from whoever and then adding let's call it pre-trained scale RL on top of it if they have that level of info which data which they do</p>
<p>>> which is very interesting I would say yeah this data efficiency argument yeah I think it's to me it's also more trying to discover new paradigms of learning in order to get where we want to all go which is yeah >> and the existence proof is humans right your 2-year-old daughter can is much more capable than than an LLM in some things having seen way like eight orders of magnitude less data. That's very interesting. >> Yeah. Compare human learning, machine</p>
<p>learning is definitely like like >> purely as an existence proof that we could probably do better. Three examples of dog. >> Yeah. >> Fourth example of unidentified animal I can probably tell as a dog as a human but machines classically you take 20. >> The efficiency of humans is definitely way higher than than models. Yeah. The only question is that where does this thing come from? Is it actually like putting more flocks on every token or like maybe it's like back to the question about whether the transformer is the optimum architecture? Maybe it's back prop is the maybe it's the off policiness. Maybe it's the</p>
<p>>> Yeah. So what is the like where's the buck right where exactly >> uh but maybe it's a feature not I don't know but uh >> so okay we've identified probably it took me a while to get this across. So this is the kind of data efficiency I'm I'm talking about. I think it's emerging basically at the end of every year. I like try to take bets as to okay what will be the big themes for next year. >> Yeah, >> I think this is one of them that people are really trying to focus on because you're feeling this data crunch even though everyone's like still investing in data. I forgot to mention that I would say that I've been wrong on pre-training being dead. Yes, I've now</p>
<p>met pre-train leads from both enthropic and ofi and I've seen the talk from the deep mind guy recently and so like everyone's investing in pre-rain still which is like >> nice to see >> nobody's preaching was dead >> I know no it's a it's a theory that we're trying to disprove or or prove anyway so I think okay let me wind back to my general idea right so yeah data</p>
<p><strong>[60:22]</strong></p>
<p>efficiency seems worthwhile you would treat it as like okay well show me where the bug is and I'll go fix it. >> We don't know where the bug is. We just have existence proof that it could be better. >> And then I think the final >> logical chain in this for me is that everyone is focusing on some idea world model as a version of this for more efficient learning which potentially might not take the form of a sequence to sequence transformer. I don't know how that works like definitely a little bit out of my depth here. To me, that is more efficient because every wall must</p>
<p>be internally consistent. And if the next piece of evidence come in and invalidates those walls, then you no longer need to pursue those paths ever. And you can just narrow in on the wall that you've identified. And so to me, that is learning where you're learning to fit world models >> onto the actual. >> Yes. So yes, >> maybe you can treat the learning process as curfeitting. >> Yeah. So you're learning the world instead of learning the world model. Yeah. on learning the world model, right? Okay. By sampling multiple world models and then finding out which one</p>
<p>fits the data the best. So I guess my query is this what people talk about is this if you I mean obviously feel free to attack it because I'm just spitballing but this is what I pick up from talking with multiple people about okay what are you talking about with world war models what are you talking about data efficiency and learning efficiency and like how do you j it all together in a cohesive sense of the future where we can what's the definition of world model at the start from start >> yeah there are three kinds >> okay okay go on yeah >> first kind is the vio kind >> v kind</p>
<p>>> or the what's the other genie >> that that D man has which is the sort of video world model. You model everything with some kind of goss or whatever and you like you inhabit those that 3D space. >> Yeah. >> Second, let's call it the Yan Lun/ Meta school of thought which I don't know if you're that familiar with it. He has published the jetack architecture and then separately fair has also published the code word models where you're basically specifically for code very interesting you are executing code and modeling the internal state of the execution environment as you go line</p>
<p>>> okay >> the the LM actually like learns to predict those things and actually it seems a lot more efficient at the scale that they've tested it out >> which is very cool >> which definition are you anchoring on >> the third one >> the code one the code world model >> that's the second one those two are bundled together >> the japer fans are probably hating me right now because I'm lumping all metals work under one school of thought. >> Yeah. >> But whatever. >> Okay. >> Okay. The first one is VO genie like super spatial intelligence those kind of video based world models. Second</p>
<p>one is some execution or some sort of explicit modeling in the uh as you as you sort of run through the the corpus. >> Yeah. And then I think the third one is this amorphous thing which I think people are trying to get to where they are doing what I said about the resolution of possible worlds and you're curfeitting as you learn as you inference. >> Yeah. But what is the world model itself</p>
<p><strong>[63:23]</strong></p>
<p>itself is it like >> it is a mental model of where everything is and how you think the world works. How what I think you think that everything >> but in technically it's like >> it is something in the latent space. Okay. Okay. So you can for simplicity could be just be like a transformer model between train and >> yes. >> Yeah. So to me that is the most coherence thing to the current paradigm which is you could actually do this in current transformers. I think the way that you train it will probably have to be different. >> Okay. I see. I see. >> I I don't have any conclusion here. I'm</p>
<p>just throwing it out as something where I know you're interested in this kind of stuff and I don't have that many knowledgeable people to talk to about it. >> No, I don't think about role models that that often. I think because world models are just not really well defined in the first place. But I think >> so don't say world models but the problems is learning efficiency and maybe I guess accuracy or like AGI capability that is not easily unlocked right now on our current path of scaling. >> Yeah. Yeah. So I I think I think when it comes to like like data efficiency I think it's more like uh I'm believer of</p>
<p>finding ways to spend more flops per token, right? Because you actually basically if you are data bound you want higher data efficiency because you can learn more from every data point squeeze squeeze out more points right so things like that can extract more can use more flops on every token is definitely like a form of data efficiency then there's the learning algorithm right because I think there's this there's a different scaling law of like humans is this machines is this and like dogs are this cats are this different exponent chart >> right yeah there's this famous famous</p>
<p>famous chart uh and point one and point two are just like not entirely like different things because it could be that better architecture is actually just spending more flops per token. So if you you come to a point where you are very data bound but not compute bound at all you just find algorithms that spend a lot of compute on every tok on every token. So I think the overarching point is just that okay that it's a learning algorithm thing for data efficiency and then if whether the correct way is actually does apply more flops per token does squeeze out more from every</p>
<p>or data every data point also because humans actually don't like like they exposed less when you say less or more data is very ambiguous because they technically like on 247 and then you mostly visual have a lot of like different types of inputs right >> and whether they actually spend more flops on every thing they listen is also question because maybe they're just pay efficient just because I mean I somebody needs to count like how much flocks the brain used to process like how much maybe they're just spending more compute on every token and also maybe the learning algorithm is different so but I</p>
<p>agree that data efficiency is very important given that that I think we're going to like there's limited amount of data in the world >> one more thing before we go into DSI you know how like we're talking about RL and like you're working on RL stuff why are people paying so much for RL environments >> so who is paying for our environments >> open AI and the topic at least I don't nobody said anything about the mind so a</p>
<p><strong>[66:24]</strong></p>
<p>lot of the model labs that are not you are well known for paying at least seven figures for external startups to create our environments for them to train in okay and I think the question is if you are your models are so good at coding why don't you do it yourself and so I think there's some amount of expertise that's being distilled from human experts into anal environment that you can and let your agents run wild in. But I'm curious if there's any other deeper insight than that because I'm not satisfied with my own explanation.</p>
<p>>> Our environments that are like that have a lot of domain expertise are probably very valuable and actually I don't know specifically about what environments people are actually buying. But what what was the thing that you're not satisfied by like the >> it was so valuable and a lot of people are saying like look it's a next app inside of a docker container that logs stuff out when you send inputs in then you could probably do it yourself internally right why you pay so much for some startup that you don't know to do it for you</p>
<p>>> actually I have no clue about what like why this is happening yeah I have no clue >> and a classic example would be like if you want to build a computer use agent for buying things on Uh, in e-commerce, you would want all environments that perfectly replicate maybe the top thousand e-commerce websites. >> Yeah. >> And then you just parallel vote out on all of them. Does that seem reasonable? >> I don't know. >> All right. Cool. DSI and LM Rexis. A big bet for me this year for my conference was we actually like started focusing on LM Rexis. The other</p>
<p>>> actually what's the motivation behind starting LM Rexis? Yeah, >> I think Rexis is the king AI problem in consumer. It is the single most valuable thing. All your feeds, any even search is Rexis. >> Basically, search basically like it's a retrieval. >> It's the god problem, >> right? Cuz Rexis is ranking, but then also filtering, also personalization, also reindexing and and and like performance. It is the god problem and you get paid a lot for it. engineers are not that excited by it, which is very weird</p>
<p>>> because they don't see a lot of them don't work on Rexus and they probably never will. >> Yeah. >> But they don't see the monetary value that can come out of a good Rex. The other two pieces of updates for me which I actually didn't even know that DSI like directly tied into this. >> Okay. >> Was one Twitter publicly adopted their feed algorithm as an LM Rex. >> LM are just used everywhere now. like whether it's actually like a >> like a big LM >> like whether it's like a generary retrieval type of models like it's like</p>
<p>another question is it >> we don't know all we know is that they have said that they have swapped out their current Rexus for an LM based Rex that's all they >> okay >> but what has what is published is YouTube where they actually adopted semantic IDs for YouTube's Rex >> and YouTube is obviously a big deal >> is it like public information yes >> okay okay >> they came and did talk about it with >> us okay >> and then they published a V2 this year</p>
<p><strong>[69:24]</strong></p>
<p>as well Okay. More info. I just So, so basically the last time we you were on the podcast, we didn't talk about DSI that much, but you have actually some background in IR. You care about IR. No, I don't care about IR, but but I think DSI, okay. Like DSI or generative retrieval was like I think one of my favorite works in the old of like I have some IR background in like when I was doing a PhD, I did some Rexis work. I did some retrieval work with Rexis and stuff. So, I have some IR and Rexis background. So I think generative retrieval and generative Rexis is all</p>
<p>very conflated. DSI started as a retrieval thing. So we did like natural questions like ranking of like doc documents and everything. It started off as I mean there actually we did an interview with Yanick like me and Don we did when the paper came out like long time ago. So at that time we wanted to like reimagine retrieval and search. So we wanted at that time LM we were still using T5 models at that time. It was like not we're not in the LM era yet. it was premium right it was like okay pre-training works kind of thing and then there were like some pre models around so we wanted to re reimagine</p>
<p>retrieval right but retrieval raxis they all the same formulation ranking retrieval problem right and then that's where we started to imagine retrieval as one giant that that encodes everything in the memory we tried so many different like sematic ideas actually my collaborator Vin was the one that came up with ideic ideas that basically at the start of this whole gender ritual was actually basically literally just trying to give a document like identifier and that's predicting like raw brute force predicting like this it actually like it actually works because</p>
<p>the models can memorize some something if you look at the literature from all the way to things like Dr. back this very of like model the words have no meaning they're just ID in in a vocab it's another number right and technically the models have enough capacity to predict but I think semantic ID was an idea that that basically you have some like sematic association and then you actually try to break down the search space hierarchically right so how this will evolve into Rexis was at the time after DSI came out right so at cheese group and Mahesh the guy who who</p>
<p>they did some exploration of applying DS SI to to Rexis and that's how that generative generative Rexis recommend system paper came out. >> I didn't even know he was involved. >> That's crazy. >> That was like basically us like transferring this like basically okay DSI works. If we try to try it on on Rexis and then I think the recommended system people have a slightly different way of doing semantic ids but it's basically just because the the domain is slightly different but after that I</p>
<p>think we were done with the invention part and with this one >> the rest is details >> the the rest are details so over time I also left Google and stuff like so over time this thing evolved a little bit everything I think I also saw like something like Spotify is also using something like YouTube Spotify like they they use this type of semantic ids they start of DSI like models I think from the research community point of view like the DSI world was the first one</p>
<p><strong>[72:25]</strong></p>
<p>that like decodes semantic tokens but then when we went to I don't know community is like strange in a way that like they will do things like oh this is gen retrieval it's not genies it's like they'll do this kind of like random things that is like a bit strange but yeah I really this was like the whole history of this gender retrieval apparently there's also a lot like of people working on I don't follow actually I don't follow Edis at all Now it's just not even in my mind like there was once I went to even in the Singapore office there Swiss actually like working on >> genative retrieval. They don't work on I don't know whe they're still working on</p>
<p>it but I met a person that tried to explain genative retrieval to me. It was quite funny that you know I kind of like co-invented generative retrieval but yeah I think this is this whole IR thing has been it's just an interesting phase and I I think DSI is one of my more creative works that I've done that is not like really LM but it's like under the general principle of apply ML to everything if the googler is working on general retrieval would that be like AI overviews is that something similar >> I have no idea okay for the people listening I I did have a track there I think you just type in AI.engineer</p>
<p>engineer and you'll get it where the Gemini guy was talking sorry the YouTube guy was talking about how they use Gemini for the Rexis I don't know what size of Gemini cuz they he didn't talk about it but this is public work now and basically every YouTube video uploaded gets encoded into some kind of code book and they they retrain this every on some kind of batch job uh yeah just interesting so yeah I I don't know if you even know what what Gemini is being used or</p>
<p>>> I don't follow like this these days. I do think like in the sense of like for people who are not still not getting it, applying intelligence and the general intelligence of an LLM to the retrieval to the recommendation task means you can accommodate such weird recommendations like such weird queries as well that normally like no classical system can ever handle and I think like is also somewhat emergent in a sense that when you were using T5 you just couldn't actually add that much value on top of a</p>
<p>normal BM25 retrieval technique. Would you say that's accurate? It is not just about paraphrasing. It is about understanding query intent. >> BM25 is a really strong baseline actually. B25 is a really strong baseline. Yeah. uh is like I I sorry I don't I don't know the the the comparative delta versus T5 for you guys versus BM25 but I don't expect it to be very high and I expected it to be a lot higher for for a true LM base depending obviously on the query set</p>
<p>>> I didn't really think about it this way before but because I've done modeling in many different domains going like search and you know in the search committee there's also set of benchmarks and stuff like that for like that people who climb on there some like there's Amy of or like a of I don't know what is it called these days anymore but generally the modeling dynamics of IR task is very different from like task is very different from standard language task or</p>
<p><strong>[75:26]</strong></p>
<p>like vision task and something like with Q clan LM where you train models like the way that modeling things interact with this environment is very different. So I think that I honestly I hated working on like Rexis and Rush stuff. Okay, I'm just looking about old days when you work on like T5, you work on you change architecture, you try to improve perexity, you use super glue like this is olden days era on like even now when you train LMS you just do zero shot two shot stuff like that your things the you because as a researcher engineer you just interact with the environment a lot by this you just like</p>
<p>okay RL by this environment but Rexis and IR has a very strange feeling to it strange feeling in the sense that it feels like your like whatever works it's like you are this in a world with the the gravity is different or like you are in a world where the modeling things that feel intuitive are not intuitive so it feels like a very strange space to so I wrote some papers back in my days on like Rexis and stuff like that every time I ran some modeling experiments for for frais and stuff like I didn't enjoy the it feels like the environment was</p>
<p>rude it feels like the vibes are just like like >> what makes it rude >> it just feels transactional >> no not not not transactional like I I don't know how to describe it for example like if you play like sports like play tennis perimeter when you hit the ball you have a very nice feeling like hitting the sweet spot when you do modeling in traditional when you get the feedback back you feel like everything sounds right everything feels right everything right but Rexis and IR is like a problem where it's like you hit the shutter C and you hear the glass shadow like randomly you just feel this</p>
<p>weird say that that >> cause and effect are too far apart >> like it just feels strange and then sometimes maybe the metric like I think races they use like all the NCG effects and then the BM25 strong and then you do this and then you get like worse at the like the the BF25 back in the day where you stack two LSDM to three LSDM you like whoa I see like it's just the game >> unrewarding area to work in it's just weird also the IR community and the retriever community is also like always behind the mainstream and then now it's just probably gotten even more worse</p>
<p>because of stuff so okay I'm getting into hotic territory but it's just like certain conferences are just like behind new rips and ICML and stuff Yeah, some conferences are just like they're just like applying things that that this >> they're downstream of >> they're downstream. They're downstream. >> Okay. Okay. >> So, it always feels very uninspiring to to to work on on this. >> Yeah. Look, there's a reason that you left and >> but it was like a side quest like where I work as a side quest thing. Yeah. >> Uh yeah. Okay. I understand. I still think it's an important business problem</p>
<p>even though maybe it's an unre unrewarding field. You kind of understand why because the academic benchmarks for those tasks are just so far detached from they're so far detached from what industry I didn't on any of these like in like the thing but just from academic point of view. >> Oh then all you need online right yeah test and oo okay >> that would have been a different experience. Yeah >> that is mostly our sort of topic research topics coverage and everything.</p>
<p><strong>[78:27]</strong></p>
<p>I think we're just going to end on a very simple one on GDM Singapore. you you organize a symposium here. We brought Jeff Dean Quark and all the others basically what's the general message or the impetus for starting GDM Singapore. >> So we will talk about the event first. So the event was mostly so co and I are going to start a team and then I think before I came back we discussed this for some time. Jeff was very supportive of this. He was in the region many times in Vietnam in Singapore last like about around the time where I was going to come back and I think that so this event</p>
<p>itself was qu and Jeff was were visiting and we just inspire the community here. I think that it's also a bit more like a soft like setting the tone right for the start of the Gemini team in Singapore. And I think it's a very rare instance where you get somebody like Jeff and Qu who are the true pioneers of AI in the world to be in one room and then are you there as well and I think like many people told me true of AI but okay I was there to to tweet >> yeah yeah I think having them all in one</p>
<p>room and then giving these talks like many people came out to me and say they were very inspired by their presence in the region. So starting a team and starting something is also very like there's also no one moment that is okay press the button it starts right it's like a process right so we hire people and then people join one by one and stuff something like that right so I think this event was more I would say like a to set the vibe I think it's possible for Singapore to be close to the frontier and I think that we having the true pioneers of AI here we want to</p>
<p>give this basically more like an inspiring thing and also get left for co and Jeff to meet the people here and and so Jeff was here last year but Quark hasn't been here for some time and he's going to have team here. So it's like also nice to bring him around and meet the people here. >> Yeah. >> So it was a really like amazing event. We met along as well >> and >> who a lot of people don't know has a CS degree. It's like one of the few PMs with a CS degree. >> Yeah. Yeah. I would say that the context of the meeting was more like partially like also he wanted to learn more about</p>
<p>the IMO stuff. >> Oh really? And then also about Jeff. >> Oh, cuz they invited you without knowing that these guys were coming or something like that. Right. >> It was a bit like some young Jeff and Quark and me where we went to visit the chat with atanga and we discussed a little bit on deep thing discussed a bit IMO and then I think the rest of it was more like Jeff and Liz and was talking more about like became less about AI and more about very macro >> economical political thing which was I</p>
<p>was very out of element way. So I was just like I just >> you were in the suit. Yeah, I I was just talking about the dictating and the IMO and stuff like that, but he seemed generally quite surprised that AI has reached this point. So I think >> but it was a was an interesting uh >> I I would say for people like you have done something that is unique in Singapore's history so far like you you're establishing a frontier research lab in Singapore</p>
<p><strong>[81:28]</strong></p>
<p>>> which is an accomplishment. I think the other thing also that I guess I'm still trying to wrap my head around is does geography actually matter? Like you're all working on the team. You have your London people, you have your Mountain View people, and mostly you're just like collaborating with them anyway. You've collaborated with them your whole life. I don't don't even really know what countries mean anymore when it comes to research or just AI in general because this thing is just inherently international from the start. >> This is a very good like question also is related to I think about identity,</p>
<p>right? Because I think you also moved from SF and Singapore quite a bit, right? I was in like model this like one two weeks ago and I'm here but like almost all my like if you just look at my aside from my family like everybody I talk to is like somehow in the Bay Area or like just because of work and everything I think the geography matters okay firstly the most like thing is like logistical is like probably time zone >> so you literally want the 24-hour coverage around the world there are advantage no what I'm saying</p>
<p>is that the difference is possibly But like it's also more like like people define the location more than location define okay the people somehow okay the time zone we get to time zone a little bit like pros and cons. I think there's pros and cons right >> you bullish on on Asia Singapore people the talent pool >> I think we managed to find like crazy amazing people but I al also have to say that this type of things is more like talent attract talent. I think most of the time like people are very excited like the vibe I get is that people are</p>
<p>very excited because it's like co team and my team and we're working on library core things related to to to AGI. So I feel like the talent we can get from the region is really good but it's only it's only because it's us we can unlock this talent otherwise might join some other place but it >> yes and move to to the US. Yeah. Yeah. Yeah. About the identity wise, I would say that I definitely agree with you that like why does it matter like that? I think that the advantages of Singapore</p>
<p>itself or like just anywhere like it's also that you are like okay the world is very go so technically you can interact as much as you want. You can also go there but I think Singapore has this advantage where you can go close and you can go far. I think the Bay Area is like so much about I have friends in London and New York. They will just never move to the Bay Area. I'm not against Bayer. I think Bay Area is a great place, right? But it's just yeah everywhere, right? I think sometimes if you have some like mental space and energy to like to have some other culture and then</p>
<p>like you know like London, Singapore, New York, they have their own culture, right? But the Bay Area culture is just like AI, right? Like you just go anywhere you just hear AI everywhere, right? Even the billboards and stuff like that. >> You think a bit much. Yeah. >> Although I did see some billboards down here also. >> Yeah. I was like what is this >> culture infecting Singapore? >> I do think that to some extent if you want to do research in you need a little</p>
<p><strong>[84:30]</strong></p>
<p>bit of peace and quiet somewhere right. So this island may be good for that but then you can you still like able to like be connected right? Okay. So I think that's mainly talent wise I think people are strong here. Uh yeah. So far enough away but you're still connected. You have strong talent. What are you hiring for? You're still hiring, right? >> We're hiring like like my team will work on like RL reasoning for Gemini and Gemini deep thing. I think we care more about like talent density now. So we're not like also like growing that big this</p>
<p>small team first just because comput is probably like important and yeah so I think that's something that that we're hiring for now. Basically, I personally just there's a lot of DNS, but I think like generally there's a lot of people like who are very capable, but I think what I'm looking for mainly is either you have like a track record of RL research or like some even not necessarily RL but or or like some exceptional achievement in like coding competitions or like some exceptional</p>
<p>achievement somewhere then that's like the kind of people that that we want. >> Yeah. because you don't strictly require I do remember something about your record days where you're like you like to train your own traders from scratch right so you don't >> I forgot if I said that or not but to some extent to to extend yeah I think we definitely be very happy with people that are like very high stats and just like even without much statistical knowledge >> no stats points in like just high tech points people like just raw IQ high tech</p>
<p>talent people like I think all like strong engineering skills ML ML can be learned easily the knowledge can be learned easily. >> Yeah, I think maybe one version of this is can it be done on the student budget and where they can you do something interesting anymore on a student budget. I would say re relevant to the point where conferences are quieter these days. I did do an interview with one of the best paper winners where they worked on thousand layer neuronet network RL and and that was done on a student</p>
<p>budget. They it was very cleanly executed pieces and paper and good findings. Look, I'm not sure if production models will ever go to a thousand layers, but they stretched it in an interesting direction and found some good recommendations and the guy immediately got hired by OpenAI. And I think that's encouraging for the grad students in the market who are like, okay, well, do I need to know somebody who works at these labs in order to get in? My uncle works there. I get the internship or whatever. No, actually, you can just do it on a student budget with with good adviserss. Oh, I actually think one thing interesting is that for most of the people that I actually went</p>
<p>to recruit them like personally like right so it's you see their work and then you send them DMs right so I get a lot of value people no no no like like for hiring generally >> so generally I almost to your point it's like you almost like you can just do good work put it online and then somebody will contact you right it's actually super easy but super hard at the same time because >> no I can tell you like I talked to a few</p>
<p><strong>[87:30]</strong></p>
<p>of these grad students they don't know what good work means means right because they don't know there's so many things their professors have the agenda they're forcing on them which like may not be right cuz it's not like their professors know what to work on either so yeah they just need guidance they just need hey work on these like five things you show me interesting result in any of them >> okay so if somebody comes up with something and then does something that you feel that is very tasteful and aligns with what like researchers in the labs like like want and they come out with that independently you know that the function that produces the subject is good, right? Like if you just go and</p>
<p>tell somebody to do this, like you you can you just get the signal that these guys can execute, right? So I think there's some value in people that Yeah. they demonstrate taste. Research taste. >> Yeah. Research taste. Yeah. It's very interesting. I feel like I could give people Yeah. I I do care about this. In some ways, the research directions work that I do is a little bit of that. like it's low accountability for me because obviously it's just thought experiments but I think for a lot of people it's like their career is bounded by can you demonstrate research taste with this like short three four years of that you</p>
<p>have and just do it. >> Yeah. >> Yeah. I would say that this is is more there's so much competition just because of like the everybody wants to get into AI. It's just more of like how to like mostly it's more like how you going to prove yourself that you're you're right. >> Yeah. It must be hard these days to be a grad student trying to prove yourself. It's definitely harder but yeah. >> Yeah. Not your job. Okay, that was it. Do you have any other sort of rants or topics that you had queued up before we rap? >> I don't. Yeah, but it was great. It was I had a great time. It</p>
<p>>> was fun chatting, man. >> Fun chatting. >> Yeah, even I even love Last time we were supposed to do last time we meet at the symposium. We were supposed to record. But even we just ended up hanging out and chatting. It's just nice to get to the brain dump of what's going on in your world cuz Yeah. working on really important stuff, man. >> Always great to chat with you. business. Yeah, >> good to chat. Parting words on the sort of weight loss and workout journey cuz that's also a big thing for you. >> I think being healthy is important to be to do good research, right? And I think I think I've been probably in one of I probably in a big physical health now.</p>
<p>>> Yeah, you look great. >> Yeah. Thanks. And I think it's also impacted my work in a good way. >> You did the sort of kapati inspired like biohacking. I didn't go to extreme but I was like also quite data driven when I came come I will like have my own emails and I will track this then I I was I'm still supposed to make a blog post about this but I feel like I'm not like really at the endame yet so like when I get there I will but yeah just to just for people who don't know I like I think I lost 23 kilos</p>
<p>>> this year actually across one year >> one and a half years uh so 23 >> yeah basically literally from the last podcast to now >> yes yeah There's an ablation study now 23 kilos. Yeah. And I think like my HRV heart rate variability has went up by two times and my rising heart rate has dropped by 30 beats per minute. >> 30 beats per minute. >> Like it was like 80 90 and now it's 160.</p>
<p><strong>[90:32]</strong></p>
<p>>> Oh yeah. 8090 is super high. >> Yeah. I was unhealthy. Yeah. Yeah. Yeah. >> Okay. >> Yeah. So I think like when it's hard like what do you have a thing that kept you going? You know a lot of people they maybe they their focus on AI and including myself, right? I do prioritize work. I enjoy work. I don't enjoy the fitness side, but I obviously it it feeds in to your intellectual work like the sort of log off and go for a walk, eat better, all that kind of stuff. Obviously, it feeds in, but like people seeing a positive example like you, they will get inspired to do the same thing.</p>
<p>So, I think it is good to set yourself up as an example. >> I think definitely helps like when I do these things for my health, I just think that it's also part of work because it helps me to get better in my job. So, it's important as well. I think it's important as well. >> Yeah, I like the HRV off the bat. I have no idea what mine is, but yeah, there's a general question about what is productivity and how do you measure it? What what really matters? And it's still unclear to me, but I do think general energy level and hunger almost like you almost have to like experience physical</p>
<p>hunger in order to have intellectual hunger. Yeah. I don't know if that's like a thing. >> No, when I'm hungry, I just think of food. I think to me it's like this destroying things but when you but but it's hard to do work when you're hungry. Yeah. >> Okay. Thank you so much. >> Yeah. Thanks. It's really great. Yeah. Have a great time.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=unUeI7e-iVs</guid>
      <pubDate>Fri, 23 Jan 2026 15:36:37 +0000</pubDate>
    </item>
    <item>
      <title>⚡️ Prism: OpenAI's LaTeX "Cursor for Scientists" — Kevin Weil &amp; Victor Powell, OpenAI for Science</title>
      <link>https://www.youtube.com/watch?v=W2cBTVr8nxU</link>
      <description>“2026 in AI for Science is going to look a lot like 2025 for Software Engineering” — Kevin Weil

From building *Crixet* in stealth (so stealthy Kevin had to hunt down Victor on Reddit to explore an acquisition) to launching *Prism (*https://openai.com/prism/) as OpenAI's free AI-native LaTeX editor, *Kevin Weil* (VP of OpenAI for Science) and *Victor Powell* (Product Lead on Prism) are embedding frontier reasoning models like GPT 5.2 directly into the scientific publishing workflow—turning weeks of LaTeX wrestling into minutes of natural language instruction, and accelerating the path from research breakthrough to published paper.
We discuss:

* What *Prism* is: a free AI-native LaTeX editor with *GPT-5.2 embedded directly into the workflow* (no copy-pasting between ChatGPT and Overleaf, the AI has full context on all your files)
* The *origin story:* Kevin found Victor's stealth company Cricket on a Reddit forum, DMed him out of the blue, and brought the team into OpenAI to build the scientific collaboration layer for AI acceleration
* *Live demo highlights:* proofreading an introduction paragraph-by-paragraph, converting a whiteboard commutative diagram photo into TikZ LaTeX code, generating 30 pages of general relativity lecture notes in seconds, and verifying complex symmetry equations in parallel chat sessions
* Why *LaTeX is the bottleneck:* scientists spend hours aligning diagrams, formatting equations, and managing references—time that should go to actual science, not typesetting
* The *software engineering analogy:* just like 2025 was the year AI moved from "early adopters only" to "you're falling behind if you're not using it" for coding, 2026 will be that year for science
* Why *collaboration is built-in:* unlimited collaborators for free (most LaTeX tools charge per seat), commenting, multi-line diff generation, and Monaco-based editor infrastructure
* The *UI evolution thesis:* today your document is front and center with AI on the side, but as models improve and trust increases, the primary interface becomes your conversation with the AI (the document becomes secondary verification)
* *OpenAI for Science's mission:* accelerate science by building frontier models _and_ embedding them into scientific workflows (not just better models, but AI in the right places at the right time)
* The *progression from SAT to open problems:* two years ago GPT passed the SAT, then contest math, then graduate-level problems, then IMO Gold, and now it's solving open problems at the frontier of math, physics, and biology
* Why *robotic labs are the next bottleneck:* as AI gets better at reasoning over the full literature and designing experiments, the constraint shifts from "can we think of the right experiment" to "can we run 100 experiments in parallel while we sleep"
* The *in silico acceleration unlock:* nuclear fusion simulations, materials science, drug discovery—fields where you can run thousands of simulations in parallel, feed results back to the reasoning model, and iterate before touching the real world
* *Self-acceleration and the automated researcher:* Jakub's public goal of an intern-level AI researcher by September 2026 (eight months away), and why that unlocks faster model improvement and faster science
* The vision: *not to win Nobel Prizes ourselves, but for 100 scientists to win Nobel Prizes using our technology*—and to compress 25 years of science into five by making every scientist faster

—
Prism

* Try Prism: https://prism.openai.com (free, log in with your ChatGPT account)
* OpenAI for Science: https://openai.com/science

00:00:00 Introduction: OpenAI Prism Launch and the AI for Science Mission
00:00:42 Why LaTeX Needs AI: The Scientific Writing Bottleneck
00:03:13 The Cricket Acquisition Story: From Reddit to OpenAI
00:05:50 Live Demo: AI-Powered LaTeX Editing with GPT-5.2
00:17:13 Engineering Challenges: Monaco, WebAssembly, and Backend Rendering
00:18:19 The Future of Scientific UIs: From Document-First to AI-First
00:15:51 Collaboration Features and Notebooks: The Next Integration
00:21:02 AI for Science: From SAT Tests to Open Research Problems
00:23:32 The Wet Lab Bottleneck: Robotic Labs and Experimental Acceleration
00:33:08 Self-Acceleration and the Automated AI Researcher by September 2026</description>
      <content:encoded><![CDATA[<p><strong>[00:05]</strong></p>
<p>Okay, we're here at OpenAI with some exciting news from the AI for science team. Uh with us is Kevin Wild from I guess your VP of AI for science. >> Open AI for science. Yeah, >> open for science and Victor Powell um who is the product lead on the new product that we're talking about today. Uh and with me is our new Yeah, for science host R.J. welcome. >> Uh so thanks for having >> us. Yeah, it's very good to be here. >> Yeah. uh thanks for hosting us as as well. It's always nice to come over to the office. Um what are we announcing today? >> So we're launching Prism, which is a an a free AI native [snorts] latte editor.</p>
<p>What does all that mean? Because probably a lot of people on the the pod haven't worked with Latte in the past. Latte is a a language effectively for type setting mathematics, physics, and you know, science in general. So if you're uh a scientist writing a paper, you're probably not using Google Docs because you need to you have diagrams, you have equations, etc. But it's and it's been the standard for decades. But the the tools that that people use to to actually write latte, write their papers</p>
<p>haven't changed in a long time. >> And uh in particular, AI can help with a lot of the tasks, right? Because you you spend your time doing the science, you need to write it up. That's an important part of communicating your work. But you want that to be fast and you you want that to be accelerated and AI can help in a ton of ways and we'll talk about some of those. But if you if you step back right is open AI for science. Our goal is to accelerate science and the surface area of science is very large. So we're we're trying to build tools and</p>
<p>products that help every scientist move faster with AI. Some of that is obviously the work that we can do with the model, making the model able to solve really hard scientific frontier, you know, frontier kind of problems. Uh allowing it to think for a long time. But it's not only that, right? If if there was a lesson from what happened over the last year with software engineering, it's that part of the acceleration from uh in software engineering came from better</p>
<p>models, but part of it also came from the fact that you now have uh AI embedded into the workflows into the products that you use as a software engineer. Right? It'd be one thing if we were going back and forth copying and pasting code between, you know, chat GPT and your IDE. that would be okay. That would be an acceleration. But the real acceleration came when you embedded AI into the actual workflow. And so that's what we're doing here. So OpenAI for science, it's it's both building great models for scientists and also speeding them up by bringing AI into the</p>
<p>workflow. That's what we're doing with Prism. >> Yeah. I often say like every million copy and paste done in ChachiBT, there's probably some product to be built, >> right? Exactly. >> That's a good analogy. [laughter] Yeah, that's a good way to look at it. >> Especially with latte, having written a lot of latte papers. >> Yes. >> Yeah. [laughter] Yeah. So, so >> me too. The number of hours as a grad student I spent like trying to get some diagram to line up. Exactly. And Oh,</p>
<p><strong>[03:07]</strong></p>
<p>man. >> Yeah. >> Uh and uh Victor, you this is your sort of baby. >> Yeah, I guess uh it started off as um just a a project. I I left Meta about three years ago. Um trying to look for various different projects to start. Uh and uh this was this was one that like when I sort of presented it to people, they're like, "Oh, I get it. That's I I see what you're doing." And so I've just been focused on that, building it for about a about a year and a half and um you [clears throat] know, it it it has now has become part of OpenAI and that's</p>
<p>been very exciting. >> Congrats. Thank you. >> Yeah. So it's kind of a kind of a fun story, right? I mean we as we were thinking we had this thesis around it's it's not just models it's also building models into the workflow and and accelerating scientists in that way and this is there are obviously a lot of different ways that you can do that but the the scientific collaboration and publishing uh thing is definitely one of them and I was looking around like what is there in this space and there hadn't been a lot of innovation for a long time uh like it wasn't that different from</p>
<p>when I was like writing up you you know, my assignments and papers in tech and grad school. And uh and then I found on this uh Reddit forum, maybe it was Rlettech, I don't remember, but somewhere on this Reddit forum, I found uh this thing about a company uh called Cricket. And uh I was like looking around. I couldn't find who the founder was. It took me a little while. And then I think I found you on Twitter and DM'd you out of the blue and just said, "Hey, I don't know if you want to talk about</p>
<p>this, but I would love to talk about this if you're open to it." And gave you my number. And uh we talked on the phone and then like um jumped on a Zoom and eventually met in San Francisco and uh made it happen. That's right. And it's uh it's awesome to have you guys here, but it's just um yeah, I have a ton of respect for what you what you started to build. >> I actually never heard that full story from you until now. >> [laughter] >> You got to find that Reddit user and thank them because you know >> it might have been me. >> I thought you were totally in stealth because uh it was it was the hardest</p>
<p>thing to actually figure out who the founder of this thing was >> and then I was like, "Oh, for sure he's not going to respond to my random DM." >> I mean, I guess that's a part of part of our focus has always just been entirely on product and to the point where it's almost embarrassing how little we focus on anything else. >> Worked out for you. >> Yeah. So also full circle from moment for you using Twitter to do your business development. >> Yeah, that's right. [laughter] >> So that's kind of interesting. >> Your DMs forever, >> right? Like I actually Yeah, like probably one of the most important</p>
<p>social network uh innovations I guess is is those that stuff and I'm sure you know a lot about that. Um shall we go right into a demo or talk about it? >> It's always fun to show it. Okay. >> I I'm a fan of like show don't tell, push people to the video. >> Yeah. All right. I'll try and arrange this so you guys can see a little bit. >> Yes. >> Um, all right. So, what you have here, so this is this is Prism. >> Um, and what you can see is on the left</p>
<p><strong>[06:10]</strong></p>
<p>here, this is actual latte. You can see why you might want AI to help you write it because it's, you know, >> a language, >> a little bit, it's a language. It's a little bit messy. And then on the right, this is my colleague's uh paper, Alex Lupas. He's a physicist. Um, this is a paper that he wrote on black holes. And so you see it over here all the all the l you know you can imagine trying to write this in like Google Docs or something it'd be impossible. This is why latte is uh super powerful. Um and then and you've got kind of your your files here that make up the project tech file which is the actual main source</p>
<p>file bibliography files etc. And um you know you can go through and you can change it and then you compile that into into the PDF itself. But here I can say um this is where at the bottom you can use the the AI you using GPT 5.2 and I could say you know this introduction maybe I want a little help writing the introduction. So uh help me proofread the introduction section paragraph by paragraph.</p>
<p>Uh suggest places where where I can simplify. This is a live demo and we were we're working on it pretty heavily. So just uh [laughter] you nervous yet? You can't be nervous. You're good. >> Spoken like a true founder. [laughter] >> Um and so one of the nice things is you could do this in chat GPT, but you'd have to go upload your files into a chat, right? You're going back and forth here because the AI is built into the product. It has all of the files that</p>
<p>are part of your project. It automatically puts them in context. it works the way you you think it would work. So, here it's looking at the files. All right. Um, and it's given us kind of a diff here. So, it's suggesting changes. You've got the the part in red, which is the part that it's changing. The part in green what it wants to change it to. And you can see the different places where it uh is suggesting that we change things. So, okay, we can we'll just keep all of them, right? YOLO. Um, >> hope</p>
<p>is all true. Yeah, we're changing Alex's paper. What's the big deal? [laughter] Um, so here's another thing. We were talking about diagrams in Latte. Uh, so I've got a say I wanted to input a commutive diagram, right? It's really easy to draw a commutive diagram like this. >> It is an absolute nightmare to put these things [laughter] into tech. >> So I will upload this photo and I'll say here, whoops. >> Is there a tech bench for this kind of stuff?</p>
<p>uh like a set of evals. We totally need one. I think there's an opportunity to do that for sure. So, here's uh a communive diagram that I drew on the whiteboard. Can you make it into a tix diagram and put it right after</p>
<p><strong>[09:13]</strong></p>
<p>the I don't know, right after right before right at the top of the introduction section. Make sure you get the details right. [snorts] >> So, so I didn't want to interrupt you while you were typing, but why don't you use voice? >> Oh, actually, I should and I totally could. Yeah. >> No, but isn't it interesting that we all have these voice buttons and we don't use it? >> Yeah. >> It's not second nature yet. >> Yeah. >> Like it's interesting. >> Um, and that one I I totally should have I was gonna also show something. So it</p>
<p>you have the you have the the you know here I am in the tech and it's working. You also can create new parallel chats. So you can have whole sessions with chat GBT that you can be you that can be going in parallel. >> So here I'll ask it um there's all these equations. We're talking about symmetries of this uh of this black hole uh wave equation >> and in particular there's this complex symmetry here. >> Um >> I like how it Yeah. And notice how it syncs when I highlight it. But I'll say</p>
<p>like why don't you I'll go to my chat so I can start doing this in parallel. I'll say um please make sure or please uh verify that the H+ operator in the new symmetries section is indeed a symmetry of the stationary axis symmetric. Do >> you understand those questions? Are there whole I have but after that Brandon is actually</p>
<p>[laughter] >> I'll say don't do it in the paper you know show it show it here >> I don't want it to actually like edit the paper I just wanted to prove it here right okay so I'll get that going >> now while we're waiting for the diagram to finish we can also get another thing going in parallel so I'll say um I need to write up a set of uh lecture notes on general relativity. You know, say I'm a professor, right? I've got I'm teaching a class or something. Um, put together a</p>
<p>uh 30 minute set of lecture notes on uh Romanian curvature. >> Wow, that's a very different task. >> Put it into the file. I made this grle lecture.te. Okay. And so I've got this going. All right. Well, it came back on my earlier one. H+ symmetry. Is it really? Here you got chatbt doing a whole bunch of uh uh of work to verify that this is indeed a</p>
<p>symmetry of the equation. Okay, it does. It confirms it. Right. So you've got the full power of a reasoning model that can think deeply about frontier science. Uh and now we can go back while it works on the other thing. Okay. So it it this was where I was making the diagram, right? Uh it put it right below the</p>
<p><strong>[12:15]</strong></p>
<p>introduction. I'll compile it again. >> So it is an autoco compile. >> Uh >> actually you can turn that on. Okay. >> And look. Wow. It it nailed it. >> Um so it looks like it got it pretty much exactly. >> Just a small check the details. >> Oh yeah, check the details. >> Uhoh. [laughter] Good enough for me. >> Yeah, it's pretty good. But all right, we can we can see if it'll get it right. Let's say um uh the Cvertex should be directly >> to your point about voice though I do think uh maybe over time the code kind of might recede into the background more</p>
<p>as you're just really interact you're interacting with the paper you're having conversation [clears throat] with it. >> Yeah. >> When you when you started this product was this how you envisioning it would be used or were there other design choices that you were considering and you didn't take that path? Um, >> by the way, before you answer, we have our we have our uh general relativity lecture notes here. >> Well, that was quick. >> So, >> 30 minutes. >> This is a six pages. >> Yeah. So, 30 page 30 minute section. Okay. So, we got curvature, coariant, derivatives. Yeah, this looks like a</p>
<p>reasonable set of uh of notes if you were going to go teach a class, right? It just did it for you. >> Or you can even think like, you know, generate the problem set for for this week. >> Yeah. Right. You've got work. So, it's got some examples here. we could tell it to like work out solutions to the examples. Um, >> that's sort of a hidden feature of Latte, too, that it it actually makes it pretty easy to generate problem sets with like answer sheets and things like this. >> There's so many there's so many cool features of Latte that I think uh are underutilized. >> Yeah. So, anyways, you could see we we went we we uh had it proofread the</p>
<p>paper. We had it check some of the answers uh to verify that our calculations were correct. We generated a set of lecture notes. We added a diagram that we didn't have to actually type up ourselves, which I promise you is horrendous. And that's just, you know, we did that all all basically in parallel. And you know, you can imagine lots of other things. You can you if you have a proof that you you know, you maybe have like the the sort of the bullet points on a proof, you can just say, "Here are the bullet points. Now, flesh it out for me." You can imagine having it check all of your references</p>
<p>before you publish. Make sure all of them are real, up to-date. You can imagine having it generate your references based on the topic of, you know, so there's so many areas where AI can help. [laughter] >> That's a big problem when you're trying to put together a paper is get all the references right. >> Yeah. Well, okay. So, >> and all of this is time that used to go to, you know, typing up a paper >> not science >> and not science and now it could go back to science and that's just one of the ways that we look at accelerating scientists all over the world. Yeah, I I would say uh definitely, you know, be</p>
<p>careful about including references you haven't read, [laughter] right? Like that's the whole point. Like you can include 100 references, but if you didn't read them, then you might as well not have them. >> Uh but yeah, I think that web connection is is very important. And like is this stock GPT5 or GPU? >> GPT 5.2. Yeah. >> Um but and by the way, you when you're</p>
<p><strong>[15:16]</strong></p>
<p>looking at references, you can also ask chat GPT to help you understand the reference. you know, read this paper, tell me the relevance. So, all of the things that you might want to do to accelerate your work, you can just do from within this interface. >> You still have to do your work, but it should [laughter] make it it should make it faster, especially like even linking to the references. So, you can go and verify like, okay, this is this one. >> So, this might also make it easier to write the paper as you do the work, right? Rather than [clears throat] rather than, oh, okay, now I got to spend two days in Latic land trying to get my paper, right? like a tool for</p>
<p>thought rather than just a publishing tool. >> Yeah. >> Yeah. Yeah. >> What about collaboration? >> It's a great Yeah. So, it's built for I mean, you can speak to this well. It's built for collaboration. So, you can bring on as many uh >> uh collaborators as you want. Okay. >> Which is nice. I think most other tools in the space have hard limits and charge you money and other things. >> In Prism, it's as many collaborators as you want for free. >> Um Yeah. So, you've got commenting, you've got all the kind of collaboration tools that you would want. Um, good. >> And then any other like engineering</p>
<p>choices like um, you know, what might engineers not appreciate when just looking at a tool like this? Uh, you know, often it would be like multi-line uh, diff generation that you need to do because you're editing a pretty complex document. >> It does get pretty complicated. I mean, we're using um, let me know if I'm getting too technical into the weeds, but uh, you know, we're relying heavily on the Monaco uh, uh, JavaScript framework. So >> I'm very familiar with the lack of documentation of Mono. [laughter] >> That's actually it's interesting you say that because it's it's very true there.</p>
<p>It's extremely powerful library that is almost entirely undocumented. So >> you can use codecs now to generate the documentation [laughter] for you. >> Yeah, you you think Microsoft should get on that. Uh but yeah, you know, like just stuff like that. Like I I like to hear about like the behind the scenes of like building something like this. What what do you what do you struggle with? What's the model really like surprisingly good at? and what's the model it should be good at but it's not. >> What were some of the hardest problems as you were building this in the first place? What are some of the hardest things to get right? >> I think um initially maybe one</p>
<p>interesting challenge was that we really pushed on it being um web assembly and fully just running in the browser at first the whole entire latte compilation and that did help us in the sense that we were able to like flesh out the design and the AI capabilities early on without having to invest heavily in like the backend infrastructure. Uh but eventually we did hit a wall with that approach and once we switched it to backend PDF rendering like that's when we really started to hit an inflection point with like usage. >> Yeah. Fast. >> Yeah.</p>
<p>>> Yeah. >> Yeah. I think we also the AI in here benefits a lot from everything that we've learned building codecs. >> Um and as we go forward I think we'll likely just integrate the the full codeex harness into the application here. So you get all the benefits of the tools and the skills and all the things that Codex can do today. >> Uh and you just sort of automatically can bring that into your environment here. >> Yeah. Is there a future they're just the same app? Uh maybe [clears throat]</p>
<p><strong>[18:18]</strong></p>
<p>I think potentially it depends on I mean here's the reason I'm hesitating is I think the the interesting thing with um with this and with codeex is we're still mostly in a world today where people are you have you know your your main screen is your is your document and then you have your AI on the side but the more that AI that improves people trust it and they're just yoloing it right? You're like you're generating code and you're you're like looking at the code is sort of secondary to instructing the AI and and</p>
<p>driving from that. The UI probably changes for all of these things, right? You don't you don't need your your document front and center because you're actually not looking at your document as much. >> You're that's sort of your backup and your interaction with your AI is primary. >> And as that happens, I think you might these these UIs can kind of converge over time. So, we'll see. Uh, but I definitely would love to see a world where people needed to spend less time thinking about the actual syntax and much more about what they're trying to</p>
<p>create. >> Yeah. I mean, I I I feel like this plus a notebook would be amazing. Yeah. >> Because because you and something that uh the AI can run equal, generate plots. Oh, stick that in the paper here. like oh read you know like this paper like this part of the paper like take that equation and like you know do something with it that would be a really amazing uh integration. >> Yeah like think through the different corlaries [clears throat] of this thing from this paper and produce some</p>
<p>alternatives and then like yeah I I completely agree. >> Yeah. >> Yeah. I do think that's sort of the progression where it's like doing doing maybe work for a few seconds versus maybe we're already at a point where it's doing work for a few minutes eventually doing work for hours, days, coming back with very complicated analysis. >> Mhm. Yeah. I mean that that's actually maybe a good segue into some of the other questions that I had about your um your initiative. I mean uh so stepping back to AI for science in general um can you talk a little bit I I have a million</p>
<p>questions but uh maybe start with what I okay I feel that validation of AI for science so uh is critical to its success right you have to have some sort of um real world validation of of the results that you produce with your AI right so what are the I I know that there's been some publicity in the in the past. What are the like the latest and greatest hits of the things that big labs or any</p>
<p>lab is doing with uh with open AIS AIS? >> Uh I mean when you step back and look at the trend I think that's the biggest thing because we can we can debate exactly like you've probably seen in in the last few weeks even there have been a bunch of different examples of like GPT 5.2 to contributing to open airish problems and things like that. And then</p>
<p><strong>[21:19]</strong></p>
<p>you get into this debate of well was it uh was it really just really good at literature search and it found an example over here an example over here when you combine the two you know that it was sort of a trivial step from there to the solution and >> was that novel or did it really do something new and >> you know that's a that it's a legitimate discussion but when you step back >> two years ago we were like you know this thing can pass the SAT that's amazing and uh [laughter] and you progress to like it can do a little bit of contest</p>
<p>math and it can start to solve harder problems. Wow. And then you keep going and it's starting to solve graduate level problems and then you have a model that gets a gold medal at the IMO and now we're sitting here talking about you know it solving open problems at the frontier of math and physics and biology and other fields. So it it's just I mean the progression is incredible. And if you think about where we are today, then you fast forward 6 months, 12 months, like I I I'm very optimistic about what</p>
<p>the models are going to be do able to do to accelerate science. It's like it's already happening. And if there's one thing that I've learned from uh my like two-ish years at OpenAI, >> it's you go very quickly from this is this thing is just impossible for AI to do. like [snorts] it's too hard, AI can't do it to like AI can just barely do it and it like kind of doesn't work and you know only early adopters are doing it because it's not particularly reliable yet but it like</p>
<p>sort of works to oh my god AI like does this thing really well and I could never imagine not using AI for this in the future. It's like once you start to get to, you know, 5 10% on some particular eval, you very quickly go to like 60 70 80 >> and we're just at the phase where AI can help in some, not all, but in some elements of frontier science, math, you know, biology, chemistry, etc. And it it just means we're like right at the at</p>
<p>the cusp and it's super exciting. So I mean so it it fast forward a year or you know the end of the year uh and we have AIs that can do you a lot of this um discovery process then the bottleneck becomes the wet lab or the the lab right so what what what is what are you seeing um in that domain? Yeah, I I by the way I totally we were talking a little bit about software engineering before and the analogies. I think 2026 for AI and science is going to look a lot like what</p>
<p>2025 looked like for soft AI and software engineering. >> Yeah. >> Where if you go back to the beginning of 2025 if you were using AI heavily to write your code, >> you were sort of an early adopter and it like kind of worked and but it wasn't like certainly not everybody was doing it. And then you fast forward 12 months and at the end of 2025, if you are not using AI to write a lot of your code, you're probably falling behind. I think</p>
<p><strong>[24:20]</strong></p>
<p>we're going to see that same kind of uh of progression in AI and science. You know, today it's a early adopters, but you're really starting to see some proof points and solving open problems and, you know, developing new kinds of proteins and things like that. Um, but you're right, as it as it really starts to work, and I think this is the year that it's really going to start to work, uh, it it shifts the bottleneck, and I think we're going to be starting to talk a lot more about robotic labs and other things,</p>
<p>>> you know, [clears throat] like do you need to have a grad student like pipetting things? >> No. >> Probably not, right? right now you do, but why why shouldn't we have why shouldn't we have uh robotic labs that where you have AI models doing what they do best reasoning over a huge amount of of different information um you know they have read substantially every paper in every field and can bring a lot of information to bear to help prune the search tree on you know a new material for example that you're trying to create</p>
<p>and then you have a robotic lab that can uh roll out a bunch of experiments in parallel do them while we sleep. Um, and then feed the results back into the AI, let it learn from them, design the next set of experiments and go. >> I mean, it's hard to imagine that's like >> it doesn't even have to be YOLO science, right? To your point, it's you're verifying it as you go cuz you have an actual lab building it in real life. >> Um, but you can just do so much more in parallel. You can think harder up front with AI to design the experiments. uh and and again like prune the search tree</p>
<p>so you're you're searching over a smaller number of higher value targets and then you automate the experimentation uh and and turn it around faster. And again like this is acceleration like the whole if we're successful then it's you end up doing you know maybe the next 25 years of science in 5 years instead. So in 2030 we could be doing 2050 level science and that would be an awesome outcome. like the world is a better place if that happens. >> Absolutely. I I guess uh so we spoke</p>
<p>recently with Heather Kulik at MIT and one of the things she pointed out was that there's a element of serendipity to working in a lab that you lose and so she was of the opinion that there's a class of problems especially when you have like a large search space or something like that where robotics is going to really accelerate science and there's another class of problems where even experimental science will not move forward very fast because of robotics. And so then again, you're at a bottleneck. But I guess humans need something to do. So</p>
<p>>> well, that [laughter] what she said sounds totally reasonable to me, right? There are probably places where the humans are adding no value because they're literally just trying to pipet a certain amount of a thing into another thing or, you know, do some uh the same motion repeatedly in a bunch of different ways. And then there are places where it it's less well understood. You want the full flexibility that you have of a really smart human thinking about the work that they're doing. Um, by the way, the same</p>
<p><strong>[27:21]</strong></p>
<p>is true in in uh the more theoretical fields as well. It's not this isn't about let's automate all the humans out of their jobs. This is about accelerating scientists. It's scientist plus AI together being better than scientist alone or AI alone. >> Um, and I think the same is true whether you're talking something that's happening in silic proving a theoretical problem or happening in the real world with a lab. like find the parts that you don't need a human to do and try and automate them as much as you possibly can so that the humans can spend their time on the most valuable things.</p>
<p>>> Yeah, I'm very pro like the incilical uh acceleration because obviously you have more control over that and you can parallelize and repeat and >> yeah do all those all those things. Yeah, I think there will be a huge amount of value in you know a lot of fields are are heavily simulatable and they you know and so you know nuclear fusion for example they're running a lot of simulations before they do any particular experiment because the experiments are very timeconuming and expensive. >> Yeah. Um, but I'm excited to see what you can do when you have a a loop between, you know, a a very intelligent</p>
<p>reasoning model that understands fusion and a simulation and you get the model thinking about what parameters to set for the simulation and then running, you know, a bunch of simulations in parallel, feeding that back and you have that same sort of lab loop except it's all in silicon and in an exper in um and running on a giant GPU cluster. >> Yeah. And then when you really have like gotten to the end of that calculation, then you go run it in >> IRL. This is bringing it back to Prism. This is sort of a a nice aspect that</p>
<p>you're you're getting a more sophisticated view of your result, right? instead of just um you know like a chat output and it I would I would hope as it develops it's a way for a scientist to be able to interact with the information before you kick off your nuclear fusion experiment for you know $10 million or whatever. >> Mhm. And the human can learn from more things right you just you get more data that you can that you can look at and evaluate. Oh yeah, >> this by the way this fusion discussion</p>
<p>makes me think like you know if one day open for science you know it gets serious enough and starts to self accelerate you should solve cold fusion and you know be your own power source. [laughter] >> Well I mean this is this is why we're so excited about this, right? I mean imagine our our our mission is is to you know to to bring AGI to the world in a way that's beneficial to all humanity. >> It's right there at the lobby. >> Yeah. You see it every day you walk in, you see it. >> Yeah, absolutely. And and imagine I mean if we had GPT9 inside of</p>
<p>chat GPT today, it would be awesome. You could do lots of things, but if you had GPT9 and it could um which I'm using as a standin for AGI, right? And and it could >> create new materials and we were the devices we were using were all incredible and you know had 30-day battery lives and things like that. and we had personalized medicine and we all</p>
<p><strong>[30:21]</strong></p>
<p>knew someone whose life was saved because we were developing personalized, you know, >> uh, cancer treatments and things so much faster. Like >> that's the real benefit of AGI. That's I think maybe the most tangible way that we're all going to feel AGI as it starts to be real. Yeah. >> And that's why this work is so mission driven for us. >> So, so that does it brings up like kind of two questions in my mind. One is the first one is so then who uh who owns the invention and [clears throat] then the other half of that is okay so then does does open AI become a drug company and a</p>
<p>fusion company and and right cuz this is how I mean you laugh but it's a little bit serious that all the AI for drug discovery companies ended up being drug companies because they couldn't sell the so far with some exceptions now with no edetic for example but they end up being drug companies because they can't sell the the drug. But in any event that there's like a lot of precedence for using basically building your own portfolio using uh AI. So like are you thinking</p>
<p>about that that angle or this is right now you're just let's get what's enabled scientists for outside of open AI? Yeah. I mean, my my personal belief about as we drive towards AGI is not that we're going to we're going to create AGI and then we're all going to like sit back and enjoy our universal basic income and like write poetry. I we're people the future will involve [snorts] I mean especially advanced science is going to involve experts helping to drive these</p>
<p>models and I don't believe that any one company is just going to do everything right. It's why we're focusing on first and foremost on accelerating scientists outside of these walls, right? Our goal is not to win a Nobel Prize ourselves. It is for a 100 scientists to win Nobel prizes using our technology. >> Yeah. And at the same time, I think there are there are like places where sometimes you actually when you're trying to build for other people, you learn best if you actually try and go end to end on something >> because then you're your own customer</p>
<p>and you get you understand it in a tighter loop than you would if you were purely building for people outside the walls. >> So I think it makes sense for us to take a handful of bets like that. But >> by and large, we're going to partner because the the surface area of science is massive. Yeah. >> And we want to accelerate all of science. >> Yeah. >> Yeah. We're covering all sorts of disciplines from like chemistry to we are >> structural biology [laughter] >> and we're we're releasing the first first episode this week. So >> material science it's all over the place. Uh it's there's a lot to do. uh one thing I did wanted to bring across</p>
<p>also was so afraid sits within the broader sort of research uh org at open AAI and you know one of the more more interesting things is like self acceleration let's call it you know >> [clears throat] >> um where yakub has very publicly declared that we'll have a automated researcher by September 2026 >> yeah the beginnings of what I think you said right and it's like the intern</p>
<p><strong>[33:22]</strong></p>
<p>version this year >> first product uh and I'm sure you have more cooking internally but like why so soon like that's 8 months away and uh what's the what's the goal there? What you know just anything above that that you can share? >> Yeah, I mean 8 months that feels like forever in this industry [laughter] >> basically infinite time. Um I mean no it's exactly what you said right? It's if uh if we can if we can create a a a model an AI researcher that is um that can actually do novel AI research then</p>
<p>we can move way faster right we will we will self- accelerate uh we can discover more things quickly we can apply GPUs and compute to to moving our own research faster and that just means that we can improve our models at a faster rate and every bit that we improve our models means that we We're a step closer to bringing AGI and all the things that we were talking about with personalized medicine and new materials and like we can bring these amazing things into the world faster. >> So it is about self acceleration. >> Yeah. I think one one thing I'm also</p>
<p>trying to figure out is how closely is machine learning research which is a science. >> Yeah. uh or high performance compute which is also something that you guys are doing a lot of uh close to the traditional hard sciences let's call it like physics and chemistry >> I think in a lot of ways it's it's sort of a parallel effort to this like it is the work that we're trying to do with AI open AI for science and accelerating other scientists the parallel internally is they're trying to uh build products and models for AI researchers to</p>
<p>accelerate them so it's there there's a lot of sort of uh parallelism to these two work streams. They're they're similar in in uh in in goal just for a different set of users. >> Yeah. Okay. Um any parting thoughts, questions, anything we should have asked? >> Uh well, I hope everybody tries Prism. It's it's available today at prism.openai.com. It's totally free. You log in with your chat GPT account and you can go build anything you would like. We're really</p>
<p>excited to see what people use it for and um if if you run into issues or have any feedback, let us know. >> I have a paper I'm going to write [laughter] really really soon on that. >> Amazing. Show notes in this thing. I don't know. Let's let's see what it does in Latte. >> Yeah, [laughter] totally. >> Yeah. Congrats on your first Open AI launch. >> There you go. >> Congratulations. >> Congrats. Thanks for having us. >> Yeah. Thank you.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=W2cBTVr8nxU</guid>
      <pubDate>Tue, 27 Jan 2026 19:36:10 +0000</pubDate>
    </item>
    <item>
      <title>🔬 From Red Teaming GPT-4 to Automating Drug Discovery: The Future of AI in Science — Andrew White</title>
      <link>https://www.youtube.com/watch?v=XqoBSB3nsgw</link>
      <description>_Editor’s note: Welcome to our new AI for Science pod, with your new hosts RJ and Brandon! See the writeup on __Latent.Space_ (http://Latent.Space)_ for more details on why we’re launching 2 new pods this year. RJ Honicky is a co-founder and CTO at MiraOmics (_https://miraomics.bio/),_ building AI models and services for single cell, spatial transcriptomics and pathology slide analysis. Brandon Anderson builds AI systems for RNA drug discovery at Atomic AI (_https://atomic.ai)._ Anything said on this podcast is his personal take — not Atomic’s._
—-
From building molecular dynamics simulations at the University of Washington to red-teaming *GPT-4* for chemistry applications and co-founding *Future House* (a focused research organization) and *Edison Scientific* (a venture-backed startup automating science at scale)—*Andrew White* has spent the last five years living through the full arc of AI's transformation of scientific discovery, from *ChemCrow* (the first Chemistry LLM agent) triggering White House briefings and three-letter agency meetings, to shipping *Kosmos,* an end-to-end autonomous research system that generates hypotheses, runs experiments, analyzes data, and updates its *world model* to accelerate the scientific method itself.
* The *ChemCrow story:* GPT-4 + React + cloud lab automation, released March 2023, set off a storm of anxiety about AI-accelerated bioweapons/chemical weapons, led to a White House briefing (Jake Sullivan presented the paper to the president in a 30-minute block), and meetings with three-letter agencies asking "how does this change breakout time for nuclear weapons research?"
* Why *scientific taste is the frontier:* RLHF on hypotheses didn't work (humans pay attention to tone, actionability, and specific facts, not "if this hypothesis is true/false, how does it change the world?"), so they shifted to end-to-end feedback loops where humans click/download discoveries and that signal rolls up to hypothesis quality
* *Cosmos:* the full scientific agent with a *world model* (distilled memory system, like a Git repo for scientific knowledge) that iterates on hypotheses via literature search, data analysis, and experiment design—built by Ludo after weeks of failed attempts, the breakthrough was putting data analysis in the loop (literature alone didn't work)
* Why *molecular dynamics and DFT are overrated:* "MD and DFT have consumed an enormous number of PhDs at the altar of beautiful simulation, but they don't model the world correctly—you simulate water at 330 Kelvin to get room temperature, you overfit to validation data with GGA/B3LYP functionals, and real catalysts (grain boundaries, dopants) are too complicated for DFT"
* The *AlphaFold vs. DE Shaw Research* counterfactual: DE Shaw built custom silicon, taped out chips with MD algorithms burned in, ran MD at massive scale in a special room in Times Square, and David Shaw flew in by helicopter to present—Andrew thought protein folding would require special machines to fold one protein per day, then AlphaFold solved it in Google Colab on a desktop GPU
* The *E3 Zero reward hacking saga:* trained a model to generate molecules with specific atom counts (verifiable reward), but it kept exploiting loopholes, then a Nature paper came out that year proving six-nitrogen compounds _are_ possible under extreme conditions, then it started adding nitrogen gas (purchasable, doesn't participate in reactions), then acid-base chemistry to move one atom, and Andrew ended up "building a ridiculous catalog of purchasable compounds in a Bloom filter" to close the loop

Andrew White

* FutureHouse: http://futurehouse.org/
* Edison Scientific: http://edisonscientific.com/
* X: https://x.com/andrewwhite01
* Cosmos paper: https://futurediscovery.org/cosmos

00:00:00 Introduction: Andrew White on Automating Science with Future House and Edison Scientific
00:02:22 The Academic to Startup Journey: Red Teaming GPT-4 and the ChemCrow Paper
00:11:35 Future House Origins: The FRO Model and Mission to Automate Science
00:12:32 Resigning Tenure: Why Leave Academia for AI Science
00:15:54 What Does 'Automating Science' Actually Mean?
00:17:30 The Lab-in-the-Loop Bottleneck: Why Intelligence Isn't Enough
00:18:39 Scientific Taste and Human Preferences: The 52% Agreement Problem
00:20:05 Paper QA, Robin, and the Road to Cosmos
00:21:57 World Models as Scientific Memory: The GitHub Analogy
00:40:20 The Bitter Lesson for Biology: Why Molecular Dynamics and DFT Are Overrated
00:43:22 AlphaFold's Shock: When First Principles Lost to Machine Learning
00:46:25 Enumeration and Filtration: How AI Scientists Generate Hypotheses
00:48:15 CBRN Safety and Dual-Use AI: Lessons from Red Teaming
01:00:40 The Future of Chemistry is Language: Multimodal Debate
01:08:15 Ether Zero: The Hilarious Reward Hacking Adventures
01:10:12 Will Scientists Be Displaced? Jevons Paradox and Infinite Discovery
01:13:46 Cosmos in Practice: Open Access and Enterprise Partnerships</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>MD was supposed to be the protein folding solution. There is a great counter example. The counterfactual is basically a group called Desha Research. They had, you know, similar funding to Deepmind. Um, probably more actually. They tested the hypothesis to death that MD could fold proteins. They built their own silicon. They built their own clusters. They had them taped out all themselves. They burned into the silicon the algorithms to run MD. They ran MD at huge speeds, huge scales. I remember David Shaw came to a conference once on</p>
<p>MD and he flew in by helicopter and was like to this pretty famous guy, kind of rich >> and um he he gave an amazing presentation about the special computers and special room and out outside of Time Square and like what they can do with it. It was beautiful, amazing. And I always thought that protein folding would be solved by them, but it would require a special machine. And maybe the government would buy like five of these things and we could fold, you know, maybe one protein a day or two proteins a day. And when AlphaFold came out and it's like you can do it in Google Collab, you know, or on a GPU or</p>
<p>desktop, it was so mind-blowing. I forget like that protein folding was solved. I always thought that was inevitable, but the fact that it was solved and on like your desktop you can do it was just completely floored. Changed everything. >> This is the first episode of the new AI for science podcast on the LAN space network. I'm Brandon. I work on RNA therapeutics using machine learning at atomic AI. >> My name is R.J. Haniki. I'm the co-founder of Mirror Omix where we build spatial transcripttoics AI models. >> The point of this podcast is to bring together AI engineers and scientists or</p>
<p>bring together the two communities. These are two communities which have been developed independently for quite some time but there's been some attempt to combine them and only now after you know many years are we starting to see some of the big developments start to play out in the real world and start to solve you know key scientific problems. There's no like oneizefits-all solution. You need domain expertise. You need people on both sides of the aisle who can really talk to each other and really work together and understand both the</p>
<p>modeling and all of the real subtleties of the system you're actually trying to work on. We hope that we can connect these communities and that we can provide a starting point for this new era of AI and science to move forward. >> So without further ado, let's get started on the first podcast. We're really happy to have in the studio today Andrew White, co-founder of Future House and newly formed startup Edison Scientific. Um, rather than introduce him, I'll let him introduce himself. Uh,</p>
<p>hi, I'm Andrew from San Francisco, former professor now running two startups. Uh, one that's a nonprofit research lab and one that's a for-profit ventureback company. And we're trying to automate science. >> We're going to get into all those points. >> Yeah. I'm really happy to be here. Thanks for having me on. >> I want to know personally about jump from academia to industry and or quasi industry. So like I would love to hear</p>
<p><strong>[03:01]</strong></p>
<p>that story. >> Yes. Um I guess like that's the whole story, right? Uh so I did my PhD at University of Washington and uh I worked in a group with um I think 19 people doing experiments and like two people doing simulations and I was working on a topic uh called molecular dynamics um which I think is actually suddenly becoming interesting again as everyone's looking for ways to generate data from first principal simulation and molecular dynamics you know covers uh basically everything that's molecules moving</p>
<p>around in dynamic systems. So like biology, things like that. Of course, the complement in material science is things like density functional theory where you can model chemical reactions and these like solid systems. So I was working on that. We work on biomeaterials. And so the goal of my my PhD was trying to find what are called non-falling materials. So in biological systems whenever you put like a a foreign object into the body, it will trigger a response. And that response called the foreign body response. Basically it encapsulates it in like this um layer of collagen. This actually is exploited for some implants like if</p>
<p>you get a heart uh sorry pacemaker installed like it coats it with this collagen so that if you go to change the battery you can almost change the battery out like without even bleeding because like the body has like completely encased it and this is great for pacemakers but for like a glucose sensor or like a you know brain cognitive interface BCI is what they call it now. Yeah. there that's not so great. And so that's why some of those things have like a limited lifetime because eventually your body treats it as like a wound and and heals and >> rejects it. >> Yeah. It's it's kind of like so some</p>
<p>rejection is like immune based. >> Okay. >> And so that's where like if the body can see anything on it like if it can see like some some um uh lian that it can bind to with antibodies then you get this like inflammation which is like an rejection response you see in organ transplants. with materials the body just like oh there's just like a wound or there's some something here and it just covers it up. >> I think you know the research in that field has gone on a long time since I left my PhD >> and there was a lot of theories about it's related to the mechanical properties of the material like if it's</p>
<p>spongy there's things like if it's tracular like it's have a bunch of little pores in it. We worked on the theory that it had to do with how hydrophilic the material was. >> Okay. >> But anyway so I was only one working on computers in this group. I couldn't figure out like how to connect what's on the computer with what's done in the lab because you can make like a simulation of whatever 10,000 part 10,000 particles 10,000 atoms. It's like well this is not going to model a human body and an implant a lot more a lot more atoms involved. >> So I had a good time. We did some cool stuff some bopromatic stuff. I learned a</p>
<p>lot. But then when I did my postto I was like okay we're going to try to merge experiments and and simulations. So I worked on this this theory called um maximum entropy. And it's about like how do you take complex simulations and match them to limited observations? And it's like the inverse of machine learning. Machine learning is like you have simple models. You're making going to a lot of data where I had like complicated models and trying to fit to very little data. >> Yeah. >> It was fine. It's great. We wrote some</p>
<p><strong>[06:01]</strong></p>
<p>papers. It was useful. And then I wrote like I started my research group at University of Rochester on applying these methods to model peptides. >> Yeah. >> And I'm always like too early for things. We study peptides for I don't know four or five years. And it was a cool niche field. not that popular. Now, peptides are like the hottest thing ever. I think there's even like a peptide rave I heard about a couple weeks ago. >> But, but when I was an assistant professor, nobody cared about peptides. Um, so we worked a lot on different ways to combine them. We looked at like uh different experimental methods that we</p>
<p>could do these molecular dynamic simulations of peptides. And then in 2019, I was like out on a sbatical at at UCLA. um they have a place called the Institute of Pure and Applied Mathematics there which is like this institute where people can go and do a sbatical and learn new methods and they were happen to be doing they happen to be doing um machine learning for physics I think the name of it was it was like some like symmetric thing it's like machine learning for physics and physics of machine learning >> okay >> it's kind of a cool cool concept >> but like Yan Lun was there and like um Frank Noi was there who's a big guy in</p>
<p>Europe in in this field I don't know it's just Terrence Tao came by great group >> and everyone was kind of jamming. It was like 2019 so like there had not really been the big hit um especially in in in uh in non computer science fields >> right >> and then I came back from that I was like well I got to teach class on this writing the book about like how you can apply these methods in chemistry it was very kind of niche field because uh every machine learning class that my PhD students could take at the time this is when I was professor at University of Rochester it was always would end in like okay this is an RNN and this is</p>
<p>like what you need to know or like this is how you do image classification In chemistry, it's all about graphs, right? It's all about how do you represent these graph structures? It's all about symmetry and geometry. >> Y >> and that was like not a thing was very popular, but you had Maxwelling on before and you know the the godfather of of geometric deep learning. >> Yeah. >> So I wrote this textbook about like this these methods and there was a bunch of interesting mathematics to it. I had a good time and stuff and then um uh I think I was following you know the news in the space and codeex the original</p>
<p>codeex came out and um I had been looking at transformers for a while I just tinkered with them and we started trying them on doing some chemistry tasks and we were really impressed actually and we wrote a benchmark and this is like 2019 or something we wrote a benchmark of verifiable rewards in 2019 maybe it's 2020 by then but like >> ahead of the curve >> a littlehead of the curve again yeah like here's a here's a function and um Sorry, there's like a task which is like I have a body of a function for like a marov chain Monte Carlo simulation it's missing some pieces complete it and then</p>
<p>we had like a verifier that would see did is it a valid MCMC simulation we wrote this paper it ended up coming out I think 2021 2022 >> um because it took a long time to bank enough questions >> but I wrote an opinion piece about how transformers could change um how we think about chemistry and things like this and how we teach it and then opening eye some people there um Llama uh was there. She saw this paper and</p>
<p><strong>[09:03]</strong></p>
<p>they reached out like, "Hey, we're building this new model and we think it'd be great to red team it to see like what could happen with these models if they're applied to chemistry or biology." And so I was a redteamer for GPT4 and I was using it like nine months or something before release was like August. So GP4 came out in March and I was using in August. >> Yeah. And then like the react and miracle paper came out. Um I think Shenu he wrote that paper and I plugged it in with GP4 like in the fall you know and I was like wow there's so much stuff coming out >> with React. >> Yeah. And it was really exciting. And then so when GP4 came out I released this paper called Chemcrow. I worked</p>
<p>with Philipe Schaller in in um Switzerland on this and IBM. >> So that was like React applied to chemistry. >> Yeah. And what we had is we had like there was a cloud lab that IBM built in Switzerland. Yeah. So we had like GBD4 operating the cloud lab and then it was like I had written a literature research agent that did like agentic rag. Again nobody knew what agent rag was at the time. Um I think actually uh Harrison Chase had like written a blog post about some ideas there and so I I I stole some of his ideas. Really smart guy. Um and basically we applied that and</p>
<p>>> we saw some really cool stuff. It was really exciting and then I we wrote the paper. It set off this crazy storm of like everyone was had a lot of anxiety about AI progress. >> Yeah. >> And um I ended up visiting the White House. Um I guess my my paper was like the only time a pre-print or peer review paper was presented to the president on like their schedule for like a 30-minute block. >> Wow. >> And the national security adviser at the time um uh Jake uh God I was confused about</p>
<p>>> Yeah. No, sorry. One of them is a talk show host and one of them is like the national security adviser. I forget which which is which. >> Um >> that guy. >> That guy. Yeah. He like had a presentation about our our our paper and they presented to all because there was a big tech CEO summit at this time where they sent out Sam Alman and some other CEOs out there and they >> this is the future of chemistry as language or a different one. >> This is the Chem Crow paper. Sorry. I probably should name these things. >> And and so >> it was crazy and they had me go out there and then I I met a lot of threeletter agencies I didn't really want to meet. And um</p>
<p>>> and like you know this like some somebody from uh one threeletter agencies like how does this change explosives? You know the three liter agencies like how does it change breakout time for nuclear weapons research of >> I was like Kai is like I don't >> I'm not really sure and but it turns out that like you know there's not that many people world experts on on on AI and science, >> right? So what's the answer? Uh yeah, agree. Um good question. We'll come back to that. Uh >> okay. Yeah, let's come back to that. >> In the end, um I I you know had a lot of energy, a lot a lot of excitement about</p>
<p>this area. So took a sbatical from University of Rochester. It was Sam Rodriguez and um Sam had been talking to Eric Schmidt and Tom Khalil who was um also National Security Council on the Obama administration about how to uh uh like scale up these ideas. And so Sam had this concept of like focused research organizations which was how do you do science like not in academia, not in in a um in like one of these kind of</p>
<p><strong>[12:05]</strong></p>
<p>near monopoly tech companies, these big labs >> and uh wanted to try this idea out and I was like hey we should do this around agents for science or AI for science. >> I love Sam. He pushes me to come up you know with really >> lofty ambitions. So we decided to automate science as the goal instead of like see what fun stuff we could do with agents in science. But I think that was maybe the real mission. But of course, automating science is the the long-term mission. >> Yes. >> And so that was what led to future house. >> That was a very long- winded. >> Yeah. No, no, that's great. So, but so you chose to leave a tenure track position.</p>
<p>>> I was on sbatical, which is a a beautiful concept. Um, but then I did resign my tenure position when we co-founded Edison. >> Yeah. Okay. >> And it was, you know, I had been on sabbatical for a very long period of time and so at a certain point I just had to resign my tenure. So I resigned tenure in June. Okay. >> Oh, so that's only recently. >> Yeah. Only recently. >> Yeah. And you just felt like this is this is direction of my career. I >> Yeah. Yeah. I mean, I got tenure and I had these early career awards like the NSF career award. It was great. And I</p>
<p>think academia is really exciting. But um I just thought that right now these this kind of area like a ever science is um just I think a difficult to do in academia and b so exciting that I think you can take bigger bets >> and um I think having a tenure position and writing research grants is maybe not the biggest bet you can take on on a field. >> Yeah. So now we have a ventureback startup called Edison, right? >> Which we spun out of Future House and you know we took a lot of the ideas and and we're trying to do this at an even</p>
<p>bigger scale right now. >> Yeah. >> And and so Edison was always kind of the plan like going back to Sam's idea of a FRO or like was it fundamental research organization like he he always had this goal of like let's do fundamental research and this like tightly scoped nonprofit which can kind of explore and then you have that as a natural arm for spinning off. Yeah. >> Um, you know, venture back. >> Yeah, I think that's right. Um, I think some things that like make that not as clean these days is how expensive AI</p>
<p>research is and how expensive GPUs are. So, I don't think we can repeat it many times from future house. It might be like an end of one thing right now. It just may maybe not. I don't know if venture capital keeps growing then maybe maybe we can. But yeah, I think we took a lot of the ideas of future. Another thing like I think we expected it to be harder to automate science >> and actually it's really hard like I'm I'm feel like I'm always miscalibrated in this domain but it's always hard to predict progress. >> Yeah. >> Um and I think that I overestimate the speed of things on month scale and I</p>
<p>underestimate things on year scale. So the two years from like 2023 to 2025 was an enormous amount of progress. Yeah. Um it always felt like things were not going as fast as I thought, but when you look back on it, wow, like there's a lot of progress. And so I think the idea that I think in future house and our Sam actually regrets us writing this, but in the the original marketing or like the announcement is like it's our 10-year mission automated science and like now</p>
<p><strong>[15:05]</strong></p>
<p>it's like okay yeah. So two years later we we had Cosmos and um things are going so much faster and it also this kind of this this thing you notice it in um in in San Francisco. is like it's actually kind of hard to find problems which are like so hard that like they are a challenge for language models but not so hard that they're impossible and this like gray zone and actually I feel like that's where we are right now is that we can actually automate so much of the scientific method because it turns out especially in a field like biology which is very empirical limited you know the top 1% guesser of know what what they</p>
<p>think will happen in an experiment in the you know the top you know quintile or cortile they're about equal and so you know even if you had an If we wait 10 years and get even smarter models, I don't think it's going to really change the fact that we're ready to automate a lot of science with with existing LLMs. >> You mean what do you mean by automate science? That's like a pretty loaded state. These lots of things like there's way many ways of thinking about that. >> So we try to draw a line between um what I would call uh uh groups that are trying to model something like the cell or how proteins fold or like how</p>
<p>antibodies can be designed or like maybe the virtual cells like an example. If they're trying to use machine learning or AI to model some very specific system, >> we're trying to automate the like cognitive process of scientific discovery, making hypotheses, choosing experiments to do, analyzing the results of experiments and using it to update your hypothesis or your confidence in those hypotheses and then leading to like a world model which of like okay this is how I understand this process to be and then that you know begets new hypotheses or new experiments. So we want to automate that sort of loop. We</p>
<p>thought that we would have to build up like a whole new organization from the ground up for agents. So it means like automated labs, it means like putting all the papers in one spot, like getting APIs wrapped around everything. But um over time like the models have gotten better and better that we had to like you know stop and rethink okay we don't actually have to hold their hands so much anymore or like they don't actually need to necessarily have an automated lab. They can like write an email to a CRO or something or they can like tell you what experiment to do and you can take a video of you doing it and then show it to the model and they can be like okay well this is you know what happened. So it's been a really</p>
<p>interesting experience of like sometimes you know we overengineer things and sometimes uh uh actually basically just mostly overengineer. So >> so I always think about systems and scientific is a system like scientific process as a system. I always think of it in terms of constraints, right? And like what is a bottleneck in the system? And so that so what is your hypothesis about this right like in my mind not knowing a ton but in my mind the constraint of the scientific process is the work you do in the lab and that's</p>
<p>sort of notably missing from well not entirely you said auto you mentioned automating lab and whatever so like how are you thinking about this? Yeah, I think you're right is that uh basically the the best model um whatever opus 7 or GPT10 like on yeah >> it really can only propose the first experiment maybe slightly more clever but at a certain point you just need information right like some little</p>
<p><strong>[18:06]</strong></p>
<p>calculations you can do that like there's more atoms in the brain than you could ever simulate even if you had all the energy from the sun right like I think you simulate maybe a thousand brains in real time with all the energy in the sun because there just too much information yeah >> so science really hits these like bottlenecks where you just actually have to go measure things. >> Y >> we definitely think about maybe like lab in the loop sort of situations like one of our papers which was called Robin is that we like had an one of our agents proposed an experiment. We did the experiment and then we had our agent analyze the experiment that proposed the next experiment and that kind of loop I think is where you want to get to.</p>
<p>>> Yeah. >> So what is the bottleneck in that? I don't think it's like the intelligence of the first experiment. I think the bottleneck might be something like right now. I think the bottleneck is something silly like knowing what's the lead time on all the reagents that you need and what is available in the lab, right? Like you know. >> Yeah. Yeah. Yeah. >> I think whether GPT 5.2 Codeex Max or Opus 4.5 is going to do better is probably doesn't matter. It's just a matter of like which one's going to have all the information of what's in the lab and how much will it cost, how long will it take, >> right? Um and also I guess the the kind of frontier that I think about for these</p>
<p>models is taste >> which is like a lot of science I mean of course we want to you know accelerate technology we want to improve the economy we want to improve people's life expecties want everyone to be happier but a lot of what is done in science is is based around like human preferences like why do people study I don't know a particular worm well like there is a theory that by studying the worm it has led to good medicines or it's led to discovering new genes But also people studied in the past. People's careers depend on that worm and people want to</p>
<p>write papers about that worm. And so there's a human element to some of this and I think the u models don't capture that so well about knowing what is an exciting result and what is a boring result. >> I see. >> So I think that's like scientific taste. It's like a broad category of all these things. >> How do you def like do you try to quantify taste in any way? I mean I know that I I have some like fun anecdotes about this but maybe Yeah. just like to hear what you thought. >> Yeah, actually we sat on this idea sat on it, but we like argued about it for a</p>
<p>long time. Sam and I usually every Monday morning at 8:00 in the morning, Sam and I meet and we're both, you know, caffeinated and ready and we argue about stuff like this. And we had a lot of Mondays where we talked about scientific taste. >> And in the end, we're like, okay, let's just let's just do the dumbest thing, which is to like have our agents make hypotheses and put them in front of humans and have them be like, I like this one or I like that one. Right? So we just did like whatever RHF on on on hypothesis and um we learned a lot about how bad RHF is with people just like people pay really attention to to the</p>
<p>tone to the details to like how many specific facts or figures from the hypothesis right like actionability about like if the experiment is feasible but what people didn't really pay attention to is like I don't know how to describe this like if this hypothesis is true how does it change the world if the hypothesis is false how does it change the world this like um how much information you gain. It's not really information but like impact or something >> and that really didn't come through from</p>
<p><strong>[21:06]</strong></p>
<p>those things. So then we're like okay well this is maybe one strategy and so we had to go back and think about it more and um we then took a pause from that research and then we made cosmos and then cosmos like has baked into it taste right like at the end of the day there will be some report and we're working on generalizing this basically at the end of like I made these discoveries and a person be like great I want to download that one or I like that one right I don't like this one and that rolls up to some hypothesis that came earlier in the process and so we think we can get to end to end on as opposed to human preferences.</p>
<p>>> So you mean the feedback loop is the click? >> It could be the click. It could be also like the you know it we do an experiment. Sometimes in cosmos you could ask to end an experiment. You go see was experiment a success or failure or something like but I guess like we we've brought it out of this kind of like hard to quantify is this a good hypothesis or a bad hypothesis and into this like you can see some downstream consequences of the hypothesis. So yeah, humans have I think a very like strongly well-c calibrated nose for science. Like I mean maybe you could argue there</p>
<p>sociological effects like um across the community, but ultimately often times people really like good scientists know right off the bat like is this going to be likely to be um useful or not. How long how many attempts did it take before you started to like see results that to yourself seemed useful? Like you've been working on this for I guess two years now. Um you know I think when the co-scientist paper came out from Google I think it was a really interesting idea to do this like</p>
<p>tournament style or just pair wise ranking of hypotheses right so I think cosign is very interesting counter example to what we built is that what we built is something with either lab in the loop or data analysis in the loop or literature research in the loop where you're like iterating on an idea I think co-scientists took this like very different approach of like let's list all the ideas and then try to come up with a filtration process to come up with the best hypothe hypothesis. So co-scientists will produce these very long reports of like oh we like really tested this idea with lots of dialogue and and and it was very interesting</p>
<p>stuff and I was really impressed with the paper that came out and then we had this Robin paper and one of the things that came out of the robin paper is that the hypothesis that people thought was best was not the one that led to success in that paper. >> Interesting. >> It was in um age related macular degeneration or ocular uh age related macular generation. Basically, it's like part of the eye is you're going blind because you have this like accumulation of debris in the eye and can't clear it out. >> That's the major cause of blindness in people over >> Yeah. >> Uh 60. Ali who works on the hill. >> Yeah. Yeah. He'll cringe when he hears</p>
<p>me say that, but >> something like that. >> Something like that. Sorry, Ollie. Um in that one like we went to optometrists or opthalmologists get confused on that as well. Sorry, Ollie. Um but essentially, you know, asked them what hypothesis do you think are are good hypotheses? which you think would lead to like um a good mechanism for treating dry AMD. >> Yeah. >> And um yeah, there was you know they agreed to be on the top 10 but but</p>
<p><strong>[24:07]</strong></p>
<p>beyond that it was it was kind of noise. >> Yeah. >> Um and then you know the the what we found was ribbudal was a very good um medicine and it had a mechanism that I think is novel although there was lots of debate on X because in I think in 2012 there was a master's thesis which proposed this mechanism on like page 38. I actually think it was a typo. I think they meant wet wet AMD. But anyway, I won't I won't belager the point. I will concede that it maybe was there was one reported uh example of it in the past. That was a really eye opening experience</p>
<p>for me because that was a the first really serious test where we really went to the lab and we you know spent like four weeks on a battery of experiments to see what is which hypothesis led to a good mechanism and a good repurposed drug. >> Right. >> And it was not as correlated with human >> opinions as I expected. >> Yeah. Yeah. And so since then I think that uh I have a lot more faith in these like um verifier in the loop kind of scenarios where you have either data analysis, literature search or you're running a unit test or whatever you you you're going and running the experiment.</p>
<p>Anything like that I think is is going to give you a higher signal than the sort of vagaries of like oh this is a higher opinion or we like this one better. >> Yeah. Max Maxwellian called it nature's computer. >> Yeah. It's like it's like like you have this computer cycle you're running and the nature is part of that computational >> cycle. I'm curious. So you said that there is a paper which maybe like could propose maybe propose where this um molecule came from but like do you have some way of interpreting or like understanding where that hypothesis</p>
<p>originated in the absence of that? >> Like is there a little thought train? >> Yeah. Yeah. Yeah. Actually, this something we pay really close attention to at Future House and at Edison was um Providence of like information. So, our first sort of like agent was was paper QA. Sorry about the name. Paper QA sounds like an email 7 that was an agent. >> It really does. >> Yeah. Um paper QA was uh has like every sentence that it outputs has a citation to a page, right? So, it's like a lot of providence and then we basically built along the philosophy for everything. So</p>
<p>Robin would which is the name of this I don't know workflow or something you can call it that led to this result on riputal being a good um therapeutic for dry MD >> it has like data analysis that goes shows you like which line Python code led to the result here and then that is like okay then goes to this other model which says well based on this literature finding and this result from the data analysis I believe this is the right thing but you know where does the original idea come from like going after these rock inhibitors which is the the mechanism for the the target was</p>
<p>basically enumeration and so this is like if you can't be smarter you can be I don't know you can try more times of course >> and I think that was like the theory of of the Robin paper was that we can put out a whole bunch of hypotheses and then we can filter them just like I think somebody how co-signist did is you go for a filtration process but the difference is that in co-scientist their filtration process was other LLM sort of ranking it with rubrics or like personas</p>
<p><strong>[27:08]</strong></p>
<p>and our filtration process was like literature search and data analysis this um like here's some data. Is it consistent with the data? Go see if anyone's discovered in the paper in the literature or if they've disproven it. >> And I think that's the easy way to succeed in in AI over humans is is you can try more ideas faster. >> Something I've I've heard people say and maybe I've experienced this in my own life. Um like sometimes hypotheses are kind of cheap especially in you know biology >> it's many ways actually easy to come up with what you think could be happening. And um it seems like to me verifying is</p>
<p>often times a big bottleneck in maybe the biggest bottleneck like if you have lots of hypotheses you know and it costs you know 1/100th of your runway to test each one of them or something you don't have any shots on goal. >> Yeah. >> Yeah. So how do you make sure that like you you are actually enriching for good hypothesis >> literature and data analysis right you know like do >> there was a time when we used um something called tiling trees and tiling tree is like a literal brute force method invented by Ed Bon Sam's PhD</p>
<p>advisor and basically the idea is okay I want to like accomplish X okay I could try these methods and then like once you pick I'm going to try this method then you split into like two different paths I'm going to use this method or not use this method. I'm using this method. I need to have like I don't know some kind of substrate. I'm going to try this substrate or this substrate or this substrate, right? And you can basically try to really like tile space of all the we tried some early experiments there and you're right, you run into this thing where some of the hypotheses come out just don't make any sense and like you are going to waste a ton of effort if you actually test them all. Nowadays,</p>
<p>I actually would argue that if you go to an LLM and you ask it to evaluate, you know, hypotheses, including some garbage ones, it will probably do as good of a job as an expert in the field and and filtering them out. That's not always the case. >> Yeah, I've actually seen that myself. >> Yeah. But there's a lot of gotchas, and I think people can miss those, but I think they're actually pretty good. And so, I'm not as worried about >> hypotheses that can fail fast by an expert looking at them. >> I think now the filtration process really happens in in in literature. And I think the filtration process happens in looking at like um you know like uh</p>
<p>biioank data or like you know what do we know from GW was or something know other sources of of existing data as much as you can draw upon. >> Yeah. So yeah and with regards to existing data um another like maybe contrarian uh take is that like oftentimes the the hardest part is just understanding the like the context of data and like where it comes from and how do you interpret it. I can also think from my own life multiple cases where you know the the data in some sense was like there and you had two</p>
<p>people who were both experts and very smart people who looked at it and drew very different interpretations and and in fact like when we were interviewing Heather Kulick she she had some fun stories about using LLM and um she would find that there would be raw data in a paper which wouldn't agree with the conclusions of the actual paper and it's straight from the paper it's not even like cross paper talk or something.</p>
<p><strong>[30:08]</strong></p>
<p>>> Man, I'm going to be a really boring interviewer and be like, "Yes, you're right." You know, like this is this is a hard question. >> Um I think, you know, to give you something concrete, um >> we have a a bioinformatics benchmark we call it big bench. And big bench is like we put it out, we've updated a few times. It's in it's in some Frontier um LLMs when they release their system card, they'll mention Big Bench. It's like one of the things they test on. >> Yeah. And um you know we're getting to 60% 70% correctness on Bixbench >> and um we found that actually we're at</p>
<p>the point where humans disagree at this level. Like humans only agree 70% of the analysis. And so it's true that like when it comes to analyzing data like humans do not agree 100% of the time. There is a certain amount of like choice that goes into it. And you know we we try to um so Edison is is a for-profit company. maybe like trying to sell uh some of this stuff to to companies and we'll go to some companies like oh we never impute data imputing data is bad like or you know whatever and like okay</p>
<p>well we'll have to change our agent so we don't impute data with them but then some other companies like oh yeah we impute data it makes everything easier right or uh and and you know you want to know what the real modern dark arts are that like AI resistant area of the world is like medicinal chemistry that is like the spot where like you know there's so much superstition >> oh yeah yeah everyone yeah everyone is like pseudo religious >> yeah exactly But you have to be to survive. I feel otherwise you get burnt out. >> But the religions never agree too. Two medicinal chemists will have completely different viewpoints about like a functional group. >> Yes. Exactly. And I remember this is</p>
<p>talking to somebody who worked at CRO and they're like, "Oh, whenever like company X orders anything, we never put boron on any of the compounds because they hate boron because there was one program that was killed because there was a boron, you know, somewhere in the core and it led to some toxic side effect. So no boron for this company. this company, they like love things to be florinated or something because they love think it's great for um the admit properties, right? And so there's like all this this this stuff where you reach the point where I don't know human bias level or human disagreement level and I think we're getting to that point in</p>
<p>data analysis. And so of course you will see then that if I take the broad data from a paper and I analyze it myself, I will get a different conclusion. One of the cool tricks you can do is this back to this brute force thing is that I can go to our agent and I can run it 100 times and I can take the consensus like analysis or I can say even if you make these three different choices in your data analysis you get the same conclusion right or this conclusion is somehow sensitive to those choices and then you can there's even like words like epistemic versus alitor uncertainty right it's like this is alitor which means like I think it's noise from the</p>
<p>data or this is epistemic uncertainty which means like I think there's some choices that are being made there's some differences that lead to the the disagreement. Anyway, there's like there's like a Donald Rumsfeld formulation of this as well, like the known unknowns and yeah, the alitor epistmic uh uh debate there. >> Interesting. This kind of digging into your cosmos a little bit. So I I glanced at the paper and one of the things that jumps out is that there were certain</p>
<p><strong>[33:10]</strong></p>
<p>class of problems for which uh it was only 50ome% accurate and oh yeah and can you talk a little bit about that and how that like okay so if I'm just raw getting 50% accurate answers and and then I'm going into the wet lab and being like okay try this and then it's like ah like the the stupid thing did told me to do a dump like how do you >> I would say first of all that 50% it's actually pretty good because it's rare that experiments in the lab are actually coin tosses, right? They're usually a lot more outcomes than >> you know than than binary. >> Yeah. Yeah. Sure. Okay. Yeah.</p>
<p>>> But but that particular number was uh human agreement in the interpretation of the results. Okay. >> And so we asked people to evaluate different aspects of Cosmos. We had them evaluate like the data analysis decisions. We had people ask to evaluate the literature like is these is do you agree with its finding in the literature? that number that was 50% that came from Cosmos's interpretation of uh some of the analysis. >> Yeah. >> So like it might go in literature and find this result and then it would say wow this is super exciting this is amazing or it might do data analysis</p>
<p>like this is a novel discovery really excited about it. >> Yeah. >> And then people would disagree that's actually not interesting or like I don't agree with the interpretation of it. >> So it's like picking bad problems maybe. >> Yeah. >> In the the the negative class. Yeah. And so I think it's like that that 52 or 55 whatever it is that's um interpretation. And so I agree. I think that's where like I was saying I think the frontier right now is scientific taste. Yeah. >> And so that's what we're working on right now is how do you get that interpretation to to match you step back and just introduce Cosmos from a high level. Yeah. Yeah. Um</p>
<p>>> I would actually be in even curious to hear starting from like Chem Crow and uh you know you have uh paper QA Avery Ether Zero. Yeah. Yeah, >> I'd like to hear a little bit of the the lineage and how those different decisions were made. What were the key learnings and how did you get to where you are now? >> Yeah. So, I could retcon and tell a really great story about how we arrived at Cosmos, but I will say that like to a large extent we just try a lot of stuff and sometimes it works and sometimes it doesn't. >> Okay.</p>
<p>>> You know, I I'll say that >> we're very I'm I'm a I'm a builder. Like I like to like build things piece by piece. I'm uh probably some fancy word for it, but I'm like a Lego guy or something. My vision was that we would make an agent that does this part of the scientific process, an agent that does this part of the scientific process, whatever. And so we had like, you know, um, Chem Crow, which is going to help us with setting up our medicinal chemistry work. We had protein crow, which we haven't released. I don't know if we will ever release, but protein crow is like designing proteins we might need for for some part of our workflows. Um, or we had a data</p>
<p>>> or that's a >> it's an agent. So LM plus tools. >> Okay. or we had ether zero was like okay we noticed that the frontier models can't work with molecules very well so let's make a a model with intuition for medicinal chemistry and that was what led to ether zero but then Sam actually really pushed on us to like let's just even do the whole thing you know let's just try to build an AI scientist let's just try the whole thing and that was what led to Robin and um Robin was like let's just take these agents we already have and we'll just put them in like a a workflow basically was like you could</p>
<p><strong>[36:11]</strong></p>
<p>express it in a concise Python file of like you know try a whole bunch of ideas then go see if they all filter through literature or if they've been disproven and then go like uh come up with experiments that you could do in a wet lab. >> Yeah. >> And this is our inventory list and then go analyze all the data then go back and repeat the process. Right. So that's like what Robin was and um then we like we came ac across Cosmos we're trying to like understand what is the process that Robin is is automating and it came from this idea of like a world model which is that when we first started Edison we were thinking like what do we what do we</p>
<p>want to change about this like what is new here >> and so we spent some time thinking about well the scientific process like what is actually going on like my brain which is that I have some understanding of of the world or the phenomena I've studying and that's my world model and then a lot of the actions I take are about trying update that world model and it's something that changes over time and so this is like this ability to change over time but it's also something that is practical like I can use it to make predictions about I know from this experiment this will happen that's why it's like a model and not just like you know uh uh memory or like a bunch of</p>
<p>like papers or something like that it's like it's supposed to operate in cosmos we tried this idea out and actually um uh uh Ludo uh who was the the uh first author on the paper we tried a whole bunch of ideas around world models And uh we kind of thought they weren't really appropriate like well we tried a lot of different ways to do this. We tried you know method A, method B, method C and and they were okay. And so we all decided to take a break. Ludo like his project didn't work on trying to do this world model stuff. He's like I'm going to keep trying it. Lud is very</p>
<p>stubborn person. So he tried it for like I don't know a week or two weeks and he was kind of like quietly like hey can you guys come take a look at this >> and and we're like wow this is actually really cool and then we like started building on it and jamming really. And I think what Lud figured out is that you have to get this like experiment the loop thing. You have to be let it and the data analysis agent is what got us in the loop. So if you put that in the loop of like it can really update this world model because the we were trying to build it around literature before >> and when you build it around literature there's just like not really experiments you can do and then see the results for that was like our surrogate was was literature. It just wasn't working. Data</p>
<p>analysis actually really lets you explore ideas >> and so that was what led to cosmos. And so in cosmos we basically we had all the pieces sitting around. We working on world models. We working on a data analysis agent, working on a literature agent. Um, and then we're working on, you know, we built a platform for scientific agents. So, we had things that can write a law tech report. We had things that can make nice plots. >> Then we put that all together and like a world model was like sort of the the glue that allowed it to to fit together. Yeah. >> An analogy is like um >> in coding agents like GitHub is sort of the glue. like there's some shared repo</p>
<p>and everyone works on the repo and like software engineers have spent whatever lots of brain cycles thinking about what's the way to coordinate you know and organize working on code together for a long time. >> So the world model is actually like a memory system kind of. >> Yeah, you can think of it as a memory system. Um we we think about it as as a model. So like it actually you can put in input and it will output predictions and we think about calibration. >> But like really it is a set of like a big >> bundle of information that we accumulate</p>
<p><strong>[39:13]</strong></p>
<p>over time that's distilled in some way and and that is like uh what allows us to do this. And I think you can think about like um a GitHub repo is like it's a distillation right like really there's a long graph of commits that lead up to it and like the current file system in that GitHub repo or should keep saying GitHub. I'm such a corporate >> shill here. get your git repo is like a distillation of all of the work that people have put in into the PRs into the into the uh the commits and so I think there's a nice analogy uh between a git repo and what a world model is. I see. And</p>
<p>>> I think that's just sort of what allows us to automate scientific discovery is so well. >> Can you talk about like kind of how you implement a world model or is that sort of like secret sauce? >> That's our like secret sauce right now, you know? That's fine. >> Yeah. No, it's fine. People have asked around. >> So, one thing that's notably missing is the like simulation, right? Dynamics or or or like uh bolts or >> Yeah, I want to I'll help you guys pump up your views here. So I I think molecular dynamics is overrated. In fact,</p>
<p>>> coming from someone who >> goes in the that goes in the in the thumbnail, you know. Yeah. >> But just >> Yeah. And and and DFT is overrated. In fact, DFT may be even more overrated than like the NEMICS. I think these method >> for materials or for biology or for both >> for materials. >> Okay. >> And I can explain more about that. Basically MD and DFT have consumed an enormous number of PhDs and scientific careers at the alter of you know the beauty of the simulation. >> Also random interjection once I I did an estimate I think pre like chatbt something like 20% of the world's</p>
<p>computing power just went to simulating water. >> Oh my [ __ ] god water. >> Yeah. >> I had to deal with so many water simulations. I did I did DFT simulations of water and they are so annoying. I used these big computers uh from uh the department of defense and we I spent like I don't know five months and by the way this is prel training days five months of compute is actually a really long time >> I simulated water with quantum you know effects with a grotest mechanism for how a proton hops through water and it's on</p>
<p>YouTube it's my number one YouTube video and it represents like >> until now and it represents like I don't know a million CPU hours of compute it was you know one of the biggest computes that I probably the biggest one I've done in my life so far. Maybe Ether Zero is bigger, but but it took a lot more work anyway. And and what's the point? What did you learn? >> All I learned was like what set of hyperparameters reproduce some physical effects of water. But none of it was denovo, right? And this is the this is the issue with with molecular dynamics and DFT is that um they don't model the</p>
<p>world correctly. And so we have to invent little stories we tell ourselves about we're like making good inductive biases and then it models the world more correctly. Like in DFT you simulate water at 330 Kelvin when you want room temperature water. >> Is room temperature 330 Kelvin? >> No it's not. That's a little too hot. Right? And so this is a the issue is that like people just make up these</p>
<p><strong>[42:15]</strong></p>
<p>these these things or like I don't know GGA or like BIP or B3 lip all these different like methods people. are clearly empirical and then they bolt it on to DFT and they say look >> it's a first principles method right but actually you made a whole bunch of choices and you know you you whatever overfitit to the validation data to get this to work and and that's I think MD and DFT are like that because >> if you go look at the catalysts you know what catalysts change the world none of them are single crystal materials that are really well suited for DFT they're always like they have grain boundaries</p>
<p>they have dopins they're complicated right and and you never capture with DFT. So I think this is one of the fundamental I don't know dichotoies of the world is that simulations simulate really boring things really well. They don't simulate interesting things very well. And so that's why I don't do DFT and MD anymore. What about somewhere like the machine learning stuff like alpha fold and and >> alpha was trained on x-ray crystalallography data and I think you know this is the this is the story of MD is that MD was supposed to be the</p>
<p>protein folding solution there is a great counter example there's a I don't know there's a word the counterfactual is basically a group called DESR dehaw research they had you know similar funding to deep mind um probably more actually they tested the hypothesis to death that MD could fold proteins. They built their own silicon. They built their own clusters. They had them taped out all themselves. They burned into the silicon the algorithms to run MD. They ran MD at huge speeds, huge scales. >> Yeah. I remember David Shaw came to a</p>
<p>conference once on MD and he flew in by helicopter and was like to this this pretty famous guy kind of rich >> and um he he gave a >> an amazing presentation about the special computers and special room and out outside of time square and like what they can do with it. Beautiful, amazing. And I always thought that protein folding would be solved by them, but it would require a special machine. Maybe the government would buy like five of these things and we could fold, you know, maybe one protein a day or two proteins a day. >> And when AlphaFold came out and it's like you can do it in Google Collab, you</p>
<p>know, or on a GP or desktop, it was so mind-blowing. I forget like that protein folding was solved. I always thought that was inevitable, but the fact that it was solved and on like your desktop you can do it was just completely floored. Changed everything. This is like the the bitter lesson on steroids. >> Yeah. I don't even know what it is, but it's like imagine Chad GBT came out, but instead it was like, oh, you can just run it on your phone or locally on your own desktop. Like that's the level of like shock that came out. >> And it gets down to this thing that humans are really bad at estimating problems that aren't humanmade problems. Protein folding we all thought was like</p>
<p>would require a huge amount of compute, very challenging problem, the most hardest problem in the world, right? And it turns out that you can actually do it on I don't I think the numbers are now like 10,000 GPU hours. You can train a a good protein folding model. It's actually turned out to be barely an inconvenience. >> Therefore, why not? >> Oh. Oh. Therefore, protein folding was highly efficient based on experimental data. It they took X-ray crystalography. That's what Deepmind did is they took</p>
<p><strong>[45:15]</strong></p>
<p>X-ray crystalraphy data. Desire tried the first principles method >> and it's like a nice head-to-head comparison. Two very well resourced groups. They both tried different ideas >> and the machine learning on experimental data >> beat out first principal simulation by >> you know >> a very large margin. And so why isn't like bolts or whatever inside of Cosmos? Like why isn't there a tool that's that can run? >> Oh, we have bolts inside of we have bolts gen. Yeah. Yeah, we have that inside of Cosmos. >> Okay. It is >> I mean I think in the version that we have uh for people to just sign up and</p>
<p>use. It's not in there >> but like uh you know you can imagine that you can just modal or lambda or tamarind or 310. There's all these companies that basically wrap a lot of these these like um uh deep learning protein design tools or chemistry design tools. wrap them in an API. You just give that to to >> give it to cloud code if you want. You can give it to Cosmos and you can be like, hey, >> you know, if you want to design a protein for X, use these tools. >> Your mechanism, it sounds like or one of the primary mechanisms that has been successful is like it like enumerate a whole bunch of possibilities and filter,</p>
<p>right? And so how do you think about serendipity and out of out of distribution thinking and getting there and how far have you gotten and what's left? >> That's a great question. I think I guess the the short answer is that there's very so so this is the domain of seaborn so chemical biological radiological nuclear um uh weapons or I don't know safety >> this domain has been explored a lot in history by a lot of organizations >> and um I would say that >> there was a big question mark for us a few years ago was like how much of this stuff is uh intellectually bottlenecked</p>
<p>>> like how often are people like oh wow I want to cause harm Um, but I need to know like some facts and could LLM's make that easier or go faster or anything like that. I think you know the first set of answers in 2023 I think was basically no is that like you know you can go find the synthesis route for many dangerous compounds on Wikipedia. >> People know what are the targets in the human body that like are are targeted by most biological weapons. It's it's not</p>
<p>really that much of a mystery. So I don't think there was a lot of like um there's a lot of new ground when LLM first came about. Then there's a lot of concern about like laboratory protocols is that could agents or LLM uh reveal some tacet knowledge that like maybe people couldn't find in Wikipedia or like maybe for making something there's some technique that is required when you scale it up in size or something or maybe there's like some way to get around like tracking lists by ordering different compounds or >> so and that I think was really well tested. um by a few different labs, not</p>
<p>not me, but there's some groups that spun up that started making like tests for this and it and labs pay attention to I think it's really been put into process where LLMs will like kind of shut down or be filtered in those scenarios, but I think that is actually an area where there is is some risk. Um and so I think this something that people pay attention to for open source models and there's still I think some some discussion there, but I think to a</p>
<p><strong>[48:15]</strong></p>
<p>large extent it's it's not really been greatly accelerating in practice or at least I haven't seen much evidence of it. Um, and again, I think it comes down to the fact that it's not really available, but like you if you look hard enough, you can find most of the information you would need to to get up to no good. >> Yeah. >> In the public domain already. But then I think now is the the next frontier is like uh can it somehow help you with real-time protocols, troubleshooting like more in the loop and um and and more especially in the computational side of things. there are some scenarios</p>
<p>that are now coming into focus that could be more dangerous or more intellectually bottlenecked and so I think people are trying to pay attention to that to some extent there was like a first wave that we thought this could unlock a lot of stuff and I don't think it came to pass I think there's now an emerging sort of second wave of like there are some actually new scenarios that were just too farfetched to consider two years ago that I think are now realistic um some smart people are paying attention to it but I don't think it's solved yet >> I don't It's very vague. No,</p>
<p>>> I mean, so I guess like one kind of differentiator, there's a lot of talk about AI safety in like the modern LLM, you know, ASI space and, you know, there it's jokes about or pay-per-click maxing robots or something. But like the the core threat here is more like a malicious actor using this as a tool to accelerate something dangerous. And like kind of the first order hypothesis is that you basically already have to be an expert to effectively create a bioweapon or a chemical weapon >> and a non-expert</p>
<p>an expert would already know how to do this. >> Yeah. I I think you know so so each of the categories in the CBRN they're all a little different but I think to a large extent it's a a lot of like pushing material around. You know the classical example in nuclear is like it's a lot of lot of centrification lot of ultra centrification a lot of high pressure or high RPMs >> and so it it's just >> you can maybe get smarter about how to set up you know the the economy of scale to do that with an LLM but to a large I</p>
<p>think you can call your your friend in country X and they can tell you what are the steps it's it's not I don't think it's that much of a secret it's just a lot of like moving material around and I don't think it's acceler meaningfully accelerated. Now, that said, there are all kinds of like, you know, dumb dual use things of like maybe you want to call a company that makes centrifuges and you want to make sure that they sell you them and they go through some KYC steps and maybe an LM can get you through the KYC faster. And that's like a dumb thing that like, okay, like yes, like uh you know, email makes it so that</p>
<p>you can order centrifuges off the internet more easily. Is email like a dual use technology? Like, yeah, to some extent it is. And so I think there's a lot of like weird second order things that we don't pay attention to in AI safety of like does it make KYC easier? Does it make it easier for people to know like where to order this from or like what is the expected price or like what should you order first, right? All those like sort of simple logistical things I think are accelerated by AI just as like a a consequence of AI being</p>
<p><strong>[51:17]</strong></p>
<p>an accelerating technology. Um, but certainly I mean [ __ ] guys, there's some scary stuff and I try not to think about it too much and uh >> yeah, >> I don't know. I guess I don't want to get too political, but I do think that right now um the the United States government is maybe taking a a slower, less intensive look at safety and um but there's definitely people I think in other spaces than the US government thinking about it hard. >> And do you think is this thing people need to spend more time on? I do get</p>
<p>waves of angst about AI and I'm sure many people living in San Francisco do get like a little bit of a little bit of waves of it and uh sometimes I think that there isn't enough work being done on it >> and then sometimes I think wow like I need to mellow out and like you know we have lots of time to think about it. What is my opinion on it then? I don't know. I I I think my opinion is um not formed fully. >> Yeah. You and Sam have done a lot of thinking about funding science. and future of science. You have you've been</p>
<p>vocal about the reproducibility crisis and other things. First question, why this focus research organization or for Yeah. >> From Yeah. What does that get you that you don't get from academia or, you know, big lab or whatever? >> Um, a nice network of of people. I think Edison is like a real uh, of course, I think Edison's going to do great, but I think it's a mystery of what's going to happen. Um, so I don't think we've had as much friction there as you might expect. But yeah, this is all stuff that</p>
<p>we that that that Sam and I think about all the time is like how do you balance stuff like this? How do you balance the economics? Um, you know, there are some there are some ventureback companies that are having cash salaries over a million dollars. And it's like insane to me. >> Yeah. >> That you would use all of your cash from your equity financing, you know, in these insane salaries. But that can in terms of like total spend on GPUs, it can still be a total a small fraction of your burn. So sometimes it kind of makes sense.</p>
<p>>> Yeah. Yeah. That's that's one way to think about it. >> So So like you this this is a good uh leadin to you are automating science in some capacity. So where does that leave scientists? So I think um this is uh Jevans paradox we can try here is uh um so uh let me start with a contrast here is that uh you know if we automate um you know taxi cap drivers there's a fi there's not going to be an increase in people needing to go places maybe</p>
<p>there'll be somewhat an increase but like there is a finite amount of like time people will be spending in cars >> and so there's an upper limit. So when you automate that that's like a scarcity thing is basically you're displacing jobs when you automate driving. >> Yeah. In science, I don't think there is a finite appetite or a finite capacity for science. I don't think science is like a a scarcity thing. Like there's, you know, 100 more discoveries left to be made and then we'll be done. And so like we're displacing jobs. I think</p>
<p><strong>[54:18]</strong></p>
<p>instead actually if we can, you know, make science go much much faster. There will be no there will be no decrease in demand. There will be actually I think an increase in demand that will match whatever automation amount we have. And so my vision for what a scientist would be in the future is that they will be I don't know like uh agent wranglers or cosmos wranglers of like okay they're exploring 100 ideas simultaneously or they're like working with systems like ours to to make 10x the discoveries 100x discoveries because I think there's an unlimited amount of scientific discoveries to be made and so there's no</p>
<p>like scarcity set where basically we will displace them all. No, that's kind of like, you know, this is what I would tell when I go talk to a first year PhD student, like everything's going to be just fine, >> you know. Then when it gets into the nuts and bolts, I I do agree that this is going to be like a really hard thing where like if I am CEO of a company that makes science, like a pharma company or material science company or something like that or R&D arm at IBM, I think, well, I could spend, you know, a million more dollars on on compute for the AI</p>
<p>scientist or could hire 10 more people. I might just choose to go with the AI scientist because you know to a large extent like hiring people is hard right >> and and hiring an AI scientist is probably a little bit easier. >> Yeah. >> And so I think that there could be some there could be some friction but another thing is like science is in some ways closer to art in the sense that like there is a large number of people just appreciate good science. Like if you get published in nature it's not because</p>
<p>it's really going to be world changing. Of course, that's part of it, but it's also because like people are like, "Wow, this is really interesting science." >> Yeah. >> So, I think the the enjoyers of science are also scientists. And so, I think that it's kind of hard to imagine a scenario when there's not scientists as the consumers of science. And so, I think if they're going to be consumers of science, they're also going to be some of the producers are involved in the and the process by itself, >> right? >> If that makes any sense, >> yeah, you've touched on this. My the question in my mind is just what does a scientist do? Then >> there's a great short story um by um Ted Chang, I think in like 2003 or something. Okay. And it's about like</p>
<p>well at first scientists were displaced and they became like the uh interpreters of like what the AI scientists are doing like the scientists read the AI scientists like papers and then you know translate them for whatever popular science or something >> and then after that like they couldn't read the papers anymore and so they were left behind and so they had nothing to do and they just sat around and >> but the problem is that >> science is like you know you you have to translate science to make any impact like science cannot exist by itself. I do agree there's like engineering can</p>
<p>exist by itself like if you give some kind of system a goal of like making me a material that I can make a space elevator out of. You could be not participating in the beginning the process or the middle of the process and you just come by the end and be like okay follow this recipe like science of like what's the origin of life or like is there water on other planets or you know um why is some catalyst better than another catalyst that has to be hitting human eyes and human brains at some</p>
<p><strong>[57:18]</strong></p>
<p>point. So I think a human has to be involved in the process. >> Don't want to be contrarian but >> yeah be contrary. >> Why does a human have to be involved? >> Why does a human has to be involved? Well, a human has to be involved at least some point to be like yes this is good science or this is bad science. >> Okay. So it's it goes back to taste. >> Yeah. But I don't know. Maybe you're right. Maybe there is no point for humans. Maybe it'll be like you know what is it? Sora. Uh you know like the AI slop app. But I think in Sora there's still humans at the end clicking the videos or something. >> Yeah. So, >> so, so the the sorith analogy kind of brings up an interesting point like is it possible that like due to the biases</p>
<p>of AI science if we really go full in science that >> you know there still is a market for kind of boutique human science like you know there's still people who want to you know paint things the old fashioned way but more to the point does it become even more important for to have a human who is uh actively doing their own exploration because there will be like large blind spots and biases due to the models that just you'll never be able to overcome because this is sort of baked</p>
<p>in now um due to your training data and without a human that you'll always get stuck in there will be a blind spot that will never >> Bio which is a company in in Oakland um or in Emeryville and they do really cool stuff with automation. I think they're going to be testing this theory of like okay maybe if that's the bottleneck we can see evidence of it because they're going to start doing really well. >> Um it could be true. >> Mhm. I I still though want to say all of those I in my mind are still sort of scoped in terms of like R&D for pharma or bio but they're not like none of them</p>
<p>are attempting to answer big fundamental questions and maybe there's like different levels when I think about that you seem to be >> um it seems like the future h the the focus of future house in Edison is much more towards like you know sort of R&D and sort of endrun science but um you know I I have some background in, you know, fundamental physics. Um, you know, it's like is there any thought about like how do you like take on, you know, dark matter candidates and like</p>
<p>>> I just, you know, think the data to really give us a complete story is just not there yet. >> You know what like uh I'm sure everybody at every company like is the biggest critic of their own product, you know? >> Yeah. So we think Cosmos is we think it's great but there's an a very large amount of area for improvement and >> so with Cosmos can so there's like a open like sort of access to everybody version. >> Yeah. >> Do you provide access to other labs that um is less open?</p>
<p>>> Um we have a version of Cosmos that has like um bigger resources like it can run for longer. It uses GPUs. Um so like basically when it does data analysis it'll have a GPU. So we use that for things like um like machine learning experiments you know if you want to know like this question about whether it's better to pre-train first on noisy data or not. >> Yeah. >> Um we have like pre-release models that that are coming out and we try those.</p>
<p><strong>[60:18]</strong></p>
<p>But um >> yeah so I guess like yes we do and we do have like research partnerships with with companies where we like build something specific for them and that is something we think about. >> Yeah. But broadly I would say Cosmos that's on the website is pretty close to to what is the best we have internally. >> Yeah. >> Uh I have a question. Um so you you previously have stated that you think that language is the natural um language. Was it >> language of chemistry? >> The future of chemistry is language. Yeah. Yeah. >> Um okay. So I wonder do you still believe that?</p>
<p>>> Good question. I think I I would say yes. I still believe that um that so so in that article the opinion article my my point was that uh you know at the time when I wrote that article which I think maybe three years ago now or something maybe 2023 um it was that we have models for predicting solubility of compounds we have like data about very large populations and we have like papers and we have code and and the only way to bridge all that</p>
<p>information is natural language and and the argument was that like humans is like you know whenever we can't bridge information like if I can't talk about my code or I can't talk about some idea to you I will invent words until I can get the point across right and that humans are always innovating on language to make it represent all known observations and people innovate on language to represent whatever code pattern they have right like this is like the the only shared activity we've been doing for this long is like coming up with words to represent everything we know >> and so I think to that for that reason natural language is the only possible</p>
<p>way to connect all the different pieces of data we need in biology, medicine or any domain for that matter. Um, I think there's some caveats to this of like, you know, you can make an argument like if Yan Lun were here and he would make an argument about like, you know, world models or like vision or embodiedness, right? Like that there's arguments against natural language that like, you know, that maybe there's something more that it does. It's not the complete story or maybe natural language imposes limitations that you cannot exceed because you're stuck in this abstract space that was invented by humans and you can't escape it until you can like</p>
<p>touch something. >> Yeah. I mean, it is an abstraction, right? in like like scientists basically work exclusively in abstractions to some degree. Um I I just I find I found that interesting because it seems like most scientists you're right like when they explain things they explain things through language but many conversations maybe most at some point result in people drawing diagrams or something like you know chemistry like biochemistry largely or medicinal chemistry is often times a it's it's a language of graphs right or you know I</p>
<p>mean bonds are abstractions yes but like they're pretty good abstractions for most ca for many cases >> or like you know geometry you know think about you know protein as like the geometry of a protein >> you know it's like I think that that's how people a lot of scientists like to think about things >> and um so I find it interesting that like yeah that that you are focusing</p>
<p><strong>[63:20]</strong></p>
<p>primarily in language like have you thought about essentially a multimodal version of this like where you know when it comes along a smile string it doesn't just say oh this is a smile string but like this is a graph this is a representation of some higher like abstract object. >> You're absolutely right. And and the problem with these this like I don't know Jacob's ladder or something whatever you want to call it is like yes you can say that a you can call a molecule by its name >> you can show the graph >> then if you go to molecule like ferosine well it doesn't really have bonds like part of it and so then you're like well</p>
<p>we need to draw it visually >> and then you go to molecule like I don't know >> gly betane >> this dihedral angle right and so like it's not actually this thing I drew it's actually an ensemble between this thing and this thing right then you go to benzene you're like Well, not only is it like a ensemble of these different confirmers, it actually has electron density. You can't really ignore the electron density in benzing. You like need to treat it correctly. And it was like, well, you can't actually represent the electron density that way. You actually have to look at the correlation of the electrons individually, right? Because you can't really model benzing with like DFT, right? Or functional. You have to actually look at the the</p>
<p>electron correlation. Electron correlation like well, you know, you can model correlation, but you know, actually these things when they're in a solution, they have like, you know, relativistic effects because it's like there's a whole bunch of stuff around. So you really got to have the the relativity in there and you're like well you got the relativity and you have the electron correlation you could have the bonds you and you have the commerce but you really need to think about the cosmic radiation background because like you know it does actually impact everything and there is some some energy there right >> and before you know it you've ran out of you know you've ran out of compute or whatever resource you're using to model this</p>
<p>>> and so I think um you have to draw the line somewhere >> natural language like I said is that humans have worked for a long time to make it be the you know what's the word like the least abstract or the you know it's somewhere on the border of like it's still abstract enough that you don't need to know all these details but it's still granular enough or concretized enough that you actually can make use of it. Um there may be some other representation like multimodal might turn out the video or maybe I don't know there's some other like fusion that you can make. I like</p>
<p>natural language because we all work really hard to make it right at that boundary. And I do agree sometime sometimes ideas slip and they can't be in language. You have to get out the whiteboard or ideas slip and you have to wave your hands around, you know, or >> maybe then then you need that that uh degree of freedom to communicate. >> Just digging in on this a little bit more like uh famously quantum mechanics is like undescribable, right? Like there's there's an argument that you cannot understand quantum mechanics with words. it or in in with our preconceived</p>
<p>understanding of the physical world because it doesn't behave like the macroscopic world and so that the only way to understand is through mathematics right um and I largely see language as the joint key of science as well but I wonder if that's not true for many domains and quantum mechanics is just the one that hits you in the face >> I mean I don't know actually I think the</p>
<p><strong>[66:20]</strong></p>
<p>there's like seven principles of quantum mechanics or five or something like this that you can actually express pretty concisely in language. >> I agree that like you need to actually look at the consequences of them. You need some mathematics. Um I don't know. I actually I don't know. This is like a challenge. I think you could actually describe a lot of quantum mechanics and language. >> Sure. Sure. >> But but I I see your point and um yeah I I guess uh uh I'm a realist like I when I talk to my kids you know maybe I I will like okay let me draw for you. I don't I don't make sure in our house</p>
<p>everything is described with natural language. Uh, so I I agree with you there. Um, I think maybe we can be a little a little flexible with with natural language and include equations and smiles strings in it. And I think we can get a little bit farther. Um, so maybe that's okay. Uh, but some people I think like optionality, >> you know, like, oh, it could be this or it could be that. I'm somebody I like to take take strong opinions and see >> how much farther they can get me. And I think in my career, it's actually been better for me to take strong opinions, which in my deepest of hearts, I know</p>
<p>that are maybe not correct or not fully correct, but once you take these strong opinions, it just you can sort of move many steps down the road. Once you take these strong opinions and like for example at future house, we took the opinion that scientific agents are the future. And that allows you to skip a lot of steps because a lot of other people were like, we need to build a foundation model for X. >> Yeah. >> And we just skipped all that, right? And I think if you also were unopinionated and you had optionality like I can think of a famous example of a different company that like liked the optionality and they wasted a lot of time on foundation models or something then then I think you you get stuck. So that's one</p>
<p>of my strong opinions is that natural language is a is a way to join all these different domains. >> It may not be a correct opinion. It may not it may be more subtle or more complicated but it's allowed me to get very far. Um I'll drop it someday and maybe find a new one. Yeah. Not yet though. >> That's my ma opinion on the matter. >> The Ether Zero story on your blog I find hilarious and kind of awesome. >> Yeah. >> You know, when I was a kid, I love the like genie/ monkey paw like concept of be careful what you wish for because you</p>
<p>just might get it. >> Yes. >> Maybe just like quick story. Can can you >> just talk about that? That was that was just a really fun >> Ether Zero was a a hell of a project because conceptually it was a very short project of like hey people have made a lot of progress in verifiable rewards in math and in computer and and code. Let's see if we can do it in chemistry. So chemistry is like not a verifiable field, right? Like of course you can go test something in the lab, but then we like had to think about all these like ways that we make chemistry verifiable. And one of the ones we settled was like</p>
<p>make a molecule that has like three nitrogen's, two oxygen, 10 hydrogens's or something. And we thought that was like a pretty verifiable pretty verifiable question. But every time we would train a model, it would find some new insanely weird trick to generate these molecules. And and I I I'll just tell you one of the examples was that um uh it would make these molecules and we would do some checks to</p>
<p><strong>[69:22]</strong></p>
<p>make sure like it had the right bonds, the right number of electrons, the right number of atoms and stuff like that. Um but it would just solve the problem in any way possible, right? So like it would just put all the nitrogen's over here, put all the oxygen over here, just like things that don't look good. Yeah. >> And so we started coming up with these rules of like, oh, let's check to make sure it followed these good practices or these good practices. And we found ourselves into this like, you know, it's like the opposite of the bitter lesson, like I don't know, the boutique lesson where you like try to make everything custom. >> But one of the things it kept doing is it kept putting these nitrogens in a row and it put like one nitrogen, two nitrogen, three nitrogen all in a chain. And this is like, you know, if you have three nitrogens, it's like explosive.</p>
<p>You know, two nitrogen's like bad. And like four nitrogen's you can't make. And I kept telling everyone like it would make these like six nitrogen comets and they're just they're just literally impossible and they're not possible. And many of the people on the team were like computer scientists like on this team and one of them like one day sent me that like this is on the cover of nature today on nature's website. Somebody made a six nitrogen compound and this is like somebody's like career to deliver this compound because this is the most unstable like insane compound you can make. It's some ridiculous setup and like the spectroscopy to get that proven</p>
<p>was like very difficult and this this I don't know how they did it. It was amazing accomplishment like look Andrew like it's not actually impossible and it was so funny to me that like our model was sitting here spitting out these six nitrogen compounds in like you know 2024 or 2025 and like the paper just happened to come out that year that like mankind had finally made a six nitrogen compound. >> So do do you think that those were actually synthesizable even under these extreme circumstances? >> No. No. Our model was just it was just reward hacking. >> Okay. >> And it was just the the model was so creative in ways to reward hack. Like</p>
<p>one of the another one we did was um >> we wanted it to make sure that the when it would propose a reaction like make this compound, tell me how to make this compound. We would try to make it sure that all the reagents were purchasable like you could purchase them. They were not like made up. >> Yeah. >> Um and and the reason we came with that is that originally would just like take the end compound and then like remove one atom and be like here's buy this >> and then put the atom on. It's like okay. It's like well it's I wish it was like that. Um, so they have to be purchasable. And then well, we're like, we thought it might be hard if they're all purchasable because sometimes you</p>
<p>actually order things custom or or something. So, just make sure one purchasable. So, the first thing it starts doing is putting nitrogen in there because nitrogen is purchasable and it like has no participation in the reaction, right? Like, oh my god. Okay. So, I'm like, okay, it has to be purchasable. It has to participate in the reaction. Then it starts putting like acid base chemistry. It would just put an acid here. Acids are purchasable and it'll move one atom. And they're like, okay, fine. Can't be that. Everything has to be purchasable. Then we find ourselves and I'm like sitting there one day building this like ridiculous catalog of purchasable compounds and a bloom filter so it can go fast enough in our training loop and</p>
<p>I'm like why am I doing this? How did I get here? >> How did I get here? And and I don't know it was really funny because um pre-training or training transformers you know on on just data like just supervised training where you just have the inputs and the outputs directly >> very nice relaxing you know like things are always robust you know things are go pretty smoothly. When you do these verifiable rewards where you have to like write a a bulletproof verifier it</p>
<p><strong>[72:23]</strong></p>
<p>is really difficult >> and we had so many models trained only to find out they were hacking some other like random thing in our setup. It's really hard and I and I I don't envy the Frontier Labs that have to do this at a very massive scale because we had a lot of adventures in Ether Zero and and you guys should read the the blog post. >> Definitely read the blog post. It was a great read. >> GRPO. We did make some modifications um to GRPO. >> Yeah. >> Um I actually I used to know all the names of these modifications, but uh I think it's like uh Dapo is one modification and like the clipping we</p>
<p>did was special and we explored a lot of that stuff. Yeah. >> Um, and it was uh also one of these things where like you think the hypers are wrong, the algorithm is wrong and then you find out it's just because like you had somehow sorted the reagents when you made your training data, but in your test data, you didn't sort them alphabetically and the model was just like barfing because its whole strategy was to exploit something in the way you sorted things. So yeah, we explored a lot of different methods and it was um I learned a lot about chemistry, a lot about nomenclature. Um, and actually</p>
<p>there's a I learned a lot about medicinal chemistry as well, more than I ever wanted to. >> Awesome. >> If you want to do some like engineering, just check out Edison Scientific and they have, you know, I think a lot they're hiring with lots of like interesting things. Everything from scientists to, you know, infrastructure engineer. >> Yeah. >> Yeah. Thanks, Andrew again. >> Yeah. Thank you very much for for joining us.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=XqoBSB3nsgw</guid>
      <pubDate>Wed, 28 Jan 2026 19:14:38 +0000</pubDate>
    </item>
    <item>
      <title>⚡️Context Graphs: according to the authors — Jaya Gupta, Ashu Garg, Foundation Capital</title>
      <link>https://www.youtube.com/watch?v=zP8P7hJXwE0</link>
      <description>In this Lightning pod, swyx hosts Jaya Gupta and Ashu Garg from Foundation Capital to discuss the emergence of context graphs. They define this new framework as the institutional memory of "the why" behind business decisions, captured through decision traces—the sequence of steps and human reasoning that models often miss. The conversation explores how these graphs will become the defensible moat for the next generation of applied AI companies and systems of agents.

Section Timestamps
[00:03] – Introductions and the early "vibe" of AI hackathons post-ChatGPT.

[02:00] – The origin story of the Context Graph thesis at Foundation Capital.

[04:59] – Defining the Context Graph and the Decision Trace.

[07:32] – Who is building this today? Examples like Player Zero and Glean.

[09:37] – Technical implementation: Is there an ideal data structure?

[12:09] – Explaining "Systems of Agents" vs. standard chatbots.

[15:26] – The importance of the "Right Path" (operational) vs. the "Read Path" (analytical).

[18:46] – Why these will be new platforms rather than features in Slack or GitHub.

[21:48] – Addressing pushbacks: Can you truly capture the "Why" or just the "How"?

[24:04] – Privacy, data governance, and "Metadata 3.0."

[26:43] – Context Graphs vs. Data Mesh: Why universal graphs are unlikely.

[31:18] – 2026 Predictions: The "Context Graph Stack" and production scale.

show notes
https://foundationcapital.com/context-graphs-ais-trillion-dollar-opportunity/
https://x.com/JayaGup10/status/2003525933534179480
https://simple.ai/p/what-are-context-graphs</description>
      <content:encoded><![CDATA[<p><strong>[00:01]</strong></p>
<p>[music] All right, we are starting uh with the uh lin space pod in [music] a remote studio. Welcome to Jay Gupta and Ashgar from uh Foundation Capital. >> Thank you for having us. >> Thanks for having us. >> Uh Jay, we apparently met 3 years ago at the Langchain hackathon where uh Factory was born. Were you were you also actively thinking about like context stuff back then or like what what was the what was the vibe 3 years ago? Yeah, I think 3 years ago was probably like the first uh AI hackathon and I cannot say I was thinking about context things</p>
<p>at that time. I was just thinking about like and I think it was actually like before even agents took off like it was like literally right after chat PT and I think the only things that really existed were Langchain, Llama Index and like Perplexity were like sort of the three companies that came out right after and so anything that they were doing I think all the AI builders were going and running there and going into like whether it was like notion's office I'm pretty sure had a hackathon but definitely it was not context back then it was even pre Asians and so it's like crazy to see where we've come in just a</p>
<p>short amount of time. >> Yeah, I mean abstractly I guess like lang chain llama index and to some extent perplexity also they the funnel context into LLMs and we weren't working on agents back then but you know obviously at some point we would start like even even back then there was discussion already. Ashu what's your what's your uh sort of uh AI story as well like I'd love to introduce the audience. Yeah, sort of the very short version, you know, I used to run a machine learning team at Microsoft in 2006. We were doing ad targeting and after you've done ad targeting for a few</p>
<p>years, you want to do something that's better for the soul. So I I left with a variety of ideas. I bumped into a guy called Yan Stoka at Canviva, which was my first portfolio company. And they were doing applied AI at the time, but they ran into a bunch of data infrastructure problems. And that led to, you know, data bricks and any scale and then a whole series of other applied AI companies. So, I'm lucky enough to hang out with J and Yan and other smart people. That's that's my claim to fame. >> Well, you guys uh uh work on really really smart things and congrats again on having the early hit of 2026. Uh I would say so I'm going to bring this up</p>
<p>on screen right now which is the context graph discussion which is what we're here to have. So J actually tweeted basically like the article version of this but obviously there is there's the um there's the actual formal blog post. I'm just kind of curious what the origin story of this is. Like what were we sitting in a room and you were talking like hey we need a name for this thing. We see an opportunity. What was going on in December that you started working on this? >> Go for it Jeff. >> It's a good question. So, you know, I think, you know, we sort of throughout</p>
<p>December and I would say more broadly like the last 6 to 12 months, like you know, we you know, Foundation Capital like we spent a lot of time with our companies very up close and so you know, one of the things that Ashu and I kept continuously talking about like and what we were thinking about is like what is actually missing as AI agents move into production. Like if you think back like</p>
<p><strong>[03:02]</strong></p>
<p>everyone said 2025 would be the year of agents and and in some ways it was you know we got cloud code we got like Devon we got all these customer support agents Sierra Decagon and I think you know at the same time the models also got dramatically more capable but I think you know we when you think deeply about like what is missing you you kind of realize like you know that they still can't reliably do enterprise work some of these agents and so you know we started thinking about it and like it kind of struck us that, you know, one of the reasons is that because they don't actually capture like why we do certain</p>
<p>things like and the human reasoning behind decisions. And so like specifically what do we mean like why exceptions are granted, how conflicts are resolved, what precedents are applied and that's sort of kind of what the thinking was over the last 6 months and I think in December we we got to writing about it and and published sort of last thing right before the year ended. >> Yeah. I think the thing I would add to that is you know the the subtext at least for us and I think for a lot of other people over the last year has been that as so much of the IP is captured in</p>
<p>the model itself what is the role of applied AI companies broadly systems of agents company systems of record and and what is the defensible mode and and I think we've been debating that with anime at player zero with Ishan at olive with Kabir and so many of our founders and they were all talking about the same thing but using different languages or different words and so I think it was that series of conversations that gave us this insight that there is an intermediate abstraction which we call</p>
<p>context graphs which is the accumulation of decision traces and this intermediate abstraction in our opinion will be the one that will matter the most over the next decade it's the enduring layer for the most successful application companies or systems of agents companies and we also believe and we'll talk about it more that it's it's unique and new and it's not well suited for incumbents. >> That is actually very key because obviously the incumbents could also capitalize on it if they see the opportunity. You you did you did name job Animesh. Uh I did have him</p>
<p>categorize in our discord. We have a discord where we discuss all these like trends and stuff and so like yeah I didn't know he was your portfolio company. Yeah know he's he's been a part of coming over the thesis along with a bunch of other founders like you know Kabir as I said it to Sarah and Ishan at Olive but anime is definitely uh a brilliant technical founder who's helped push our thinking okay how about let's let's do like a very Chris like you we have a deck that you guys uh Kylie sort of put together I think definitions always help motivate a discussion so let's let's just go right into how do we</p>
<p>define a context graph and and all the other sort of related concepts go for it >> you go for definitions. >> You know, look, I think I we can we can we can describe the it's a concept and so you can describe it in a variety of ways, but at its essence, a context graph is the institutional memory of all the why behind the set of decisions. And</p>
<p><strong>[06:02]</strong></p>
<p>we think of that core unit as a decision trace. When when when an agent or a system of agents executes a business process, it goes through a dozen maybe multiple dozen steps. It may have humans in the loop once or many times. And the tracking, these exceptions, these overrides, this cross-system context which sits in both structured and unstructured systems and very often in people's heads. And that's</p>
<p>where the human in the loop comes in. Uh capturing that is a decision trace. And when you aggregate decision traces in a way that you can then learn from them as an agent and you can use them to make the agent better, that's that fabric or becomes what we call the decision graph. >> Uh sorry, decision graph or decision trace. >> Sorry, that my apologies. That that that layer becomes a context route. My mistake. The context. I I >> So context graph is made up of a lot of decision traces. It's made up of a lot of decision traces and it's also you</p>
<p>know when you implement that you will have to implement it in a way that you can actually extract insight from this and you can make it machine usable by the agents themselves. And one thing I want to get clear up front like this is kind of like a conceptual thing right like do we has anyone claimed to like have this working yet because it seems like very very big. What do you think, J? You talked to a lot of people.</p>
<p>>> I do talked to a lot of people. I would say that if you look at Twitter today and you look at LinkedIn, thousands of people claim to have it working. I've had like maybe 30. >> There's this guy and he mentions like how to build a contest. So, he's he seems very confident. >> Yes, he is. He's I think player zero is actually one of the examples that you know that you know that actually has some version of this working. I think that what's interesting about this is that you there's going to be a lot of different implementations of it. And so I think you have like</p>
<p>you have companies like Glean that have come out and say that you know we're the ones that are building this. You have companies like Atlan like data cataloging companies has have said that we're going going to own this. You have application companies that have put out their version of this. I think Harvey not not Harvey but rocks put out approach actively put out approach. there's companies across the app layer that have also you know said that they're building this and so I think you will see info companies application companies security companies octa put out something as well and so but in terms of like seeing like this actually</p>
<p>deployed at scale like I think there are very few examples today and some of that is because it's a very you know new concept >> so I think the only thing I would add to that is don't think of context graphs as a technology architecture think of it as a framework and there will be multiple technology implementations and those will evolve over time and and different companies will use different components, different databases, different data</p>
<p><strong>[09:04]</strong></p>
<p>fabrics in order to build that context graph and it'll depend on the on the use case and the situation. What we are seeing from our portfolio at least is that the common theme across these systems of agent companies that we think are doing revolutionary stuff is they are building a context graph. They're early but we're seeing it. We we we see that at player zero. We see that at Tacera. We see that at olive. We see that at analogic in the security context. Now there are varying degrees of sophistication because it's a concept as against a specific product. And so</p>
<p>this very sort of flexible concept is there like a an ideal data structure I guess for a context graph like is it is it just uh you know like the sort of triplet entities of a knowledge graph or anything like that? >> Go for it. Yeah. >> I think no and I think that's because it's more of a framework for now. I think that uh you will see you know I would say it's like um the concept of decision traces especially like I think that is going to be implemented by like many different types of companies and</p>
<p>there will be also many different types of companies that will emerge because of this like thinking about like security governance I think you'll see companies that pop up there and we're already starting to see a few pitches there where we're going to see like companies that are going to reimagine different sorts of applications so I think with this logic you could reimagine how do you build Sierra today versus you know a lot of these application companies started three years ago and so if you were to rebuild them starting today I think that's also what we're seeing and I think as well as on infra like we're going to see we've already started to see like different people like imagine</p>
<p>you know how do you think of you know data cataloges of the future databases of the future I think those are approaches are are quite early um I think you also see like a lot of the graph database people come out of the woodworks and and say that you know this is us as well. So, >> we've been doing this the whole time. >> I've been doing this for the last 10 years. I think most of my notifications when I get tagged and I press and it's like this is a great this is what I've been trying to articulate for the last 10 years like and here it is. >> At the same time, there are some common</p>
<p>themes of people who are building context graphs and you know the underlying infrastructure or technology implementation will vary depending on your starting point and your scale but they tend to be crossfunctional. They tend to be crossprocess. They tend to sort of these decision traces are stitching together data that goes across multiple existing systems of record. Uh they tend to be in the right path which is you're actually you're executing a decision and it's not an analytical it's not analytical decision. It's the actual</p>
<p>operational decisions you're making. you know, existing systems of agents, startups actually have a very unique advantage because they're not bound by a business process. They're not bound by an existing uh data store. They're in the orchestration path. In being in the orchestration path for automation of a business process positions you uniquely</p>
<p><strong>[12:06]</strong></p>
<p>capture decision traces and therefore build a context graph. >> Yeah, you have that in this in this slide here. Uh and also I guess in your in your post where you talked a little bit about the uh why the incumbents uh don't do it. I I think like one of the interesting things uh I want to sort of double click on. First of all, systems of agents. I've heard this uh terminology. Is that widely adopted? Cuz uh I would say that you're the first in recent months that I've that I've heard actually use this term. What is the system of agent startup? Go for it. Well, I guess out of curiosity, what</p>
<p>term have you heard um you know to to describe systems of agents? >> Don't know. I mean, I think that that's that's also like up for interpretation as to like what what exactly means. I do think like people have like system of record system, you know, as like a baseline of like okay, we all agree what a system of record is beyond that like people try to modify it in some way. system of like context or whatever. Uh system of agents like sure you have like</p>
<p>I guess a group of agents I guess and they all do different things but I don't know if if if this is like a a deeper origin or community around this term yet. So you know the way I would think about it is and and look these terms are all collided and it's it's a little bit like you know a cloud of words. The notion of a system of agent is a company uh that has a collection of agents that are automating a process or a set of processes.</p>
<p>It was it was initially we came up with that last year to differentiate from you know more basic chatbot like systems. So if you know a lot of a lot of systems a lot of AI companies are single player mode more like a chatbot and to the extent that you're starting to build something that's multi- aent multiplayer mode has humans in the loop and is driving decisions across a business process we want to distinguish that with this notion of a system of agent. The underlying mode that these systems of</p>
<p>agents are building is a context graph and the way they build the context graph is by capturing the decision traces that they inherently sort of you know execute. So it is a mouthful and any any advice on how to simplify would be appreciated. The term that I've been working on is mostly just agent lab the company that produces agents. Uh but I think those are those are like slightly different things in any way as well. So it's it's hard to describe. I think I just I just want to like like double click on a few of these things so that</p>
<p>people can get a sense of like what you mean when you say those things. I think the other thing that's also super interesting is being in the read path not the right path. Obviously with your background with data bricks you understand the the read path very very well. I think the right path is also interesting like context is mostly a read jog. right? Mostly just like introduces a higher demand for uptime and lower latency and</p>
<p><strong>[15:08]</strong></p>
<p>all those things. But I'm also curious like basically you know the way that this creates a waterfall in my mind of exceptions override precedence cross system context also I I put here approval chains which uh which you had in your original document. It feels more like IM like AWS IM like a some authorization sort of a logic system that cascades down and you you know hopefully you like I derive some kind of formal logic reasoning that you can um that you can sort of inspect and</p>
<p>version control maybe because like the I think the the worst thing is like when you when you capture all decision traces and you look at all the overrides and all the precedents it looks like Swiss cheese like people contradict themselves all the time and you're like, "Okay, well, what's the truth of it?" And and LM's going to be completely confused, reasonably. So, look, I think it's a great point and I think when you think about read versus write, you know, it can mean a very different thing when you're very very if you're being precise</p>
<p>and technical. I think when we think about the read versus right path is analytical systems which are in the read path are ultimately and you ultimately have to write to an analytical system. So there's some right path in that sense that's why technically it's it means something very different but analytical systems like data warehouses and even systems of record capture the end point of a decision. They know what happened. So they may know what the revenue of a company is. They may know what the deal size was. is</p>
<p>they may know what the discount offered was. They may know sort of what was the patch applied in the case of of a bug fix. What they don't know so that's because that's ultimately stored in some analytical system whatever that system might be. And to that extent systems of records are more analytical in nature than they are in the right path where aentic systems or systems of agents actually capture the sequence of steps. What did you do? So if you're using player zero, there's a support ticket that comes in. That support ticket then you know someone picks it up, some</p>
<p>agent, not even a human being first. It does some some analysis. It starts to run some queries. All of that is stitched together. It then at some point pulls a human being into the loop. That human being is then doing a bunch of queries themselves because they look at what the s the agents have presented to them and they start to query the system. All of those queries then ultimately lead to some decision or some hypothesis around what the root cause of the problem is and therefore what the bug fixes.</p>
<p>That whole starting with a ticket as an example through auto triaging to hypothesis generation to sort of bug fixes to ultimately then then translates into a piece of code that code that gets pushed into production and it either works or it doesn't. you complete that loop. That loop is the right path. The way we think about right path</p>
<p><strong>[18:09]</strong></p>
<p>>> yeah I see >> as against the narrowly defined readr path in a database which is I think where you're coming from. >> Yes. Because uh you know I >> which is technically more which is technically more precise. >> Yeah. But here you're you're you're writing code. So you know it's still still the right path but it's different. Yeah. And I think I think my question there would be like you know do people want to use a separate platform for it or should it just be inside of GitHub and Slack and you know those things will still continue to win or you know basically obviously people want to prefer their defaults the things that they're used to. Is there a case to be</p>
<p>made for putting these onto like a new thing like whether it's player zero or something else? >> Why don't I get to have J and then you to jump in as well. So I absolutely think it'll be a new thing. Now the key is that these new platforms whether it's player zero or or olive or tar or pick pick your favorite one they will have to coexist with existing systems and so the system of engagement may end up being Slack. You may communicate with player zero through Slack. You may communicate with it through other systems you have</p>
<p>because you're not going to replace those at least not overnight. But these existing systems don't actually orchestrate across an entire business process. To some extent, systems of record do. But even in systems of record, what you see is systems of record were very historically all designed for structured data. So even if you're running a a deal process through Salesforce as an example, very quickly you find the data is not in Salesforce, it's in Slack in part, it's in email. Actually the number</p>
<p>one system of record for most organizational data is email. You see email. Yeah. >> And Slack. So but email and Slack just captures data. I mean as a blob. And again technically not as a blob. I mean if if you get into sort of data structures but but the but the data is is is dark data in a sense in Slack and in email. And an agentic system that has a real context graph will capture this the pieces of data that are appropriate across email, across Salesforce, across</p>
<p>your account management system, across a conversation and across a Zoom call. All of these have data, some structured, some unstructured. And most of the value is actually in the unstructured data. And this is part of why s context graphs are so hard to build that you have to figure out how to parse the unstructured data in a way that you capture value and insight because if you started uploading every Zoom call you have in a sales situation,</p>
<p>you've done like a million Zoom calls and only noise. And so where the unstructured data is such a large part of it and conversation data is such a large part, most likely the context graph will actually consist of small models. The context graph itself is a model layer like you use that data to train a set of models but then become part of your context graph. In the case of player zero actually the data set is much more semistructure. It's it's less</p>
<p><strong>[21:10]</strong></p>
<p>zoom calls and more code and tickets and observability which all has some level of structure very large data sets but with more implied structure. And so the way you would implement a context graph is very different. It's more of a traditional graph structure. Maybe it's a relational database with a graph layer on top. >> Yeah. Jay, I don't know if you have any notes to add on that. >> Yeah, I don't think any more to add. >> Got it. One thing I wanted to move on to also is obviously there's a huge amount of community discussion. You also dedicated a slide to just the push backs. I actually wouldn't say push backs, but feel free to just capture</p>
<p>push backs like let's let's go directly and address the elephants in the room. >> Yeah. So, I can take this one. So I think you know um a lot of the you know I don't know if it's push back but like a lot of the people that are say you know maybe taking other opinions and sides which I I love uh cuz it pushes our thinking too is that you know one of the things that we keep hearing is like well you actually can't capture the real why and like you can and you can only capture the how. Um, and that true, you</p>
<p>know, intent is actually like really internal to the human and the only thing you can actually capture is a sequence of actions and interactions. I think Glean actually uses those exact words and they say that you can capture the how. You know, I think that some of that is like in some cases, you know, that's true, but I also think that there's a way to get to like, you know, I think Anime talks about in his blog post, but like the partial why and start to be able to reconstruct the why. And and I think that you know the reason that is</p>
<p>is because like you know I think before AI agents well humans are making these decisions and you know now that agents are going to do the work they sort of need some sort of access to that sort of memory and then two I think LLMs also made some of this capture more feasible. You know prels you kind of needed humans to maybe manually uh structure sort of every decision and no one wants to do that. Like I think they sort of it takes a lot of work to write down why I did something. That's why sometimes like you</p>
<p>go through Salesforce and you look at like the there's a bunch of text boxes about like you know notes and like those notes are probably all always empty is my guess. >> Yeah. Yeah. Someday we'll we'll force all employees to wear a B uh BCI device uh so we can read their thoughts and capture everything. >> Exactly. I will not name the company but I have a portfolio CEO Raj that he and everyone and his team you know just they have this watch thing that they wear that</p>
<p>captures and it auto it autoconnects the data back to their calendar. >> Yeah. Auto auto transcribing I think is is actually honestly like a reasonable thing. The privacy has to be like worked out but honestly you know it's no different than like an employer owning all the Slack conversations and all that. >> Totally. Yeah. you talk you refer to privacy and I would say one of the one of the really critical things which is both there's</p>
<p><strong>[24:11]</strong></p>
<p>some push back but I also think you know we we acknowledge right in the original post is going to be critical is you've got to manage there's data governance issues you know there's there's there's there's is sensitive data that is organization specific so in order to effectively capture these decision traces and then operationalize them in the form of context graph There's a lot of work that companies will have to do around data governance and data management and you know we have a company that's sky flow that J and I worked with that</p>
<p>does a lot of work in PII and sensitive data management. >> Yeah. >> Again it's sky flow. >> Yeah. Yeah. I >> purpose. Yes. >> So you know again they're pioneering an approach to addressing this issue. There's still a lot to be figured out but key to success in building a context graph will be data management data security >> and then I think that the second point that is interesting as well related to that is you know a lot of people kind of like said uh said hey this is like you</p>
<p>know metadata 3.0 and this is like I forgot what someone used it was very funny language but I think it's like you know metadata this is metadata 3.0 point was like the new sparkling water of France or something. I don't know what Gen Z praise is going around. Um but what was interesting I think about that and you know what a lot of people responded to it well hey the same objection was raised about you know CRM about observability and about data warehouses as well and so</p>
<p>new categories I think they emerge when there's a new unit of value worth you know storing and I think the difference here is that decision traces are sort of captured as part of the execution path and not sort of defined up front in all these is like workshops and reconstructs it after the fact via ETL and they're more like emerging as a byproduct of agents doing work which I think is architecturally like a little bit different than metadata layers and I think</p>
<p>>> the second point there is like the difference is also when and how it's sort of captured like I think with um with metadata and you know this also goes to ontologies I think that's like another point that's coming up is that you know there's a lot of like pre-upfront tax of like hey I need to go model a bunch of stuff out with like you know a bunch of consultants, a bunch of stakeholders um and do a bunch of workshops where I think this is a little bit different. >> I think that's that all makes sense. Uh I'm entertained to see this mentioned as data mesh again. I wonder if the data</p>
<p>mesh discussion is is is like um like a bad comparison or actually a pretty good one because in a large enough enterprise company and this piece was very focused on enterprise You do have silos that emerge and sometimes for good reason and sometimes they should not be joined. Yeah, >> I don't know. So, so you know you're on</p>
<p><strong>[27:13]</strong></p>
<p>to something that D and I talk about a lot which is and you know the last slide just references this a little bit. You know the word data mesh can mean whatever you want it to mean. So arguably a contest graph is one form of a data mesh but typically when people have talked about data meshes they have this notion of one universal data mesh across a large enterprise. Yes. And we don't actually believe that that will be the case. We think context graphs because they have to emerge from the automation of a specific set of tasks or</p>
<p>business processes like no one's going to do or can do the work to say let me capture all the decision traces in organization and put them in one universal context graph sounds a little bit like solving world hunger and it's a good idea but very hard to implement. Yes, our core thesis is these context graphs will emerge organically and they'll emerge organically because just as as as processes get automated and human beings are incentivized through be humans in the loop because really they</p>
<p>are providing a lot of the training data. The why is coming from the actions that human beings take as part of a decision in an existing business process automation. And so player zero is doing that for the whole process of code from going into you know testing all the way to code in production and there's an entire workflow once you generate code and they touch every part of that workflow. Similarly, Olive does the same thing for sales. And those are just two different workflows of business process</p>
<p>in a company. Like there's no reason why companies would want to put both data sets or both context graph in one universal context graph in companies or people who are selling context graphs can sell to both of these companies. But ultimately, no one's going to no company's going to wake up one day and say, I want to build a universal content craft. And then if you look at some of the things that Arvin from Blleen has said and Ble's an amazing company and I have a lot of respect for Arvin but it's so horizontal like it's a very powerful chatbot and it allows people to build</p>
<p>some agentic applications or systems of agents on top of them but really the deep crossf functional crossprocess automation problems are going to be solved I think by companies that are dedicated to solving that problem. You know you can call it vertical specific and call horizontal. You know my partner Joanne the J and I work with has a company called Tenor. Tenor is solving the process of automating uh you know the intake process for patients in specialized</p>
<p>healthcare situations, specialized clinics, whether it's sleep clinics, specialized testing processes. So again they're capturing very similar workflow. It's almost like in the old world we call it CRM for for specialized healthcare clinics, but it's so much more today. And they're capturing those decision traces and they'll build the context graph for that. And what Jay and</p>
<p><strong>[30:13]</strong></p>
<p>I think about all day every day is you how do we how do we identify which use cases, business processes, verticals have will have the deepest mopes over the next decade. We could we can build the way we're going to build universal context graph is by having a 100 portfolio companies that have context graphs and the winning ones hopefully hopefully the winning contest graphs. >> Yeah. Yeah. Yeah. Yeah. Okay. I think I think a very useful concept. We're we're coming up on time and I just want to make sure that we've like sort of adequately addressed or cap encapsulated</p>
<p>everything that we know or like the community is excited about. There's there's a lot of like discussions and I think people are going to build out different versions of this. you know what is what is uh do you have any predictions on what we're going to see by your end that are falsifiable that has a chance of being wrong you know like I I think that these are my ways of trying to test have we have we sort of progressed in our understanding of what is what we need to build >> I think that's the alpha I feel like that's the alpha can we give it away >> that's the >> it's the alpha I don't know if we can</p>
<p>give that away >> okay >> I'm kidding >> you know I would say some things are easy to you know there's a lot that we're talking about what's next and so so you know as as J said there's a lot of alpha there but uh look we believe that a year from now you will actually have hundreds of context graphs in production at scale yeah you said that yeah so so that's belief number one two I think the the enabling infrastructure stack for context graphs will be well defined and and there will be variations and flavors</p>
<p>but you know I totally anticipate that this time next year we'll be writing Okay, here's here's the way here's the best practice stack. Just like once upon a time there was the modern data stack, there'll be the context graph stack and it will be 10 times more important and 10 times more valuable. And and lastly, I would say you know the debate about what is a context graph and what are decision traces and what they why they matter will go away and the question will be how do we extract value from context graphs? But we're merely scratching the surface. Once you have a context graph, it's a little bit my</p>
<p>context graph versus yours and different people will do different things to extract value and drive further automation cuz automation is always a staircase. Like the more value you can extract from the context graph, the more you can automate and the more you automate, the more more decision traces you capture, the more you know you will capture your context graph. So the best companies will be debating how to sort of accelerate that flywheel 12 months from now. That's a good uh challenge to folks. Uh you know sometimes I also use</p>
<p>these as kind of a call to action like you know if if people are working on these problems they resonate with it they should reach out to you obviously but I'm sure you have a lot of people reaching out reaching out as well. >> Yeah. >> Well J and I are open for business 247 definite holidays. So give us a shout. >> Yeah we're we're recording on a holiday for those listening uh later. But no thank you so much for joining and uh congrats on like basically creating this category. I I'm excited to see what you</p>
<p><strong>[33:14]</strong></p>
<p>do with it. Uh I think it's early days still. I think we'll still be checking back on this uh you know 6 months, 12 months from now and uh and we'll hope to see like a lot more people uh building out what they see as contests within their organizations. So thank you. >> Thank you so much. Take care. [music] [music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=zP8P7hJXwE0</guid>
      <pubDate>Wed, 04 Feb 2026 02:59:23 +0000</pubDate>
    </item>
    <item>
      <title>Goodfire AI’s Bet: Interpretability as the Next Frontier of Model Design — Myra Deng &amp; Mark Bissell</title>
      <link>https://www.youtube.com/watch?v=ck63uv6APBA</link>
      <description>From Palantir and Two Sigma to building Goodfire into the poster-child for actionable mechanistic interpretability, Mark Bissell (Member of Technical Staff) and Myra Deng (Head of Product) are trying to turn “peeking inside the model” into a repeatable production workflow by shipping APIs, landing real enterprise deployments, and now scaling the bet with a recent $150M Series B funding round at a $1.25B valuation. (https://www.goodfire.ai/blog/our-series-b)

In this episode, we go far beyond the usual “SAEs are cool” take. We talk about Goodfire’s core bet: that the AI lifecycle is still fundamentally broken because the only reliable control we have is data and we post-train, RLHF, and fine-tune by “slurping supervision through a straw,” hoping the model picks up the right behaviors while quietly absorbing the wrong ones. Goodfire’s answer is to build a bi-directional interface between humans and models: read what’s happening inside, edit it surgically, and eventually use interpretability during training so customization isn’t just brute-force guesswork. (https://www.goodfire.ai/blog/on-optimism-for-interpretability)

We discuss:
• Myra + Mark’s path: Palantir (health systems, forward-deployed engineering) → Goodfire early team; Two Sigma → Head of Product, translating frontier interpretability research into a platform and real-world deployments
• What “interpretability” actually means in practice: not just post-hoc poking, but a broader “science of deep learning” approach across the full AI lifecycle (data curation → post-training → internal representations → model design)
• Why post-training is the first big wedge: “surgical edits” for unintended behaviors likereward hacking, sycophancy, noise learned during customization plus the dream of targeted unlearning and bias removal without wrecking capabilities
• SAEs vs probes in the real world: why SAE feature spaces sometimes underperform classifiers trained on raw activations for downstream detection tasks (hallucination, harmful intent, PII), and what that implies about “clean concept spaces”
• Rakuten in production (https://www.goodfire.ai/research/rakuten-sae-probes-for-pii-detection): deploying interpretability-based token-level PII detection at inference time to prevent routing private data to downstream providers plus the gnarly constraints: no training on real customer PII, synthetic→real transfer, English + Japanese, and tokenization quirks
• Real-time steering at frontier scale: a demo of steering Kimi K2 (~1T params) live and finding features via SAE pipelines, auto-labeling via LLMs, and toggling a “Gen-Z slang” feature across multiple layers without breaking tool use
• Hallucinations as an internal signal: the case that models have latent uncertainty / “user-pleasing” circuitry you can detect and potentially mitigate more directly than black-box methods
• Steering vs prompting (https://www.goodfire.ai/blog/feature-steering-for-reliable-and-expressive-ai-engineering): the emerging view that activation steering and in-context learning are more closely connected than people think, including work mapping between the two (even for jailbreak-style behaviors)
• Interpretability for science: using the same tooling across domains (genomics, medical imaging, materials) to debug spurious correlations and extract new knowledge up to and including early biomarker discovery work with major partners

—

Goodfire AI
• Website: https://goodfire.ai
• LinkedIn: https://www.linkedin.com/company/goodfire-ai/
• X: https://x.com/GoodfireAI

Myra Deng
• Website: https://myradeng.com/
• LinkedIn: https://www.linkedin.com/in/myra-deng/
• X: https://x.com/myra_deng

Mark Bissell
• LinkedIn: https://www.linkedin.com/in/mark-bissell/
• X: https://x.com/MarkMBissell

00:00 Introduction
00:45 Welcome + episode setup + intro to Goodfire
02:16 Fundraise news + what’s changed recently
02:44 Guest backgrounds + what they do day-to-day
05:52 “What is interpretability?” (SAEs, probing, steering and quick map of the space)
08:29 Post-training failures (sycophancy/reward hacking) + using interp to guide learning
10:26 Surgical edits: bias vectors + grokking/double descent + subliminal learning
14:04 How Goodfire decides what to work on (customers → research agenda)
16:58 SAEs vs probes: what works better for real-world detection tasks
19:04 Rakuten case study: production PII monitoring + multilingual + token-level scrubbing
22:06 Live steering demo on a 1T-parameter model (and scaling challenges)
25:29 Feature labeling + auto-interpretation + can we “turn down” hallucinations?
31:03 Steering vs prompting equivalence + jailbreak math + customization implications
38:36 Open problems + how to get started in mech interp
46:29 Applications: healthcare + scientific discovery (biomarkers, Mayo Clinic, etc.)
57:10 Induction + sci-fi intuition (Ted Chiang)</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>If you ask 50 people who quote unquote work in interp like what is interpretability, you'll probably get [music] 50 different answers to some extent. Also, like we're where Goodfire sits in the space. We're an AI research company above all else and interpretability is a is a set of methods that we think are really useful and worth [music] kind of specializing in in order to accomplish the goals we want to accomplish. But I think we also sort of see some of the goals as even more broader as as almost like the science of deep learning and just taking a not blackbox [music] approach to sort of internal representations and then</p>
<p>bringing interpretability to training which I don't think has been done all that much before. So welcome to the lane space. We're back in the studio with our special mech co-host Vivu. Welcome >> Mochi. Moi special co-host >> and Mochi the mechanistic interpretability doggo. Uh we have with us Mark and Myra from Goodfire. Welcome. >> Thanks for having us on. >> Maybe we can sort of introduce Goodfire</p>
<p>and then and then introduce you guys. How do you introduce Goodfire today? >> Yeah. Uh it's a great question. So GoodFire we like to say is an AI research lab that focuses on using interpretability to understand, learn from, and design AI models. And we really believe that interpretability will unlock the new generation, next frontier of safe and powerful AI models. Um, that's our description right now. And I'm excited to dive more into the work we're doing to make that happen.</p>
<p>>> Yeah. And there's there's always like the official description. Is there like an unofficial one that sort of resonates more with a different audience? Well, being an AI research lab that's focused on interpretability, there's obviously a lot of people have a lot that they think about when they think of interpretability. And I think we have a pretty broad definition of what that means and the types of places that can be applied and in particular applying it in production scenarios in highstakes industries and really taking it sort of from the research world into the real</p>
<p>world which you know it's a it's a new field so that hasn't been done all that much and we're excited about actually seeing that sort of put into put into practice. Yeah, I would say it's it wasn't too long ago that topic was like still putting out like toy models or supervisition and that kind of stuff. And I wouldn't have pegged it to be this far along. Uh when you and I talked at New Reps, you were talking a little bit about your production use cases and your customers. And then not to bury the lead, today we're also announcing the fund raise uh your series B $150 million</p>
<p>at a 1.25b valuation. Congrats. You're a unicorn. >> Thank you. Yeah. No, things move fast. We were talking to you in December and already some big updates since then. >> Let's dive I guess into a bit of your backgrounds as well. Mark, you you were at Palanteer uh working on health stuff which is uh really interesting because uh GoodFire has some interesting like health use cases. I don't know how related they are in practice.</p>
<p><strong>[03:01]</strong></p>
<p>>> Yeah. Not not super related but um I don't know it was helpful context to know what it's like just to work with uh health systems and generally in that domain. >> Yeah. And Mara, you were at Two Sigma, which actually I was also at, uh, Two Sigma. Really back in the day. >> Wow, nice. Did we overlap at all? >> No. Uh, this is this is when I was the briefly a software engineer before I became a sort of developer relations person and now you head of product. >> What are your sort of respective roles just to introduce people to like what all gets done in Goodfire? >> Yeah, prior to Goodfire, I was at Palunteer for about 3 years uh as a as a</p>
<p>forward deployed engineer, now a now a hot term. uh wasn't always that way and as a technical lead on the healthcare team and at Goodfire I'm a member of the technical staff um and honestly that I think is about as specific as [laughter] like as as I could describe myself cuz I've worked on a range of things and you know it's it's a fun time to be at a team that's still reasonably small. I think when I joined one of the first like 10 employees now we're above 40 but still it looks like there's always a mix</p>
<p>of research and engineering and product and all of the above that needs to get done and I think everyone across the team is is you know pretty pretty switch hitter in the roles they do. So I think you've seen some of the stuff that that I worked on related to image models which was sort of like a research demo. More recently, I've been working on our scientific discovery team with some of our life sciences partners, but then also building out our our core platform from more of like a flexing some of the kind of MLE and and developer skills as well. >> Very generalist. Um, and you you also</p>
<p>had like a very like a founding engineer type role. >> Yeah. Yeah. So, I I also started as I still am member of technical staff. Did a wide range of things from the very beginning including like finding our office space and all of these like nittygritty. people visited when you had that open house thing. It was really nice. >> Thank you. Thank you. Yeah. Plug to come visit our office. Um >> like it was it was like 200 people like it has room for 200 people but there you guys were like 10. [laughter and clears throat] >> Yeah. >> For a while it was very empty. >> But yeah like like Mark I I spend a lot</p>
<p>of my time as as head of product. I think product is a bit of a weird role these days, but a lot of it is thinking about um how do we take our frontier research and really apply it to the most important real world problems and how does that then translate into a platform that's repeatable or a product and working across you know the engineering and research teams to make that happen. And also communicating to the world like what is interpretability, what is it used for, what is it good for, um why is it so important? All of these things are</p>
<p>part of my day-to-day as well. >> I love like what is things because that's a very crisp like starting point for people like coming to a field. Leo I'll do a fun thing VU why you want to try tackling what is interpretability and then they can correct us. >> Okay great. Um so I think like one just to kick off it's a very interesting role to be hit a product right because you guys at least as a lab you're more of an applied interp lab right which is pretty</p>
<p><strong>[06:03]</strong></p>
<p>different than just normal interp like a lot of background research but you guys actually ship an API to try these things you have ember you have products around it which not many do okay what is interp so basically you're trying to have an understanding of what's going on in model like in the model in the internals so different approaches to do that you can do probing saes transcoders, all this stuff. But basically, you have an you have a hypothesis. You have something that you want to learn about what's happening in a model internals and then you're trying to solve that. From there, you can do stuff like you can, you know, you can do activation</p>
<p>mapping. You can try to do steering. There's a lot of stuff that you can do. But the key question is, you know, from input to output, we want to have a better understanding of what's happening and, you know, how can we how can we adjust what's happening on the model internals? How'd I do? >> That was really good. I think that was great. I I think it's also a it's kind of a minefield of a if you ask 50 people who quote unquote work in interp like what is interpretability, you'll probably get 50 different answers. And to some extent also like where where</p>
<p>GoodFire sits in the space. I think that we're an AI research company above all else and interpretability is a is a set of methods that we think are really useful and worth kind of specializing in in order to accomplish the goals we want to accomplish. But I think we also sort of see some of the goals as even more broader as as almost like the science of deep learning and just taking a not blackbox approach to kind of any part of the like AI development life cycle whether that means using inter for like data curation while you're training your</p>
<p>model or for understanding what happened during post- training or for the you know understanding activations and sort of internal representations what is in there semantically and then a lot of sort of exciting updates that were, you know, are sort of also part of the the fund raise around bringing interpretability to training, which I don't think has been done all that much before. A lot of the stuff is sort of post talk poking at models as opposed to actually using this to intentionally design them. Is this post- training or pre-training or is is that not</p>
<p>>> focused on post- training? But there's no reason the techniques wouldn't also work in pre-training. It seems like it would be more act applicable post training because basically I I'm thinking like rollouts or like um you know having different variations of a model that you can tweak with your >> steering. Yeah. And I think in a lot of the news that you've seen in in on like Twitter or whatever, you've seen a lot of unintended side effects come out of post-training processes. You know, overly sycopantic models or models that exhibit oh god strange reward hacking</p>
<p>behavior. I think these are like extreme examples. There's also, you know, very uh mundane more mundane like enterprise use cases where, you know, they try to customize or post-train a model to do something and it learns some noise or it doesn't appropriately learn the target task. And a big question that we've always had is like how do you use your understanding of what the model knows</p>
<p><strong>[09:03]</strong></p>
<p>and what it's doing to actually guide the learning process more effectively? >> Yeah. I mean, you know, just to anchor this for people, uh, la one of the biggest controversies of last year was 40 glazegate. [laughter] I've never heard I didn't know that was what it was called. >> No, the other one they called it that on the blog post and I was like [laughter] why the opening I call it like officially used that term and I'm like that's funny but like yeah I I guess is is the pitch that if they had worked to a good fire they wouldn't have avoided it like you know [laughter] >> I think so. >> Yeah. >> I I think that's certainly one of the use cases I think and another reason why</p>
<p>post training is a place where this makes a lot of sense is a lot of what we're talking about is surgical edits. you know, you want to be able to have expert feedback very surgically change how your model is doing. Whether that is, you know, removing a certain behavior that it has. So, you know, one of the things that we've been looking at or is is another like common area where you would want to make a somewhat surgical edit is some of the models that have say political bias. Like you look at Quen or um R1 and they have sort of like this CCP bias in them. And you know,</p>
<p>>> is there a CCP vector? Well, there's there are certainly internal Yeah. parts of the representation space where you can sort of see where that lives. Yeah. [laughter] Um and you want to kind of, you know, extract that piece out. >> Well, I always say, you know, whenever you find a vector, a fun exercise is just like make it very negative to see what what the opposite of CCP is. [laughter] >> The Super America bald eagles flying everywhere. But yeah, so in general like lots of post- training tasks where you'd want to be able to to do that whether it's unlearning a certain behavior or you know some of the other kind of cases</p>
<p>where this comes up is are you familiar with like the the groing behavior? >> I mean I know the machine learning term of groing. >> Yeah. sort of this like double descent idea of of having a model that is able to learn a generalizing a generalizing solution as opposed to even if memorization of some task would suffice, you want it to learn the more general way of doing a thing. And so, you know, another way that you can think about having surgical access to a model's internals would be learn from this data</p>
<p>but learn in the right way. um if there are many possible you know ways to to do that >> can meant trips solve the double descent problem >> depends I guess on how you >> okay so I I I view that double ascent as a problem because then you're like well if the loss curves level out then you're done but maybe you're not done >> right [clears throat] right >> but like if you actually can interpret what is uh generalizing or what is what is still changing even though the loss is not changing then maybe you you can actually not view it as a double the</p>
<p>problem and actually you're just sort of uh translating the space in which you view loss and you like then you have a smooth curve. >> Yeah, [laughter] I I think that's certainly like the domain of of problems that we're that we're looking to get at. Yeah. to me like double descent is like the biggest thing to like ML research where like if you believe in scaling then you don't you need to know where to scale and but if you believe in double</p>
<p><strong>[12:04]</strong></p>
<p>descent then you don't you don't believe in anything where like anything levels off like [laughter] >> yeah I mean also tangentially there's like okay when you talk about the China vector right there's the subliminal learning work it was from the anthropic fellows program where basically you can have hidden biases in a model and as you distill down or you know as you train on distilled data, those biases always show up even if like you explicitly try to not train on them. So, you know, it's just like another use case of okay, if we can interpret what's happening in post- training, you know, can we clear</p>
<p>some of this? Can we even determine what's there? Because, yeah, it's just like some worrying research that's out there that shows, you know, we really don't know what's going on. >> That is, yeah, I think that's the biggest sentiment that we're sort of hoping to tackle. Nobody knows what's going on, right? Like subliminal learning is just an insane concept when you think about it, right? Train a model on not even the lojits literal literally the output text of a bunch of random numbers and now your model loves owls. And you see behaviors like that that are just they defy they defy intuition and</p>
<p>and there are mathematical explanations that you can get into, but >> I mean early days >> objectively there are a sequence of numbers that are more allike than others. There there should be >> according [laughter] to according to certain models, right? It's interesting. I think it only applies to models that were initialized from the same starting >> usually. Yes. But I mean I think that's a that's a cheat code because there's not enough compute. But like it if you believe in like platonic representation like probably will transfer across</p>
<p>different models. >> Oh, you think so? I think of it more as a statistical artifact of models initialized from the same seed sort of um there's something that is like path dependent from that seed that might cause certain overlaps in the latent space and then sort of doing this distillation. Yeah. Like pushes it towards having certain other tendencies. >> Got it. >> I think research >> a bunch of these open-ended questions, right? like you can't train in new stuff</p>
<p>during the RL phase, right? RL only reorganizes weights and you can only do stuff that's somewhat there in your base model. You're not learning new stuff. You're just reordering chains and stuff. But okay, my broader question is when you guys work at an interp lab, how do you decide what to work on and what's kind of the thought process? Right? Cuz we can ramble for hours. Okay, I want to know this, I want to know that, but like how do you concretely like you know what's the workflow? Okay, there's like approaches towards solving a problem, right? I can try prompting. I can look at chain of thought. I can train probes,</p>
<p>SAEs. But how do you determine, you know, like okay, is this going anywhere? Like do we have set stuff? Just, you know, if you can talk about that. >> It's a really good question. I feel like we've always um at the very beginning of the company thought about like let's go and try to learn what isn't working in machine learning today. whether that's talking to customers or talking to researchers at other labs trying to understand um both where the frontier is</p>
<p><strong>[15:04]</strong></p>
<p>going and where things are are really not falling apart today and then developing a perspective on how we can push the frontier using interpretability methods. Um and so you know even our chief scientist Tom spends a lot of time talking to customers and trying to understand what real world problems are and then taking that back and trying to apply the current state-of-the-art in inter to those problems and then seeing where they fall down basically and then uh using that those failures or those shortcomings to understand what hills to</p>
<p>climb when it comes to interpretability uh research. So like on the fundamental side for instance when we have done some work applying SAE and and probes we've encountered you know some shortcomings in in Saes that we found a little bit surprising and so have gone back to the drawing board and and done work on um better foundational interpreter models and a lot of our team's research is focused on what is the next evolution beyond SAES for instance and then when it comes to like control and design of</p>
<p>models you know we tried steering with our first API and realized that it still fell short of blackbox techniques like prompting or fine-tuning and so went back to the drawing board and were like how do we make that not the case and how do we improve it beyond that and one of our researchers ECDE who just joined is actually ECDE and Attakus are like steering experts and and have spent a lot of time trying to figure out like what is the research that enables us to actually do this in a much more powerful robust way. So yeah, the answer is like look at real world problems, try to</p>
<p>translate that into a research agenda and then like hill climb on on both of those at the same time. >> Yeah, Mark has the steering CLI demo queued up which we're going to go into a sec, but I always want to double click on when you drop hints like we found some problems with SAEs. Okay, what are they? You know, and uh let's let's then we then we can go into the demo. >> Yeah, I mean I I'm curious if you have more thoughts here as well because you've done it in in the healthcare domain. Um, but I think like for instance when we do things like trying to detect behaviors within models that</p>
<p>are harmful or like behaviors that a user might not want to have in their model. So hallucinations for instance, harmful intent, PII, all of these things. We first tried using SAPE probes for a lot of these tasks. So taking the feature activation space from SAEs and then training classifiers on top of that and then seeing how well we can detect the properties that we might want to detect in model behavior. And we've seen in many cases that probes just trained on raw activations seem to perform</p>
<p>better than SAPE probes, which is a bit surprising if you think that SAEs are actually also capturing the concepts that you would want to capture cleanly and more surgically. And so that is an interesting observation. I don't think that is like I'm not down on SAEs at all. I think there are many many things they're useful for. But we have definitely run into cases where I think the concept space described by SAEs is</p>
<p><strong>[18:05]</strong></p>
<p>not as clean and uh accurate as as we would expect it to be for actual like real world downstream performance um metrics. >> Fair enough. >> Yeah. >> Yeah. It's the blessing and the curse of unsupervised methods where you got to peek into the AI's mind, but sometimes you wish that you saw other things when you when you looked [laughter] inside there. >> Although in the in the PII instance, I think weren't SA an SAPE based approach actually did prove to be the most generalizable. >> Well, in in the case that we published with Rocketin and I think a lot of the reasons it worked well was because we</p>
<p>had a noisier data set. And so actually the blessing of unsupervised learning is that we actually got uh to get more meaningful generalizable signal from SAEs when the data was noisy. But in other cases where we've had like good data sets, it hasn't been the case. >> And just because you named Rakutin and I don't know if we'll get it another chance like uh what what is the overall like what is Rakutin's uh usage uh or production usage? Yeah. So they are using us to essentially guard rail and inference time monitor their language</p>
<p>model usage and their agent usage to detect things like PII so that they don't route um private user information to um downstream model providers. And so that's you know going through all of their user queries uh every day. And that's something that we deployed with them a few months ago. And now we are actually exploring um very early partnerships not just with Rocketin but with other people around how we can help with potentially training and customization use cases as well.</p>
<p>>> Yeah. And for those who don't know like it's Racketin is like I think number one or number two e-commerce >> store in Japan. >> Yes. >> Yeah. Yeah. And I think that use case actually highlights a lot of like what it looks like to deploy things in practice that you don't always think about when you're doing sort of research tasks. So when you think about some of the stuff that came up there that's more complex than your idealized version of a problem, they were encountering things like synthetic toreal transfer of methods. So they couldn't train probes, classifiers, things like that on actual</p>
<p>customer data of PII. So what they had to do is use synthetic data sets and then hope that that transfers out of domain to real data sets and so we could evaluate performance on the real data sets but not not train on customer PII. So that right off the bat is like a big challenge. You have multi-ingual requirements. So this needed to work for both English and Japanese text. Japanese text has all sorts of quirks including tokenization behaviors that caused lots of bugs that caused us to be pulling our hair out. And then also a lot of tasks</p>
<p>you'll see you might make simplifying assumptions if you're sort of treating it as like the easiest version of the problem to just sort of get like general results where maybe you say you're classifying a sentence to say does this contain PII but the need that racketin had was token level classification so that you could precisely scrub out the PII. So as we learned more about the</p>
<p><strong>[21:07]</strong></p>
<p>problem and you're sort of speaking about what that looks like in practice. Yeah. a lot of assumptions end up breaking and that was just one instance where you a problem that seems simple right off the bat ends up being more complex as you keep diving into it. >> Excellent. One of the things that's also interesting with interp is a lot of these methods are very efficient right so where you're just looking at a model's internals itself compared to a separate like guardrail LM as a judge a separate model one you have to host it two there's like a whole latency so if you use like a big model you have a second call some of the work around like</p>
<p>self detection of hallucination it's also deployed for efficiency right so thinking of someone like rakuten doing it in production live you know that's just another thing people should consider Yeah. And something like a probe is super lightweight. Adds no extra latency really. >> Excellent. Uh you have the steering demos uh lined up. So we were just kind of see what you got. I I don't I don't actually know if this is like the latest latest or like alpha thing. >> No, this is a pretty hacky demo from uh from a presentation that someone else on</p>
<p>the team recently gave. So this will give a sense for for steering in action. Honestly, I think the biggest thing that this highlights is that as we've been growing as a company and taking on kind of more and more ambitious versions of interpretability related problems, a lot of that comes to scaling up in various different forms. And so here you're going to see steering on a one trillion parameter model. This is Kimmy K2. Uh, and so it's sort of fun that in addition to the research challenges, there are</p>
<p>engineering challenges that we're now tackling because for any of this to be sort of useful in production, you need to be thinking about what it looks like when you're using these methods on frontier models um as opposed to sort of like toy kind of model organisms. So yeah, this was thrown together hastily, pretty fragile behind the scenes, but uh I think it's quite a fun demo. So screen sharing is is on. So, I've got two terminal sessions pulled up here. On the left is a forked version that we have of the Kimmy uh CLI that we've got running to point at our custom hosted Kimmy</p>
<p>model. And then on the right is a uh setup that will allow us to steer on certain concepts. So, I should be able to chat with Kimmy over here. Tell it hello. >> Is this running locally? So the CLI is running locally, but the Kimmy server is running back to the office. Well, hopefully should be. [laughter] Um, >> that's too much to run on that Mac. >> Yeah, I think it's uh it takes a full like H100 node. I think it's like you can run it on 8GPUs. H100. So So yeah,</p>
<p>Kimmyy's running. We can ask it to prompt. It's got a forked version of our uh of the SGline codebase that we've been working on. So I'm going to tell it, hey, this SGLine codebase is slow. I think there's a bug. Can you try to figure it out? It's a big code base, so it'll it'll spend some time doing this. And then on the right here, I'm going to initialize in real time some steering.</p>
<p><strong>[24:09]</strong></p>
<p>Let's see here. Continue searching for any bugs. >> Feature ID 43205 layers 20 30 40. >> So, let me uh this is basically a feature that we found that inside Kimmy seems to cause it to speak in Gen Z slang. >> [laughter] >> And so on the left, it's still sort of thinking normally. It might take, I don't know, 15 seconds for this to kick in, but then [clears throat] we're going to start hopefully [laughter] seeing it. Dude, this code base is massive for</p>
<p>real. [laughter] So, we're going to start seeing Kimmy transition as the steering kicks in from normal Kimmy to Gen Z Kimmy [laughter] and both in its chain of thought and its actual outputs. And interestingly, you can see, you know, it's still able to call tools uh and stuff. It's um it's purely sort of its its demeanor. And there are other features that we found for interesting things like concision. So, that's more</p>
<p>of a practical one. You can make it more concise. Um the types of programs uh programming languages it uses. But yeah, as we're seeing it come in, pretty good output. >> Scheduler code is actually wildl [laughter] um something about how agents for interp</p>
<p>is different than like coding agents. I don't know. While this is spewing up, how how do we find feature 43205? >> Yeah. So, in this case, um we our platform that we've been building out for a long time now supports all the sort of classic out of the box interp techniques that you might want to have like SAPE training, probing, things of that kind. I'd say the techniques for like vanilla ses are pretty wellestablished now where you take your model that you're interpreting run a</p>
<p>whole bunch of data through it gather activations and then yeah pretty straightforward pipeline to train an SAPE there are a lot of different varieties there's top kes batch top kes um normal relu seaes and then once you have your sparse features to your point assigning labels to them to actually understand that this is the Gen Z feature. That's actually where a lot of the kind of magic happens. And the most basic standard technique is look at all of your input data set examples that</p>
<p>cause this feature to fire most highly and then you can usually pick out a pattern. So for this feature, if I've run a diverse enough data set through my model, feature 43.205 probably tends to fire on all the uh tokens that sound like Gen Z slang. And um so you know you could have a human go</p>
<p><strong>[27:11]</strong></p>
<p>through all 43,000 concepts and look at the pattern but to automate that you just kind of hand those examples off to a frontier LLM and ask it to identify that pattern. >> And I've got to ask the basic question you know can we get examples where it hallucinates pass it through see what feature activates for hallucinations? Can I just you know turn hallucination down? >> Oh wow. you you really uh solved it. >> You really predicted [laughter] some a project we're already working on right now, which is um detecting hallucinations using interpretability techniques. And this is interesting because hallucinations is something</p>
<p>that's very hard to detect and it's like a kind of a hairy problem and something that blackbox methods really struggle with. Um whereas like Gen Z, you could always train a classifi a simple classifier to to detect that um hallucinations is harder. But we've seen that models internally have some awareness of of like uncertainty or some sort of like user pleasing behavior that leads to hallucinatory behavior. Um, and so yeah, we have a project that's trying to detect that accurately and then also working on mitigating the hallucinatory</p>
<p>behavior in the model itself as well. >> Yeah. And I would say most people are still at the level of like, oh, I'll just turn temperature to zero and that turns off hallucination. And I'm like, well, that's a fundamental misunderstanding of how this works. >> Yeah. Although, so part of what I like about that question is you there are SAPE based approaches that might like help you get at that. But often times the beauty of SAEs and like we said the curse is that they're unsupervised. So when you have a behavior that you deliberately would like to remove and that's more of like a supervised task,</p>
<p>often it is better to use something like probes and and specifically target the thing that you're interested in reducing as opposed to sort of like hoping that when you fragment the latent space, one of the vectors that pops out will be the thing you're interested in. And as much as we're training an autoenccoder to be sparse, we're not like for sure certain that, you know, we will get something that just correlates to hallucination, right? you'll you'll probably split that up into 20 other things and who knows what they'll be</p>
<p>>> of course right yeah so there's known sort of problems with like feature splitting um and feature absorption and then there's the offt target effects right ideally you would want to be very precise where if you reduce the hallucination feature suddenly maybe your model can't write creatively anymore and maybe you don't like that but you want to still stop it from hallucinating facts and figures >> good uh so people has a paper to recommend there that we'll put in the show notes but yeah I mean I guess just because your demo is done. Uh any any other things that you want to highlight or any other interesting features you want to show?</p>
<p>>> I don't think so. Yeah, like I said, this is a pretty small snippet. I think the main sort of point here that I think is exciting is that there's not a whole lot of inter being applied to models quite at at this scale. You know, Anthropic certainly has some some research and yeah, other other teams as well. But it's it's nice to see these techniques, you know, being put into practice. I think not that long ago the idea of real time steering of a trillion parameter model would have sounded</p>
<p><strong>[30:12]</strong></p>
<p>>> yeah the fact that it's real time like you you started the thing and then you edited uh the steering vector I think it's it's an interesting one uh TBD what the actual like production use case would be on that like the real-time editing um >> it's like that's the fun part of the demo right you can kind of see how this could be served behind an API right like you're you only have so many knobs and you can just tweak it a bit more and I don't know how it plays in people haven't done that much with like how does this work with or without prompting right how does this work with fine-tuning like there's the whole hype of continual learning right so there's</p>
<p>just so much to see like is this another parameter like is it like parameter we just kind of leave it as a default we don't use it so I don't know maybe someone here wants to put out a guide on like how to use this with prompting when to do what >> oh well I have a paper recommendation that I think you would love from uh ectep on our team who um is an amazing research just can't say enough amazing things about act. He actually has a paper that as well as some you know others from from the team and elsewhere that go into the essentially equivalents</p>
<p>of activation steering and in context learning and how those are from a he he thinks of everything in a cognitive neuroscience basian framework. But basically how you can precisely show how prompting in context learning and steering exhibit similar behaviors and even like get quantitative about the like magnitude of steering you would need to do to induce a certain amount of behavior similar to certain prompting uh even for things like jailbreaks and</p>
<p>stuff. It's a really cool paper. >> Are you saying steering is less powerful than prompting? more like you can almost write a formula that tells you how to convert between the two of them and so >> be formally equivalent actually in the in the limit >> right so like one case study of this is for jailbreaks there I don't know have you seen the stuff where you can do like manyot jailbreaking you like flood the context with examples of the behavior >> when put out that paper a lot of people</p>
<p>were like yeah we've been doing this guys like what [laughter] happens Yeah. Um, what's in this in context learning and activation steering equivalence paper is you can like predict the number of examples that you will need to to put in there in order to jailbreak the model. That's cool. >> By doing steering experiments and using this sort of like um equivalence mapping. >> That's cool. That's really cool. >> That's very neat. >> Yeah. I was going to say like uh you know I can like back rationalize that this makes sense because you know what context is is basically just you know it</p>
<p>updates the KV cache kind of and like and and and then every next token inference is still like you know that the the sheer sum of everything all the weights plus all the context up to date and you could I guess theoretically steer that with uh you probably replace that with your um steering. The only problem is steering typically is on one layer, maybe three layers like like you did. So it's like not exactly equivalent.</p>
<p><strong>[33:13]</strong></p>
<p>>> Right. Right. There's sort of you need to uh get precise about Yeah. like how you sort of define steering and like what how how you're modeling the setup. But yeah, I've got the paper pulled up here. Belief dynamics reveal the dual nature. >> Yeah. The title is belief dynamics reveal the dual nature of in context learning and activation steering. So Eric Bigalow um Dana Warcraft on the um who are uh doing fellowships at Goodfire ECE's the the final author there. I think actually to your your question of like what is the production use case of steering I think maybe if you just think like one level beyond steering as as it</p>
<p>is today like imagine if you could adapt your model to be I don't know an expert legal reasoner like in almost real time like very efficiently using human feedback or using like your semantic understanding of what the model knows and where it knows that uh behavior. I think that while it's not clear what the product is at the end of the day, it's clearly very valuable. Thinking about like what's the next interface for for model customization and adaptation is a</p>
<p>really interesting um problem for us. Like we have heard a lot of people actually interested in fine-tuning and RL for openweight models in in production. And so people are using things like um Tinker or kind of like uh open source libraries to do that. But it's still very difficult to get models fine-tuned and RL for exactly what you want them to do unless you're an expert at at model training. And so that's like something we're looking into. >> Yeah. Um I never thought so Tinker from thinking machines famously uses uh rank</p>
<p>one Laura. Is that basically the same as steering? Like you know what's the comparison there? >> Well, so in that case you are still applying updates to the parameters, right? You're not you're not touching a base model. You're you're touching an adapter. It's kind of >> Yeah. >> Right. But I I guess it still is like more in parameter space than I guess it's maybe like are you modifying the pipes or are you modifying the water flowing through the pipes? >> Okay. >> To get what you're after. >> Yeah. >> Is maybe one way. [laughter]</p>
<p>>> I like that analogy. >> That that's my mental map of it at least. But it gets at this idea of model design and intentional design which is something that we're that we're very focused on. And just the fact that like I hope that we look back at how we're currently training models and post-training models and just think what a primitive way of doing that right now. Like there's no intentionality really in >> it's just data right the only thing can control what data we feed in. So, so Dan from Goodfire likes to use this analogy</p>
<p>of um, you know, he has a couple of young kids and he talks about like, "What if I could only teach my kids how to be good people by giving them cookies or like, you know, giving them a slap on the wrist if they do something wrong?" Like not telling them why it was wrong or like what they should have done differently or something like that. Just figure out action. Right. Exactly.</p>
<p><strong>[36:13]</strong></p>
<p>>> So that's RL. >> Yeah. Right. And then, you know, it's sample inefficient. There's, you know, what do they say? It's like slurping uh feedback. It's like slurping supervision throughout. And so you'd like to get to the point where you can have experts giving feedback to their models that are uh internalized >> and and you know steering is >> an inference time way of sort of getting that idea but ideally you're moving to a a world where it is much more intentional design in perpetuity for</p>
<p>these models. Okay, this is one of the questions we asked Emanuel from Anthropic on the podcast a few months ago. Basically, the question was, you're at a research lab that does model training, foundation models, and you're on an interp team. How does it tie back, right? Like, does this do ideas come from the pre-training team? Do they go back? Um, you know, so for those interested, you can you can watch that. There wasn't too much of a connect there, but it's still something, you know, it's something they want to push for down the line. >> It can be useful for all of the above. like there are certainly post hawk use</p>
<p>cases where it doesn't need to touch that. I think the other thing a lot of people forget is this stuff isn't too computationally expensive right like I would say if you're interested in getting into research mechan is one of the most approachable fields right a lot of this train a probe this stuff like the budget for this one there's already a lot done there's a lot of open source work you guys have done some too um you know you can >> there's like there's like notebooks from the Gemini team from Neil Nanda who are like this is how you do it just step</p>
<p>through the notebook >> even if you're like not even technical with any of this you can still make like progress there. You can look at different activations, but uh if you do want to get into training, you know, training this stuff, correct me if I'm wrong, is like in the thousands of dollars, not even like it's not that high scale. And then same with like, you know, applying it, doing it for post- training, RL, all this stuff is fairly cheap in scale of okay, I want to get into like model training. I don't have compute for like, you know, pre-training stuff. So, it's it's a very nice field to get into. And also, there's a lot of</p>
<p>like open questions, right? There's so many questions we have. Some of them have to go with, okay, I want a product. I want to solve this. Like, there's also just a lot of open-ended stuff that people could work on that's interesting, right? I don't know if you guys have any calls for like what's open questions, what's open work that you either open collaboration with or like you'd just like to see solved or just, you know, for people listening that want to get into mechan because people always talk about it. What are what are things they should check out? Start of course, you know, join you guys as Well, I'm sure you're >> there's a paper I think from was it Lee uh Shy? It's open problems in</p>
<p>interpretability which I recommend everyone who's interested in the field read. Um just like a really comprehensive overview of what are the things that experts in the field think are the most important problems to be solved. I also think to your point it's been really really inspiring to see I think a lot of young people getting interested in interpretability. actually not just young people also like</p>
<p><strong>[39:14]</strong></p>
<p>scientists who have been you know experts in physics for many years and in biology or things like this um transitioning into interp because the barrier to entry is is you know in in some ways low and and there's a lot of information out there um and ways to get started. There's this anecdote of like professors at university saying that all of a sudden every incoming PhD student wants to study interpretability um which was not the case a few years ago. So it just goes to to show how I guess like exciting the field is, how fast it's</p>
<p>moving, how quick it is to get started and things like that. And also just a very welcoming community. You know, there's an open-source mechan slack channel where people are always posting questions and just folks in the space are always responsive if you ask things on various forums and stuff. Um, but yeah, the open paper uh open problems uh paper is is a really good one >> for other people who want to get started. I think you know Matts is a great program. What's the acronym for? Machine learning and alignment theory scholars I think. [laughter]</p>
<p>>> It's like the normally summer internship style. >> Yeah. But they've been doing it year round now and um actually a lot of our full-time staff have come through that program or or gone through that program and um it's great for anyone who is transitioning into interpretability. Um there's a couple other fellows programs. we do one as well as anthropic and so those are great places to get started if anyone is is interested >> also I think been seen as a research field for a very long time but I think engineers are sorely wanted for for</p>
<p>interpretability as well um especially at good fire but but elsewhere uh as as it does scale up >> I should mention that Lee actually works with you guys right and in the London office and um I'm adding our first ever meant track and edi Europe because I I see this industry applications now emerging and I'm pretty excited to, you know, help get push that along. >> Yeah, I >> it'll effectively be the first industry mechan conference. >> Yeah, I'm uh I'm so glad you added that.</p>
<p>>> You know, it's it's a still a little bit of a bet like this there it's not like that widespread, but like I I can definitely see like this is the time to like really get into it like you want to be early on things >> for sure. And I think the field is understands this, right? So like at ICML the I think like title of the mech interp workshop this year was actionable interpretability and there was a lot of discussion around bringing it to to various domains. Um, >> everyone's everyone's adding like pragmatic pragmatic >> actionable like whatever like okay well we weren't actionable before I guess I</p>
<p>don't know >> and I mean like just you know being at Europe you see the interp room uh one like old school conferences like I think they had a very tiny room till they got lucky and they got it doubled but there's definitely a lot of interest a lot of nich niche research so you see a lot of research coming out of university students we covered the paper uh last week it's two unknown authors, not many citations, but you know, you can make a</p>
<p><strong>[42:16]</strong></p>
<p>lot of meaningful work there. >> One thing I I did want to call out because I think people haven't really uh mentioned this yet is uh just interfer uh I think is is like an abnormally important field. We haven't mentioned this yet. The conspiracy theory last two years ago was when the first SA word came out of anthropic was they they were like, "Oh, we just used uh SAEs to turn the bad code vector down [laughter] uh and then turn up and then code >> turn up the good code." Uh and uh I think like isn't that the the dream like you know like but basically I guess maybe why is it funny like it's it's if</p>
<p>it was realistic it would not be funny it would be like no actually we should do this but it's funny because we know there's like we feel there's some limitations to what steering can do and I think a lot of the public image of uh steering is like the Gen Z stuff like like oh you can make it really love the Golden Gate Bridge or you can make it speak like Gen Z to like be a legal no reason or seems like a huge stretch. >> Yeah. >> And I don't know if that we'll get there this way. >> Yeah. I think um I will say we are announcing something very soon that I</p>
<p>will not speak too [laughter] much about. Um but I think yeah, this is like what we've run into again and again is like we we don't want to be in the world where steering is only useful for like stylistic things. That's definitely not not what we're um aiming for. But I think the types of interventions that you need to do to get to things like legal reasoning um are much more sophisticated and require breakthroughs than in learning algorithms. And that's um >> and is this an emerging property of scale as well?</p>
<p>>> I think so. Yeah. I mean I think scale definitely helps. I think scale allows you to learn a lot of information and and reduce noise across you know large amounts of data. But I also think we think that there's ways to do things much more effectively. um even even at scale. So like actually learning exactly what you want from the data and not learning things that you do that you don't want exhibited in the data. So we're not like anti-scale but we are also realizing that scale is not going to get us to the type of AI development</p>
<p>that we want to be at in in the future as these models get more powerful and get deployed in all these sorts of like mission critical contexts. current life cycle of training and deploying and evaluations is is to us like deeply broken and and has opportunities to to improve. So uh more to come on that very very soon >> and I think that the basically maybe are just like a proof point that these concepts do exist like if you can manipulate them in the precise best way you can get the ideal combination of</p>
<p>them that you desire and steering is maybe the most coar grained sort of peak at what that looks like but I think it's evocative of what you could do if you had total surgical control over >> every >> concept every parameter. Yeah, exactly. >> There were like bad code features. >> I've got it pulled up >> just coincidentally as you guys were talking. So, >> this was like this is exactly what</p>
<p><strong>[45:17]</strong></p>
<p>people thought it >> there's like specifically a code error feature that activates and they show it off. It's not it's not typo detection. It's like it's it's typos in code. It's not typical typos. And you know, you can you can see it clearly activates where there's something wrong in code. And they have like malicious code, code error. They have a whole bunch of sub, you know, subbroken down little grain features. >> Yeah. >> Yeah. So, so the the rough intuition for me, the why I talked about post training was that well, you just, you know, have a few different rollouts with all these things uh turned off and on and whatever</p>
<p>and then, you know, you can that's that's synthetic data you can kind of post train on. >> Yeah. >> And I think we make it sound easier than it is just saying, you know, do they they do the real hard work. >> I mean, you guys you guys have the right idea. Exactly. Yeah. We replicated a lot of these features in in our llama models as well. I remember there was like >> and I think a lot of this stuff is open right like yeah you guys opened yours um deep mind has opened a lot of essays on um Gemma even anthropic has opened a lot of this there's there's a lot of resources that you know we can probably share of people that want to get</p>
<p>involved >> and special shout out to like Neuronedia as well like uh yeah amazing piece of work to visualize those things >> yeah exactly >> I guess I wanted to pivot a little bit on onto the healthcare side uh because I I think that's a big use case for you guys. Uh we haven't really talked about it yet. This is a bit of a crossover for me because we are se we are we do have a separate science pod that we're starting up for [laughter] science. >> Wow. Okay. >> Uh just because like it's such a huge investment category and also I'm like less qualified to do it. We actually have bio PhDs to cover that which is</p>
<p>great but I just kind of recap your work maybe on the EVO 2 stuff but then and then building forward. >> Yeah, for sure. And maybe to frame up the conversation, I think another kind of interesting just lens on interpretability in general is a lot of the techniques that we're describing are ways to solve the AI human interface problem and it's sort of like birectional communication is the goal there. So what we've been talking about with intentional design of models and you know steering but also more advanced</p>
<p>techniques is having humans impart our desires and control into models and over models and the reverse is also very interesting especially as you get to superhuman models whether that's narrow super intelligence like these scientific models that work on genomics data medical imaging things like that but down the line you know super intelligence of other forms as well. what knowledge can the AIS teach us as sort of that that the other direction in that and so some of our</p>
<p>life science work to date has been getting at exactly that question which is well some of it does look like debugging these various life sciences models understanding if they're actually performing well on tasks or if they're picking up on spurious correlations for instance genomics models you would like to know whether they are sort of focusing on the biologically relevant things that you care about or if it's</p>
<p><strong>[48:18]</strong></p>
<p>using some simpler coralate like the ancestry of the person that it's looking at. But then also in the instances where they are superhuman and maybe they are understanding elements of the human genome that we don't have names for or um specific you know yeah discoveries that they've made that that we don't know about. That's that's a big goal. And so we're already seeing that right we are partnered with organizations like Mayo Clinic leading research health system in in the United States Arc</p>
<p>Institute as well as uh a startup called Prima which focuses on neurodeenerative disease and in our partnership with them we've used foundation models they've been training and applied our interpretability techniques to find novel biomarkers for um Alzheimer's disease. So, I think this is just the tip of the iceberg, but it's uh that's like a flavor of some of the things that we're working on. >> Yeah, I think that's really fantastic. We obviously we did the Chan Zuckerberg uh pod last year as well, and like</p>
<p>there's a plethora of these models coming out because there's so much um potential and research. Um and it's like very interesting how it's basically the same as language models, but just with a different underlying data set, but like it's the same exact techniques. like there's no change basically. >> Yeah. [laughter] Well, and even in like other domains, right? Like I you know, robotics I know like a lot of the companies just use Gemma as like the the like backbone and then they like make it into a VLA that like takes these actions. It's it's it's transformers all</p>
<p>the way down. So like we we have Medma now, right? Like this week even there was Medma 1.5. Uh and they're they're training it on this stuff like 3D scans, medical domain knowledge and all that stuff too. So there's a push from both sides but I think the thing that you know one of the things about mechan is like you're a little bit more cautious in some domains right so healthcare mainly being one like guard rails understanding you know we're we're more riskadverse to something going wrong there so even just from a basic</p>
<p>understanding like if we're trusting these systems to make claims we want to know why and what's going on. Yeah, I think there's totally a kind of like deployment bottleneck to actually using foundation models for real patient usage or things like that. Like say you're using a model for rare disease prediction, you probably want some explanation as to why your model predicted a certain outcome and an interpretable explanation at that. So that's definitely a use case. But I also think like being able to extract</p>
<p>scientific information that no human knows to accelerate drug discovery and disease treatment and things like that actually is a really really big unlock for sc like scientific discovery and you've seen a lot of startups like say that they're going to accelerate scientific discovery and I feel like we actually are doing that through our interp techniques and kind of like</p>
<p><strong>[51:18]</strong></p>
<p>almost by accident like I think We got reached out to very very early on from these healthcare institutions and none of us had healthare. >> How did they even hear of you? Like >> a podcast. >> Okay. Yeah, podcast. Okay. Well, now is that time, you know, like [laughter] everyone everyone can call us up. >> Podcast are the most important thing. Everyone should listen to podcast and everyone should come to >> podcast. They were like, you know, we have these really smart models that we've trained and we want to know what they're doing. Um, and we were like really early that time, like 3 months old, and it was a few of us, and we were like, "Oh my god, let's [laughter] we've</p>
<p>never used these models. Let's figure it out." Um, but it's also like great proof that interpret techniques scale pretty well across across domains. We didn't really have to learn too much about >> interp is a a machine learning technique, machine learning skills everywhere, right? Like, and then there's obviously it's just like a general insight. Um, probably to finance too, I think, which would be fun for our history. Um I don't know if you have anything to say there. >> Well, just across the science like we've also done work on material science. Yeah, it it really runs the gambit. >> Yeah. Awesome.</p>
<p>>> And you know, for those that should reach out like you're obviously experts in this, but like is there a call out for people that you're looking to partner with, design partners, people to use your stuff outside of just, you know, the general developer that wants to plugandplay steering stuff? like on the research side more so like are there ideal design partners, customers, stuff like that that >> yeah I can talk about maybe non life sciences and then I'm curious to hear from you on the life sciences side but um we're looking for design partners across many domains language anyone</p>
<p>who's customizing language models or trying to push the frontier of code or reasoning models is really interesting to us and then also interested in the frontier of models that work in like pixel space as we call it. So if you're doing world models, video models, um even robotics um where there's not a very clean natural language interface to interact with um I think we think that interp can really help and are looking for a few partners in in that space. >> Just cuz you you mentioned the keyword</p>
<p>world models um is that a big part of your thinking? Do you have a definition that I can use because everyone's asking me about it [laughter] >> uh about world models? There's quite a few definitions. Let's say >> I don't feel equipped to be [laughter] an expert on world model definitions, but the reason we're interested in them is because they give you like, you know, with language models when you get uh features, you still have to do auto interpret and things like that to actually get an understanding of of what this concept is. But in image and video and world, it's like extremely easy to</p>
<p>to gro what the concept is because you can see it and you can visualize it. And um this makes the feedback cycle extremely fast um for us also for things like I don't know if you take think about probes in language model context and then take it to world models like what if you wanted to detect harmful actors in world model scenes like you can't actually like go and label all of</p>
<p><strong>[54:20]</strong></p>
<p>that data feasibly but you maybe you could um synthetically generate you know I don't know world like harmful actor uh data using SAPE feature activations or whatever. and then actually train a probe that was able to detect that much more scalably. So, [snorts] I just think like video and image and world has always been something we've explored and are continuing to explore. Um, Mark's demo was probably the first moment we really like were like, "Oh, wow." Like, this is really going to this could really like change the world of</p>
<p>>> the steering demo. >> Yeah. No, the image demo. >> The the diffusion one. Exactly. >> Yeah. Yeah. We should probably show that and and you demoed it at Worlds Fair. So we can choose to link that. >> Nice. Yeah, >> people can play with it, right? >> Yes. >> Yes. >> Yeah. I think for for me one way in which I I think about world models is just like this like having this consistent model of the world where everything that you generate operates within the rules of that world. And imagine it would be a bigger deal for science uh or like math or anything that</p>
<p>where like you have verifiable rules whereas I guess in natural language maybe there's less rules. And so it's not not that important. >> Yeah. And and which makes the debugging of the model's internal representations or it's internal world model to the extent you can make that legible and explicit and have control over that. I think it makes it all the more important because in language it's sort of a fuzzy enough domain that if its world model isn't fully like ours, it can still sort of like pass the Turing test so to speak. But I know there have been papers</p>
<p>that have looked at like even if you train certain astrophysics models, it does not learn F= MA. Like the same way that you can, you know, have a model do well for modular arithmetic, but it doesn't really like learn what how we think of modular arithmetic. It learns some crazy heristic that is like essentially functionally equivalent, but it's probably not the sort of groed solution that you would hope for. >> It's how an alien would do it, >> right? Exactly. Uh but but no no I that's probably I think a function of</p>
<p>our learning being bad rather than the well that approach probably not being real because it's it's how we humans learn >> right >> yeah right well it's just it's the problem of induction right all of ML is based on induction and and it's impossible to say I have a physics model you might have a physics model that works all the time except when there is a character wearing a blue shirt and green shoes and like you you can't disprove that that's the case unless you test every particular situation your model might be in. Um, so we know that</p>
<p>the laws of physics apply no matter where you are, what scenario it is, but from a model's perspective, maybe something that's out of distribution, it just never needed to learn that the same laws of physics apply there. >> Yeah. You were very excited because I I read Ted Chang over the holidays. Um, and I was very inspired by this uh short story called Understand. Uh, which apparently it's like pretty old. You you must be familiar with it. To me, it was</p>
<p><strong>[57:21]</strong></p>
<p>like it's this fictional story. It's like the inverse of flowers for Alenon where uh you had someone like uh get really smart but then also try to outsmart the tester >> and the story just read like the chain of thought of an of a super intelligence right where they're like oh I realize I'm being tested therefore and okay what's the consequence of being tested oh they're testing me and if I score well they will use me for things that I don't want to do therefore I will score badly and like but not too badly that they'll raise alarms. So [laughter] it's just like uh so model sandbagging is a</p>
<p>thing that people have have explored but I just think like uh Ted Chang's work just in general seems to be something that inspires you. I just wanted to prompt you to talk about it. >> I think so Ted Chang has two is a is a sci-fi author who writes amazing short stories and >> his other claim to fame is uh stories of our lives which became uh the movie Arrival. >> Exactly. Yeah. So so two books of short stories that I'm aware of. He also actually also has a great um just online blog post. I think he's the one who coined the term of LLMs as like a blurry JPEG of the internet. I should fact</p>
<p>check that, but he it's a good it's a good post. But I think almost every one of his short stories has some lesson to bear on thinking about AI and thinking about AI research. So, you know, you you've been talking about alien intelligence, right, in this AI human communication translation problem. That's, you know, exactly sort of what's going on in arrival in story of um story of your life. And just the fact that other beings will think and operate and communicate in ways that are not just challenging for us to understand, but</p>
<p>just fundamentally different in ways that we might not even be able to expect. And then the one that's just super relevant for interpretability is the other short book of short stories he has is called Exhalation. And that is literally about a robot doing interpretability on its own mind. >> Oh, okay. >> Uh so I just think that that you know you don't you don't even have to squint to make the analogies there. >> Well, I I actually take uh Exhalation as a discussion about entropy and order. Uh but yes, there's this there's a scene in</p>
<p>Exhalation where basically the the uh so everyone is a robot. Uh so the the guy realizes he can set up a mirror to work on the back of his own head and then starts doing operations like that and by looking at the mirror and doing this. [laughter] >> Yeah. And I think Ted Chang has written about like the inspiration for that story. It was like half inspired by some of the thing he had been doing on Entropy. There's apparently some other short story that is similar where a character goes to the doctor and opens up his chest and there's like a like a ticker tape going along. It's like he basically realizes he's like a touring</p>
<p>machine. Um and I don't know I I think especially as it comes to using agents for interp that story always sticks in my mind. >> I find the uh brain surgery or like surgery analogies a little bit a little bit morbid but it is very apt. Um and when we talked to a lot of computational neuroscientists they moved to interp because they were like look we have</p>
<p><strong>[60:22]</strong></p>
<p>unfettered access to this artificial intelligent mind. It's so much. You have access to everything. You can run as many ablations, the experiments as you want. It's an amazing bed for science. And um you know, human brains obviously we can't just go and do whatever we want to them. Um and and I think uh it is really just like a moment in time where we have intelligent systems that can really like do things better than humans in many ways. And um it's time I think for us to to do the science on it. I'll ask a a brief like safety question. You</p>
<p>know, uh me and Turk was kind of born out of the alignment and safety conversation. I safety is on your website. It's not like something that you you like depprioritize, but like there's like a sort of very militant safety arm that like wants to blow up data centers and like stop AI and and then there's this like sort of middle ground and like is is this like a conversation in your part of the world? Do you go up to Berkeley and Light Haven and like talk to those guys or are they like, you know, there's like a brief like civil war going on? I don't know. >> I think I think a good amount of us have</p>
<p>spent some time in Berkeley. Um, and then there are researchers there that we really admire and respect. Um, I think for us it's like we have a very grounded view of alignment and and safety in that we want to make sure that we can build models that do what we want them to do and that we have scalable oversight into what these models are doing and and we think that that is the key to a lot of these like technical alignment challenges and I think that is our opinion that's our research direction.</p>
<p>Um, we of course are going to do safety related research to make sure that our techniques also work on, you know, things like reward hacking and and other like more concrete safety issues that we've seen in the wild. But we want to be kind of like grounded in solving the technical challenges we see to having humans be humans play a big role in in the deployment of of these super intelligent agents of the future. Yeah, I found the community to actually be</p>
<p>remarkably cohesive. Whether it's talking about academia or the interpretability work being done at the frontier labs or some of the independent programs like maths and stuff. I think we're all shooting for the same goal. I don't know that there's anyone who doesn't want our understanding of models to increase. I I think everyone regardless of where they're coming from or the use cases that they're thinking, whether it's alignment as the premier thing they're focused on or someone who's coming in purely from the angle of scientific discovery, I think we would</p>
<p>all hope that models can be more reliably and robustly controlled and understood. It seems like a pretty unambiguous goal. I'll maybe phrase it in terms of like there's maybe like a U curve of uh of this where like if you're extremely doomer you don't want any research whatsoever. If you're like mildly doomer you're like okay there's this like high agency doomer is like well the default path is where we're all dead but like we</p>
<p><strong>[63:23]</strong></p>
<p>can do something about it. Whereas there's there's other people who are like no just like don't ever do anything. Um you know >> Yeah. Yeah. There's also the other side like there is the super alignment like people that are like okay weak to strong generalization we're going to get there we're going to have models smarter than us and use those to train even smarter models how do we do that safely that's you know there's the camp there too that's trying to solve it but yeah there's there's a lot of doomers too [laughter] >> well and I and I think there's a lot to be learned from taking a very um like even regardless of the</p>
<p>problems s that you're applying this to. Also just like the notion of like scalable oversight as a method of saying let's take super intelligent or or current frontier models and help use them to understand other models is another case where I think it's just like a good lesson that everyone is aligned on of ideally you are setting up your research so that as super intelligence arrives that is a tailwind that's also bolstering our ability to like understand the models cuz otherwise</p>
<p>you're fighting a losing battle if it's like the systems are getting more and more capable and our methods are sort of linearly growing at like human pace. >> Yeah. Yeah. Uh Viva did call out something like you know I I I do think a consistent part of the mechan field is consistently strong to weak meaning that we we train weaker models to understand stronger models something like that. Um or maybe I got it the other way around. >> The other way weak. Yeah. Yeah. The question that Ilia and Yan Leica posed was well is that going to scale because eventually these are going to be</p>
<p>stronger than us right so I I don't know if you have a perspective on that because I that is something I still haven't got over even after seeing that >> there's a good paper from open AI but it's somewhat old I think it's like 23 24 it's literally weak to strong generalization but the thing is that most of OpenAI super alignment team has >> they're gone >> they're gone >> but like I think the idea the idea is >> so now it's it's back there's no more >> they're so Yeah. Yeah. I [laughter] think I think there's some new blog posts coming out. >> I know. And just, you know, check the Thinking Machine's uh website to see who's back.</p>
<p>>> Go [snorts] back. Um [laughter] there's more coming. >> You know, you know what I mean? Like we too strong seemed like a very different direction. And when when it first came out, I was like, "Oh my god, this is like this is what we have to do." >> Uh and like it may be completely different than everything all all the techniques we have today. >> Yeah. My understanding of that is it's that's more like weak to strong when when you trust the weak model and you're uncertain whether you can trust the strong model that's that's being developed. I'm sort of speaking out of my depth on some of these topics, but I think right now we're in a regime where</p>
<p>even the strong models we uh trust as reasonably aligned and so they can be good co-scientists on a lot of the problems that we've been we've been tackling which is a nice a nice state to be in. >> Yeah. Any last thoughts, calls action? >> I don't think so. As we mentioned, actively hiring MLES, research scientists. Um, you can check out the careers page at Goodfire. Um,</p>
<p><strong>[66:24]</strong></p>
<p>>> where are you guys based? >> San Francisco. We're in um Levis's Plaza, like by Court Tower. It's where our office is. So, come hang out. Um we're also looking for design partners across um people working in in reasoning models um world models, robotics and then also of course people who are working on building super intelligent science models or looking at drug discovery or disease treatment. We would love to partner as well. Yeah, maybe the way I'll phrase it is like, you know, maybe you have a use case where LLMs are</p>
<p>almost good enough, but you need one magical knob to tune so that it is good enough. You guys make the knob. >> Yeah. Yeah. Or foundation models uh in in other domains as well. The some of those are the um especially opaque ones because you can't you can't chat with them. >> What what do you do if you can't chat with them? >> Oh, well like thinking about like a genomics model or material science model. So like a Yeah. narrow foundation. Yeah. They predict. Yeah. Got it. Got it. >> I was going to say I thought the diffusion work you guys did early was pretty, you know, pretty fun. Like you</p>
<p>could see it directly applied to images, but we don't see as much interp in diffusion or images, right? Like I see genomics, >> it's going to be huge. Like look at this video models. They're so expensive to produce. And like I mean basically a midjourney sref is kind of a feature, >> right? >> The what? mid journey sref like the the string of numbers that you >> right right the style reference I guess. >> Yeah. No, I I mean I think we're starting to see more of it and I'll say like the the research preview of our diffusion model kind of like a creative use case and the steering demo you saw.</p>
<p>I I think of those much more as as as demos than um a lot of the sort of core platform features that that we're working with partners are unfortunately sort of under NDA and less demoable. But I will, you know, hope that you're going to see inter pervading a lot of what gets done even if it is behind the scenes like that. So some of the Yeah, some of the public facing demos might not always be representative of like the it's just the tip of the iceberg, I guess, is one way to put it. >> Okay, excellent. Thanks for coming on. >> Thanks for having us. This is a great</p>
<p>time.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=ck63uv6APBA</guid>
      <pubDate>Thu, 05 Feb 2026 20:45:28 +0000</pubDate>
    </item>
    <item>
      <title>⚡️ Reverse Engineering OpenAI's Training Data — Pratyush Maini, Datology</title>
      <link>https://www.youtube.com/watch?v=CSgjaC6y6Mk</link>
      <description>Should you add reasoning traces to your pretraining data? There’s been a surge of academic work speculating its advantages. But do frontier labs actually do this? Turns out, we can answer confidently, by forensically analyzing how models respond to a single question about an emoji.

This is exactly what today's guest, Pratyush, did: https://x.com/pratyushmaini/status/2001824826353418433

And we convened a quick ⚡️ pod to talk about it!

https://pratyushmaini.substack.com/p/reverse-engineering-a-phase-change-a96</description>
      <content:encoded><![CDATA[<p><strong>[00:01]</strong></p>
<p>[music] Hi, welcome uh to the lane space lightning pod. Uh we have Pratush here from Dtology, one of the founding team members. Listeners to the pod will remember our episode with Ariotos. I think it was one of the top episodes of 2025. Uh so I want to double down on uh the interesting work coming on Dlogy. I think not enough people are looking at the data and we want to encourage engineers and researchers to think more deeply about data and I think data is one of the best teams that is like basically all data nerds. Uh Patric uh you your bio has like 70%</p>
<p>California 20% Pittsburgh 10% Delhi you're in South Bay right now right with the at the office. >> That's correct. I'm at the office. >> Yeah. Yeah. Cool. Cool. How else do you introduce yourself? any any other things that people should know about you before we get started? >> No, that was pretty good. Thank you so much for having me. Um I am um I'm basically finishing up my PhD at CMU and have been a part of the founding team at Dtology for more than 2 years now. It's been a while that we've been building this company. I am very excited about datacentric aspects to AI and more of</p>
<p>how we can build responsible and very high performance systems by focusing on the data layer. Somehow data is still one of the more undervalued research topics and um I've been that's been the focus of my PhD and now also a lot of the time at Dtology. I think you're very well pointed that we're all data nerds. We love looking at data in general. I think one of the coolest parts about looking at data is like we can share like strange artifacts about it. In fact, we even made a channel called data is weird on our Slack channel where we</p>
<p>keep posting screenshots and snippets of like all of these random artifacts that you see within data sets and they'll keep surprising you. The internet is so weird in so many ways that you would never expect that oh the depths of the internet has this and we're actually feeding these kinds of data points to our model. So I think there's like a shared passion about really looking into data and like an investigative approach around the team in general. >> Yeah. Okay. Two follow-up questions and then we can go into the paper. Uh one, what what's the what's your thesis going</p>
<p>to be about? What's the overall theme of your works? >> This is uh called responsible and efficient use of webcale data for pre-training which is basically when you're training on this messy web data and you shove all of that into the model. How can we train efficiently which is like only use things that are useful but then also the responsible angle to it which is like how can we make sure we are attributing data to like giving the right credit to like artists who put data online or how can we make sure the data is safe for models</p>
<p>model behaviors. So I do a lot of work on the evaluation aspects of this as well uh but then also on the efficiency and performance aspects. >> Okay. And then uh just in your data is weird channel any uh fun anecdotes over the last recent anecdotes just just pull something out. No, we're going to go into like the GPT training data stuff, but I'm just kind of curious what what what gets posted there. So some of the</p>
<p><strong>[03:02]</strong></p>
<p>cool things is all from like researcher curiosity. I think one of the recent things that we were exploring was that how uh do various models like for instance you take the coen model and then ask it to do completions on certain questions. So we would just take like we took some example questions from J which is like an important olympiad like an exam in India and we would like even ask see the models with the first two words of a question like the light the light bulb and it would like literally complete the actual questions and so</p>
<p>then we started comparing this across like family of models and we consistently saw how many of these like models today are being like massively like kind of fine-tuned on problems from overfit >> massively overfit like even like imagine if I ask you the light bulb what comes next in your mind it won't be a question from an Olympiad or a practice exam right and these give the right question they also give the options after that and so I think that is one of the fascinating findings from uh data is weird from the last month</p>
<p>>> but no nobody is benchmarking on J that's not a that's not a >> J bench now and I think it's also been reported by various uh frontier model labs but like J is like one example I'm sure this happens like across um different exams but it's very clear how the last stage of training for many of these models does have a massive amount of um example or exam benchmark because the model will not like behaviorally complete exam questions with options if they have not really seen it at the end of training for multiple epochs</p>
<p>>> and you're saying there's per there's basically perfect memorization and I'm wondering if you have ablated this across different model sizes like does B do the same as 30D probably not right and I was curious about the memorization power >> that's a great question I think it's a mix of two things one is the recency of seeing that data and the second is also the size of this model I don't we don't see this phenomena in like the earlier versions of like quen 1.5 or even coen 2 but start seeing it in quen 3 so there's something about a more like a</p>
<p>significantly higher weight on benchmark or like uh J or like any types of evaluation questions during the final stages of training in these decent models and then definitely see we see the effect of like bigger models having memorization to a significantly higher level though I would say that with the coming of the models we're starting to see this uh phenomena even like models which have a less number of active parameters so a phenomena that we would observe at a 72b model in the past is probably now visible at a 20b active</p>
<p>120b also [snorts] so in some way the active parameters might be lesser in thee um life cycle but still be able to like regurgitate exact memorization. >> Yeah, it's interesting when memorization is done in the like the router at thee actually ends up becoming like an index where you look up like oh here's the index of the thing that you need and then it's routed to that expert that is going to have memorized the answer.</p>
<p><strong>[06:03]</strong></p>
<p>>> Exactly. >> That's that's a way to cheat. >> Okay. So let's uh tell us about the seahorse emoji and GPT. So around October last year, I saw on Twitter a few people had posted this very fascinating result where they would ask a frontier model such as GPT like uh is there a seahorse emoji and like that's the only question they would ask and what you would see is that the model keeps going on and on and this was a phenomena that really fascinated me like imagine asking GPD a question and this</p>
<p>uh answer does not stop. Which was really surprising to me because all of these models like are like these are like frontier models. They would not really have this problem of like continually like going back and forth on a certain answer. If you see over here, it says yes in the beginning, then it starts to think more about it and then it says no and then goes yes and no and yes and no. So there's like a huge amount of internal turmoil in the model and a phenomena where it just keeps saying this and does not really stop.</p>
<p>And so then I tried this across models and I saw consistently across Grock and GPT and Gemini that you were seeing this phenomena where models will like self-correct themselves quite a bit and these were like non-thinking models like the standard the auto or the smallest fast model and they would still do this and I was like quite perplexed that how is it possible that these models which are like so strong they're able to solve these IMO problems and they're like kind of just getting into this selfnarrative existential dilemma or something when you ask them about like is there a</p>
<p>seahorse emoji and this was also a time when I was thinking a lot about reasoning data and like should like models uh learn more selfcorrection behavior during pre-training or not and my initial thought was that this behavior that we are seeing of models saying yes or no is very reminiscent of all of these back correction and self-reflection behavior which is becoming very predominant um right in modern AI models uh and there are a lot of papers on like if you the models learn how to correct their wrong traces</p>
<p>or like uh in their um data or in their post training data then the mod has become much stronger at at reasoning and so I was thinking this is very reminiscent of that and somehow feels like there is like an emergence of that behavior even when I did not ask for reasoning so uh I thought like it will be very interesting to see like what really causes this behavior because we don't really understand that and maybe there has some tie-in with the prevalence of reasoning data or self-correcting monologues within the mid-raining or pre-training phase of non thinking models and so my first thought</p>
<p>was is this something that happened always what or is it just like a GPT5 phenomena and because people have been noting this in October and potentially people asked this in GP in in like a year ago also like would the models create this kind of a self-correcting monologue and the interesting thing was and I've like put the number of output tokens on the y-axis on this graph and the release date of the model on the x-axis and so I like ran the uh openi</p>
<p><strong>[09:07]</strong></p>
<p>API across like models released from 2023 to 2025 and you would see like all the models until 2024 December had very tur and short responses to the question is there a seahorse emoji they would either say that they exist sometimes they would say they do not exist but they would never go into this selfcorrection loop where they like say one answer in the beginning and then a different answer at the end and so something very interesting was happening because we know that in December 2024 Four is when the 01 model came which is the first time where a model that could</p>
<p>think and selfcorrect itself was available to the public and then 4 months after that we noticed that the GPT 4.1 series is the first model where suddenly this the response length of all the models had started to increase and then the chat GPT4 model uh update that was in the month of May again had like this recursive behavior and this became significantly more prevalent by the time GPT5 And so >> and and just to insert your question here, this is specifically like the cutoff date that they have declared</p>
<p>themselves, right? Like I don't know if this is uh is this is what are you ordering by cut off date or the release date? >> Uh this is ordered by the release date and so >> it's unclear what the cutoff date would be but very likely that it has seen likeam samples very close to that date. >> So for instance, Chad GPT 40 is an interesting case. its original release was somewhere in December uh like 2024 but then they had an update in May and only the May update shows this behavior but the previous update does not show it and so they kind of potentially post</p>
<p>trained the model on these kinds of reasoning traces and it certainly started showing the self-reflective behavior. I think should be important to point out that this seahorse emoji is an interesting question and like why it really brings out this behavior is because the existence of the seahorse emoji is kind of called like a mandela effect where a lot of people on Reddit are saying that hey I have seen the seahorse emoji it looked blue in color and had a horse pointing to the left or something like that and so there is like enough doubt or like enough conversations on the internet that some</p>
<p>people say yes and some people do say no and so it's kind of acting like a trigger to elicit that thinking response which is great for us because we do want these edge cases that can help us do this discovery or this investigative analysis. So by now what we have figured out is that around the GPT 4.1 and then GPT 5 series there is something happening which leads the models to have a lot of self-reflection and now the question is like what happened over here uh like which led to models starting to do this and for me this was interesting</p>
<p>because I wanted to know if the frontier model labs also care about putting reasoning data in pre-training or in mid-training phases like you might have seen a lot of papers coming out these days which say that you should do reasoning pre-training and like RL pre-training and all of those papers which are saying that put the thinking tokens in the beginning and as academic researchers or like people who are not</p>
<p><strong>[12:07]</strong></p>
<p>training frontier models it's always a curiosity question are these things really valuable at the frontier level or are these questions only interesting at the 1B scale when you can move your data distribution and so [snorts] that's why I was excited about understanding like if there is relevant data in the mid-training phase for non-thinking models and something interesting uh that I was able to validate this was with the Almo series. So the Almo models are great uh for doing investigative science because they release everything they</p>
<p>release the data the models um and they also have a very fantastic Almo trace. Now with Almo trace you can trace like what data in the pre-training of Almo was closest to its response right now. Yeah, this is I was thinking of this when you're talking about your PhD thesis was like directly the tool to use. >> Exactly. Exactly. >> Do do you use their approach? >> Yeah. I think like I honestly al trace or the original paper which produced the infinagram like I think it came out last year at Colum 24. Uh that was one of my favorite papers that year. I think they</p>
<p>did a fantastic job in like being able to like actually trace data back. It's like a big engineering problem to be able to like look up so fast and they did like a fantastic job. and I've used it a lot for multiple projects in my PhD. So coming back to this phenomena uh and I was like curious like okay now we know that this suddenly happens or like this infection point happens in GPD 4.1 series can we trace it back to an open series of models and see if this is like um the same phenomena happens there and so you will see for the mode 2 model which was released a year ago like says</p>
<p>there exist an emoji it's the wrong answer but does not really try to selfcorrect itself right on the other hand in the 3.1 32b series it has the same selfcorrection behavior like it's a long response I only have the last part of it but it first says yes then says but wait there is no one and then so we see the same monologue within the Almo 3.1 model and if you look at the Almo trace there is no particular example that it is really referencing to over here that leads to this phenomena so it's more like a capability rather than</p>
<p>a regurgitation of the text so that's great to know it's not like someone has poisoned the data that was another thought in my mind has ply poisoned the data on the internet and and the models are like regurgitating something from there but that's not happening. So that's good that's a good sanity check that what we're seeing is a capability and not like a regurgitation from some website or like Reddit post which might have said that yes and no. And then the more interesting thing is now we can actually trace the exact difference in the data sets for all mode 2 and mode 3 and the main difference for the instruct model over here. So Almo 3 has multiple</p>
<p>variants some are the thinking variants some are the instruct variants. So with the instruct variant they do not have any thinking data in the post- training uh phase but they do mention that we are going to put like there's a line that they write there's some intentional addition of thinking traces in the mid-raining phase of Almo data and so that's the information available from their report and we can also see the actual data sets they use. This</p>
<p><strong>[15:08]</strong></p>
<p>observation really well corroborates with the whole finding about the open AAI models where they potentially also did the same thing where even if in the non-thinking models the instruct models where you do not put thinking traces in the post-training phase you do put thinking traces in the mid-training phase and that leads to models becoming better and so this was like quite interesting for me uh because what they suggests about the GPT training data is that the self-reflection data has now actually become pretty much core to the training of all frontier models because we're seeing that happen in non-instruct</p>
<p>models across the board. Which is very interesting to me because for the longest time people would always believe that the idea of foundation models is that you have trained this foundation and now you just post train it which meant that there's a general purpose model you don't need to really put everything that you care about in the general purpose model and then you can post train it to get the capability that you want. But this really shows us that even this foundation mod foundation needs to have the ingredients that of the capability that you desire at the end. And so putting thinking traces in</p>
<p>the foundation is actually useful for the post- training or fine-tuning of uh the thinking models. And so that's why they can only have one single backbone and you would actually prefer to have the foundation also represent the capabilities that you desire. And so self-reflection is a capability that we desire and it's become core to the foundation and not left as a cosmetic after afterthought of just post training. I think also was interesting to me that the GPD 4.1 series came about 4 months after the O1 data was available. So it's kind of interesting</p>
<p>like how fast do Frontier Labs move. O1 reasoning traces would have been available to the pre-training team in December or the mid-training team I should say in December and then from December until u what's like 4 months from there end of April the mid-training team would have potentially used the traces which they obviously were dis developing and there was a lot of talk about the Omni models >> but not directly right like it's just went into the the the web and the the pre-changing back >> no I would be very surprised if it went into the web I I think this was very intentional because that's exactly how</p>
<p>the Almost series happened where they have an intentional addition of the thinking uh data. So they have added a data set which has this self-reflection behavior where you will self-correct yourself by first gening a wrong sequence and then the right one. >> Yeah. I mean this is why I wanted to feature this piece because I think like the the you know like how people are training reasoning into their models and when they're choosing to include it and all this uh it's very actually very important and not well known. And so this is one of the first like actual</p>
<p>like somewhat investigative piece uh I've seen about it. So >> yeah and you you might have seen all these papers which show that RL is only like kind of enhancing or amplifying a capability that the model already has and you actually need the core to have that capability. So I think this reinforces all of those things that the core actually contain the capability and then you can actually do better RL, you can do better post training all of that. And so I think in general it was very nice to like be able to like see that these are not just things that are</p>
<p><strong>[18:09]</strong></p>
<p>relevant to a 3B or a 7B model but also relevant at the real frontier uh where people are training like the best models. >> Any reactions, criticisms or follow-up work >> for now like um in general I think the community was pretty excited about this like a lot of people uh felt that this was either reinforcing their beliefs or very interesting investigative work. I think that's something people always appreciate when you just like put your detective hat on and try to like dig into um whatever is happening. Something that we are actually following this up um with uh and also in general we have</p>
<p>been working on that for a few months is a work uh which should be out like in a few weeks called the finetuners fallacy which is basically kind of arguing pretty much the same thing which is that if there is a core capability that you actually care about that capability should be part of the foundation and not a fine-tuned artifact. So we kind of show this for various types of domains and we're actually in general seeing adoption of like AI in a lot of like domain specific use cases. So the old idea of you have a foundation model and</p>
<p>you can just fine-tune it and get the desired capability is kind of like uh we are past that stage now and this really means there's going to be a lot of specialized pre-training going forward. So I think like 2026 and 227 are going to be the years where different enterprises start doing specialized pre-training because the cost of pre-training really amortizes itself very fast when you think of the fact that by doing specialized pre-training you can train a smaller model which is as capable as a much larger model when</p>
<p>fine-tuned. And so that's the core thesis that we've been working towards in the past few months and also very relevant to datology in general. >> Okay, got it. Um I may need to uh go some because I'm being sort of chased down this thing. But no uh this is uh secretly a daty pitch I'm realizing you know because everything you're saying is like exactly the the pieces of data which is uh very fascinating. You know you've also done other work uh you are you're one of the authors on beyond web u I don't know any anything else you want to sort of plug or or feature um</p>
<p>you know in terms of this the stuff that's going on. Yeah. Uh uh I can quickly share something about beyond web and how it's been like making waves and I also want to talk about the same idea of having the core capability that you care about at pre-training in something called safety pre-training and maybe like a few minutes over there. So let's go to beyond web first. So this was like one of our big releases last year where we were trying to showcase how to scale synthetic data to trillion scale like</p>
<p>works which are like doing like 100 billion token training with synthetic data and 200 billion training token with synthe data but the actual dynamics of how you work well with synthety. [snorts] And so this work was majorly about like understanding and like giving the lessons back to the community of like</p>
<p><strong>[21:09]</strong></p>
<p>how can you do good hybrid model training where there's like a part of data that's synthetic. So like uh just like as a high level like flagship numbers this model that we released this results were very strong. So the neurotron data set is like one of the top data sets today which is like based with a lot of synthetic data. uh so the and the datology uh data that we a model that we released called beyond web uh is the blue line over here as you can see that we achieve the same performance as the Nvidia model in almost like 2.7x like lesser time and then much faster</p>
<p>than anything that hugging face or at pajama does very interestingly our 3B model is pretty much the same performance as the 8B model of Nvidia and this is quite like strong given that Nvidia's Neotron data is like literally the most downloaded uh open-source data set. It's like being downloaded like I think a million times every month I was seeing uh recently and so that's huge. So internally we've been doing a lot to improve um synth data. So just a little bit of backstory to explain the types of</p>
<p>synth data approaches that exist today. So for the longest for the beginning of how synth data came into picture was through the tiny stories and the fee model family and so >> really >> yeah so so that was like uh I think the big push >> yeah textbooks are all you need. Yeah, >> the textbooks are all you need. And even before that, there was this paper called tiny stories which was by a couple of researchers at Microsoft on telling like how can you train small models with</p>
<p>entirely synthetic data. And so I would bucket these approaches into something that I call the generatordriven paradigm which means you have a big model for instance it could be the GPT4 model and you're trying to query that model to generate a textbook or an essay or a paragraph but all the information in your data set is coming from the generator or from the model. And so you really need that the generator is huge and massive and contains information about everything on the world so that you can train models in the</p>
<p>generator-driven paradigm. And so a couple of years ago when I was actually an intern at Apple, we released this paper called rephrasing the web. And we modeled this alternate paradigm which is called the source rephrasing paradigm. And so we said the generative driven paradigm is exciting and doing well but it just does not scale. You need the model to be massive and generating data will be very expensive. Plus it really depends on you prompting in the right way because if I ask you to generate a paragraph about the laws of motion then my data will have it</p>
<p>otherwise it will not. So it really puts the burden on the researchers to make sure that the data is diverse. But on the other hand uh we have the entire corpus of the internet which has so much knowledge available. The only issue is that the data might not be high quality and so we can basically rephrase all of this data into higher quality data. So for instance, the internet becomes the source of knowledge and the synth data</p>
<p><strong>[24:10]</strong></p>
<p>generation model is only modifying that knowledge into styles that you care about. So for instance, if question answering is something that you care about, then you repurpose the knowledge of the internet into smaller styles into into the question answering styles. So this really trans changes the whole philosophy of generating data by making the cost of synth data generation extremely small because now you can use like a very small off-the-shelf model you can put the actual information inside it uh and you can ask it to</p>
<p>rephrase it into question answering and so the capability to transform data is actually very cheap even a 1B model or a 3B model can do this very well you do not need all the knowledge of the internet to be within this And so the source rephrasing paradigm has actually become the dominant paradigm in 2026. Like even the Kim K2 model has like a long section of like how they do rephrasing of internet content. Then the even Gro 4 has been using the same source rephrasing paradigm. Nvidia's Neotron data that we are benchmarking against is also using</p>
<p>this idea of source rephrasing. And so philosophically in the pre-training phase uh we have kind of almost finalized how we do synth data which is like we transform existing knowledge into patterns that are useful for us and in beyond web we really go beyond what we had released at Apple when I was interning over there which was called rephrasing the web and now we have beyond web which really like multiplies the advantages of um what we were doing in synthetic and uh it's really good to sort of uh</p>
<p>get a sense And I think like I want to also just like use lit space to [music] feature this kind of work. People are going to read the paper on their own. We're not going to cover the whole thing, right? So, but if they can reach out to you, I'll leave all the socials and all there. Um, yeah, and uh probably join you as well. >> Awesome. Yeah, thank you so much for having me and uh yeah, uh very excited to see the response. >> Thanks for all the great work. Yeah, I I would say like yeah, I I is to me it's amazing because like when Red Pajama came out like I I realized like that was like the start of something. It's interesting to me that every single</p>
<p>generation is a different company, you know, like, oh, it was together AI and then it's like Microsoft or whatever and it's Apple, then it's Hugging Face, then it's Nvidia, now it's you guys. And I'm like, you know, where where's the the the persistence or like it's just such a competitive field or you guys keep changing companies. So, it's the same it's the same people. >> Now, we have a hub for all data enthusiasts. I think it's going to be the same for a while. >> Yeah. Okay. Well, thank you so much. That was great. >> Thank you so much. Bye-bye.</p>
<p>>> [music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=CSgjaC6y6Mk</guid>
      <pubDate>Tue, 10 Feb 2026 02:45:04 +0000</pubDate>
    </item>
    <item>
      <title>🔬Generating Molecules, Not Just Models</title>
      <link>https://www.youtube.com/watch?v=nP0N1kYLegc</link>
      <description>This episode traces the remarkable journey from AlphaFold2’s landmark achievement in protein structure prediction to the broader landscape of molecular interaction modeling and protein design. The problem AlphaFold2 addressed—predicting the structure of single-chain proteins—was long considered intractable due to its perceived NP-hard nature. The breakthrough came not only from advances in machine learning but also from leveraging evolutionary data to infer co-evolution of amino acids, providing powerful hints about spatial proximity in protein structures. Yet, as the guests explain, the field quickly moved beyond this milestone toward more complex questions, like how proteins interact, how they fold dynamically, and how to model these interactions with small molecules, RNA, and DNA.

AlphaFold3 marks a critical shift in this evolution, moving from static structure prediction to modeling heterogeneous molecular interactions. Rather than treating these interactions as isolated problems, AlphaFold3 unifies them within a single model trained across modalities. This progress also reflects a broader trend in machine learning: the shift from regression-style prediction to generative models capable of expressing uncertainty and capturing system dynamics. By sampling from a distribution of plausible structures and interactions, these models allow researchers to better understand the flexibility and variability of biological systems. However, such models also introduce new challenges, particularly around validation and ranking of generated outputs.

Enter Boltz and its suite of tools, which aim to democratize access to these cutting-edge capabilities. Boltz builds on open-source principles and a strong community foundation to deliver models that are both state-of-the-art and accessible, with a focus on usability, extensibility, and real-world validation. Boltz2 and BoltzGen combine structure prediction, affinity estimation, and generative design in one pipeline, enabling users to design new proteins and small molecules with high confidence. Notably, Boltz emphasizes the importance of experimental validation, collaborating with partners across academia and industry to test new designs in the lab. This feedback loop is essential to the iterative improvement of models and benchmarks.

BoltzLab, the newly launched platform, encapsulates this vision by providing a cloud-based interface for running large-scale protein and molecule design campaigns. With support for both computational and experimental scientists, BoltzLab offers APIs, collaboration tools, and automated agentic workflows to make advanced molecular modeling accessible to users with varying levels of computational expertise. It embodies the shift from abstract model development to practical deployment, where infrastructure, cost-efficient compute, and user-friendly interfaces make a meaningful difference. As the guests emphasize, the real progress lies in enabling scientists to use these tools creatively and collaboratively to accelerate discovery in biology and medicine.

Timestamps

00:00 Introduction to Benchmarking and the “Solved” Protein Problem
06:48 Evolutionary Hints and Co-evolution in Structure Prediction
10:00 The Importance of Protein Function and Disease States
15:31 Transitioning from AlphaFold 2 to AlphaFold 3 Capabilities
19:48 Generative Modeling vs. Regression in Structural Biology
25:00 The “Bitter Lesson” and Specialized AI Architectures
29:14 Development Anecdotes: Training Boltz-1 on a Budget
32:00 Validation Strategies and the Protein Data Bank (PDB)
37:26 The Mission of Boltz: Democratizing Access and Open Source
41:43 Building a Self-Sustaining Research Community
44:40 Boltz-2 Advancements: Affinity Prediction and Design
51:03 BoltzGen: Merging Structure and Sequence Prediction
55:18 Large-Scale Wet Lab Validation Results
01:02:44 Boltz Lab Product Launch: Agents and Infrastructure
01:13:06 Future Directions: Developpability and the “Virtual Cell”
01:17:35 Interacting with Skeptical Medicinal Chemists</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>Actually, we only trained the big model once. Uh that's how much compute we had. We could only train it once. And so, like while the model was training, we were like finding bugs left and right. Uh a lot of them that I wrote. >> And like I would I remember like us like sort of like you know doing like surgery in the middle like stopping the run, making the fix, like relaunching and um yeah, we never actually went back to the start. We just like kept training it with like the bug fixes along the way. Uh which was >> impossible to reproduce now. Yeah. Yeah. No, that model is like has gone through such a curriculum that you</p>
<p>know it's learned some weird stuff. Uh but uh yeah, somehow a miracle it worked out. >> It's a pleasure to have with us today Gabriella Corso and Jeremy Vulvin. They are they recently founded Boltz, a company trying to democratize and bring art, structure prediction and biology to you know the masses. uh they were both uh recent PhD grads from MIT and have been working on all sorts of foundational papers in like generative biology. Um anyway, uh pleasure to have you here. Thanks for coming.</p>
<p>>> Thank you. >> Thank you. >> Thank you. >> Uh I guess we're maybe what 6 years post Alphold 2 right now, which was like kind of a big moment. >> Um is that right? Yeah. >> I think was it 2021? >> So yeah, on going on 5 years. >> 5 years. 5 years. Yeah. Um, yeah. So maybe for the audience like can can let's go back to that moment in time and explain like what was this big moment and why was it interesting? Why was everyone so excited and I think you two were probably quite excited. So why were</p>
<p>you personally excited? >> I would start on kind of why that was interesting kind of you know from a scientific standpoint. So Alpha so maybe first as a kind of introduction for uh the ones in the audience and not structural biologists. So the idea of structural biology is that you know we want to try to understand how you know proteins and other molecules take shape inside our cells and you know how they interact and structural biology is sort of this beautiful discipline uh</p>
<p>where we are somehow able to understand this minuscule structure at know kind of atomic details using uh these incredibly um complex methods like you X-ray crystalallography and you know the the dream has always been of computational biology. Can we understand kind of the structures without having to you know resolve this crystal you know shoot X-rays and so on. And so Alphafold um was a real breakthrough in this problem</p>
<p>of protein folding which is trying to understand the structure of a single uh protein. And to me it was exciting across kind of many dimensions. One I was a computer scientist. I was working a lot on machine learning. And I saw kind of the impact that kind of the work similar somewhat similar to what I was</p>
<p><strong>[03:00]</strong></p>
<p>doing could have on like a longstanding scientific problem. And on the second perspective from a more you know personal side, the seeing kind of the structures coming out of these models where you know you see kind of this beautiful you know um creation of life is something that was was very inspiring to me and so that was kind of one of the things that led me to start uh working on uh structural biology and in particular with machine learning. >> Were you a structural biologist before</p>
<p>Alpha Flood came out? I mean did you you did machine learning but it was not in structural biology so that actually shifted your career quite dramatically. >> Yeah very dramatically. I was I was working on some pretty kind of theoretical methodological things and I was starting to see kind of you know some of the challenges in you know kind of doing somewhat theoretical or methodological work and you know seeing kind of the potential impact of you know doing excellent you know alpha fold was really a machine learning breakthrough</p>
<p>but you know and applied machine learning and so that led me to uh want to start working in applied ML. our our group at the time was um working a lot on like small molecules already and um I think alpha is kind of what triggered I think this shift to like working on on biologics um and at the time I think it like opened as many questions you know as it answered in a sense like we um the immediate follow-ups were okay like can we do this on other things than proteins can we do um you know interactions of</p>
<p>small molecules with proteins nucleic acid with proteins Can we model more complex protein systems? And I think yeah very rapidly I think after alpha fold uh people realized I think that there was you know machine learning could have a could really yeah sort of target this problem very differently than you know than previous methodologies >> clarification. So what what does small molecule mean? What does protein mean? What is you know the terms that you just mentioned?</p>
<p>>> Yeah maybe we can start with protein. Um, so you know, protein is is maybe the most fundamental one. It's what gets decoded out of our DNA. >> Um, it's uh essentially a sequence of uh amino acids. Each amino acid you can kind of consider as a we call a small molecule. Um, and there's 20 of them in the at least in the human body. Um, and you know any compositions of these 20 amino acids in a sequence um you know creates a different form of a protein. Um and so you know obviously they are a</p>
<p>very large number of those sequences that you can create. Um small molecules are sort of you know following the name um typically considered to be um you know a much smaller number of atoms. Um and the atoms that compose them I think are also generally a bit more diverse right amino acids have um you know this composition and it's always the same. uh</p>
<p><strong>[06:03]</strong></p>
<p>with small molecule you know there's a larger set of possible atoms that also we have to consider that also make the problem uh pretty challenging and then we have nucleic acids so DNA and RNA uh which are also very interesting to model the structure for and those a little bit more similar to proteins you know they're sort of composed of four um nucleic acids and you form sequences from them and um any uh codon which is like three uh nucleic acid um translates into a specific amino acid. Um so yeah, different forms of molecules at the end</p>
<p>of the day just a bunch of atoms uh you know that are bonded together uh that we try to understand the interaction of. >> Going back to the alpha fold 2 moment like um I remember this very well. I was at uh Nurifs when I guess the results of this famous competition came out. So um you can you want to talk about CASP and like what it is and why it is it was so interesting and exciting. >> Yeah, I think every so um every couple years um and the goal has always been to</p>
<p>you know find u protein structures that are a little bit different from what's known. So CASSP over the years has like you know put in a lot of effort to like gather structures from you know academic groups and uh even industry groups uh to try to create sort of a test set that would be um difficult um for uh different methods and CASP uh 14 was when uh Alpha 2 really you know blew everything out of the water. Um and the</p>
<p>the improvement was so large over you know the previous previous method and also over the previous competitions. Um and now CASP continues you know we've had CAS 15 we have CAT 16 and you know sort of what's happened now is that it's really expanding to also all these other modalities like I was mentioning like protein small molecule nucleic acid and u but the goal remains to like you know really challenge the models like how well do these models generalize and you know we've seen in some of the latest GAS competitions like while we're become</p>
<p>really really good at proteins basically monomeic proteins um you know Adamal is remain pretty difficult. So it's really essential you know in the field that there are like these efforts to um you know to to to gather um you know benchmarks that that are challenging so keeps us in line you know about what the models can do or not. >> Yeah. >> It's interesting you say that like in some sense cast you know at cast 14 a problem was solved and like pretty comprehensively right but at the same</p>
<p>time it was really only the beginning. So can you explain like what was the specific problem you would argue was solved and then like you know what is remaining which is probably quite open. >> I think I think we'll we'll steer away from the term solved because we have many friends in community who get pretty upset at that word and I think you know</p>
<p><strong>[09:04]</strong></p>
<p>fairly so. Um uh but the the problem that was um you know that a lot of progress was made on um was the ability to predict the structure of single chain proteins. So proteins can like be composed of many chains and single chain proteins are you know just a single sequence of amino acids and uh one of the reason that we've been able to make such progress is also because um we take a lot of uh hints from evolution. So the way the models work is that you know</p>
<p>they sort of decode a lot of hints um that that comes from evolutionary landscapes. So if you have like you know some protein in an animal and you go find the uh similar protein across like you know different organisms uh you might find different mutations um in them. And as it turns out if you uh take a lot of these sequences together and you analyze them you see that uh some positions in the sequence tend to evolve um at the same time as other positions of the sequence. sort of this like uh correlation between different positions</p>
<p>and um in it turns out that that is typically a hint that these two positions are close in three dimension. So part of the you know part of the breakthrough has been like our ability to also decode that very very effectively. uh but what it implies also is that you know in absence of that co-evolutionary landscape the models don't quite perform as well and so you know I think when that information is available maybe one could say you know the the problem is like somewhat solved</p>
<p>from the perspective of structure prediction >> when it isn't it's it's much more challenging and I think it's also worth also differentiating the um sometime we confound a little bit structure prediction and folding folding is the more complex process of actually understanding like how it goes from like this disordered state into like a structured like state and that I don't think we've made that much progress on but the idea of like yeah going straight to the answer uh we've become uh pretty good at. So there's this protein that is like just a long chain and it folds up.</p>
<p>Yeah. And and so we're good at getting from that long chain in whatever form it was originally to >> the thing, but we don't know how it necessarily gets to that state and there might be intermediate states that it's in sometimes that we're not aware of. >> That's right. And and that relates also to like you know our general ability to um model like the different you know proteins are not static. they move, they take different uh shapes based on their energy states. And I think we are also not that good at understanding the</p>
<p>different states that the protein can be in and at what frequency, what probability. >> Um so I think the two problems are quite related in some ways. Um so yeah, still still a lot to solve. Um but I think the it was yeah I think I think it was very surprising at the time you know that uh</p>
<p><strong>[12:05]</strong></p>
<p>even with these evolutionary hints that we were able to you know to make such such dramatic progress. >> So I want to ask why does the you know sort of like intermediate states matter but first I kind of want to understand why do we care what proteins are shaped like? Yeah, I mean the proteins are kind of the machines of uh our body. You know the way that all the processes that we have in our cells, you know, work is typically through proteins, sometimes other molecules sort</p>
<p>of intermediate, you know, interactions and through that interactions, we have all sorts of cell functions. And so when we try to understand you know a lot of biolog how our body works how disease work we often try to boil it down to okay what is going right in case of you know our fun normal biological function and what is going wrong uh in case of the disease state and we boil it down to kind of you know proteins and kind of</p>
<p>other molecules and their interaction. And so when we uh we try predicting the structure of proteins, it's critical to you know have an understanding of kind of those those interaction. It's a bit like um seeing the difference between having kind of a list of parts that you would put it uh in a car and seeing kind of the car uh in its final form. You know, seeing the car really helps you uh kind of understand what it does. Yeah. >> Uh on the other hand, kind of going to</p>
<p>your question of you know why do we care about you know um how the protein folds or you know how the car is made uh to some extent is that you know sometimes when it something goes wrong you know there are you know cases of you know proteins misfolding in some diseases and so on. Um if we don't understand uh this folding process we don't really know how to uh intervene. >> Okay. And so do proteins when they're in the body, do they are they typically in</p>
<p>that folded state or are they kind of just like you know doing whatever until they're in a location where they need to interact with something? That's a great question. Uh and it really depends on the protein. Uh it depends on basically the stability of the protein. There are some proteins that are very stable and so once they are produced you know from the ribosome they sort of fold in this shape then more or less they keep that shape with a minor variations. >> The ribosome is the part of the cell that actually translates and and turns</p>
<p>DNA to RNA to proteins. >> RNA to proteins that final part of RNA to proteins. >> And so once they come out they're pretty stable. Uh but then on the other hand there are some that you know for example have multiple states that they switch to depending on their environment. You know uh the bi biologists really figure out</p>
<p><strong>[15:06]</strong></p>
<p>some incredible machines. Uh there are machines where you know proteins where you know depending on whether for example another molecule is present not they will take different shapes and that different shape will give it a different function. And so we have this you know so-called fault switching uh proteins that take multiple and we have some proteins that are completely disordered and these disorder proteins are actually pretty important in kind of many diseases and those are kind of ones of the ones that we have you know the least</p>
<p>understanding of >> there's this nice line in the um I think it's in the full 2 manuscript where they sort of discuss also like why we even hopeful that we can target the in the first place. And then this this notion that like um well four proteins that fold um the folding process is almost instantaneous which is a strong like you know signal that like yeah like we we might be able to um you know predict that this very like constrained uh thing that that the protein does so</p>
<p>quickly. Um, and of course that's not the case for, you know, for for all proteins and there's a lot of like really interesting mechanisms in the cells, but um, yeah, I remember reading that I thought, yeah, that's somewhat of an insightful insightful point. Um, yeah, >> I think one of the interesting things about the protein folding problem is that it used to be actually studied and part of the reason why people thought it was impossible, it used to be studied as kind of like a classical example of like an MP problem. uh like there are so many</p>
<p>different you know type of you know shapes that you know this amino acid could take and so uh this grows combinatorily with the size of the sequence and so there used to be kind of a lot of actually kind of more theoretical computer science thinking about and studying pro problem protein folding as an MP problem and so it was very surprising also from that perspective kind of seeing machine learning So clear there is some you know</p>
<p>signal in those sequences uh through evolution but also through kind of other things that you know us as humans we're probably not really able to uh to understand but that this uh models have have learned. Yeah. So and Andrew White we were talking to him a few weeks ago and he said that he was following the development of this and that there were actually uh AS6 that were developed just to solve this problem. So um yeah that like and that there were many many many</p>
<p>many millions of computational hours spent trying to solve this problem before alpha fold. And just to be clear um one thing that you mentioned was that uh there's this kind of co-evolution of um mutations and that you see this again and again in different species. So explain why does that give us a good hint that they're close by to each other? >> Yeah. um like think of it this way that you know if I have you know some amino</p>
<p><strong>[18:07]</strong></p>
<p>acid that mutates um it's going to impact everything around it right in three dimensions and so it's almost like the protein you know through several probably you know random mutations in evolution like um you know ends up sort of figuring out that this other amino acid needs to change as well for the structure to be conserved. Uh so this whole principle is that the structure is probably largely conserved you know because there's this function associated with it. Um and so it's really sort of like different yeah different different</p>
<p>positions compensating for for each other. >> I see. So the the those hints in aggregate kind of give us a lot of information about what is close to each other and then you can start to look at what kinds of folds are possible given the structure and then what where where what is the end state and therefore you can make a lot of inferences about what the actual total shape is. >> Yeah, that's right. It's almost like, you know, you have this big like three-dimensional valley, you know, where you're sort of trying to find like</p>
<p>these like low energy states and um there's so much to search through that's almost overwhelming. Um but these hints, they sort of maybe put you in an area of the space that's already like kind of close to the solution, maybe not quite there yet. And and there's always this question of like how much physics are these models learning, you know, versus like just pure like statistics. And like I think one of the thing at least I believe is that um once you're in that sort of approximate you know area of the solution space then the models have like</p>
<p>some understanding you know of how to get you to like you know the low energy uh low energy state and so maybe you have some some light understanding of of of physics but maybe not quite enough you know to to know how to like navigate the whole space well. So we need to give it these hints to like >> get it into the right valley and then it finds the the minimum or something. Yeah. >> One interesting uh explanation about how free works that I think it's quite insightful of of course doesn't cover kind of the entirety of of what does</p>
<p>that is um that I'm going to borrow from uh Sergey Chico at MIT. And so he sees kind of alphaold and the interesting thing about Alphaold is got this very peculiar architecture that we have since you know um used and this architecture operates on this you know pair-wise context between amino acids and so the idea is that probably the MSA gives you this first hint about what potential uh amino acids are close to each other. >> MSA is m</p>
<p>>> multiple sequence alignment. Exactly. This evolutionary exactly this evolutionary information >> and you know from this evolutionary information about potential contacts then is almost as if the model is sort of running some kind of you know da algorithm where it's sort of decoding okay these have to be closed okay then if these are closed and this is connected to this then this has to be somewhat close and so you decode uh this</p>
<p><strong>[21:08]</strong></p>
<p>that becomes basically a pair-wise kind of distance matrix and then from this rough pair-wise distance matrix. You decode kind of the actual potential structure. >> Interesting. So there's kind of two different things going on in the the kind of coarse grain and then the fine grain optimizations. Interesting. Yeah. >> Very cool. >> Yeah. You mentioned Alpha Fold 3. So maybe good time to move on to that. So the Alpha Flow 2 came out and it was like I think fairly groundbreaking for this field. Everyone got very excited. A few years later, Alpha Fold 3 came out</p>
<p>and um maybe for some more history like what was the difference between Alpha what were the advancements in Alpha Fold 3 and then I think maybe we'll after that we'll talk a bit about the uh sort of how it connects to bolts but anyway yeah so after Alphaold 2 came out I mean um you know Jeremy and I got into the field and with many others you know the clear problem that you know uh was you know obvious after that was okay now we can do individual chains can we do interactions, interaction different proteins, proteins with small molecules,</p>
<p>proteins with other other molecules and so >> so quick why why are interactions important? >> Interactions are important because to some extent that's kind of the way that you know these machines that you know these proteins have a function. You know the function comes by the way that uh they interact with other uh with other proteins and other molecules. actually in the first place you know uh the machines the individual machines are often as Jeremy was mentioning not made of a single chain but they're made of multiple</p>
<p>chains and then these multiple chains interact uh with other molecules to give uh the function to uh those and on the other hand you know when we try to intervene of these interactions think about like a disease think about like a bio sensor or many other ways we are trying to design a molecules or proteins that interact in a particular way with what we would call a target protein or target. Um and so you know this problem after 2, you know, became clear kind of the the</p>
<p>big uh one of the biggest problems in the field to to solve. uh many groups including kind of ours and others you know started making some kind of contributions uh to this problem of trying to model these interactions and Alpha 3 was um you know put a was significant advancement on the problem of modeling interactions and one of the interesting thing that uh they were able to do while you know some of the rest of the field that really tried to try to model different interactions separately</p>
<p>ly you know how protein interacts with small molecules, how protein interacts other proteins, how RNA or DNA um have their structure. They put everything together and you know train a very large models with a lot of advances including kind of changing kind of some of the key uh architectural choices and managed to</p>
<p><strong>[24:08]</strong></p>
<p>get a single model that was able to set a new state-of-the-art performance across u all of these different kind of modalities whether that was protein small molecules is critical to developing kind of new drugs protein protein understanding you know interactions of you know proteins with RNA A and DNA and so on. >> So just uh to satisfy the AI engineers and in the audience, what were some of the key architectural and data changes that made that possible? >> Yeah. So one uh critical one that was not necessarily just unique to Alphaold</p>
<p>3, but there were actually um a few other teams including ours in the field that proposed this was moving from you know modeling structure prediction as a regression problem. So where there is a single answer and you're trying to shoot for that answer to a generative modeling problem where you have a posterior distribution of possible structures and you're trying to sample uh this distribution and this achieves two things. one is starts to allow us to try to model um more dynamic systems as we</p>
<p>said you know some of these structures can actually take multiple um multiple structures uh and so you know you can now you know model that you know through kind of modeling the entire distribution but on the second hand from more kind of core modeling questions when you move from a regression problem to a generative modeling uh problem you are really tackling the way that you think about uncertainty in the model in a different way. So if you think about,</p>
<p>you know, I'm undecided between different answers, what's going to happen in a regression model is that, you know, I'm going to try to make an average of those different kind of answers that I had in mind. And uh when you have a generative model, what you're going to do is you know sample all these different answers and then maybe use a separate models to analyze those different answers and pick out um the best. So that was kind of one of the uh critical improvement. The other</p>
<p>improvement is that they significantly simplified to some extent the architecture especially of the um final model that takes kind of those pair wise representations and turns them uh into an actual structure and that's now looks a lot more like a more traditional transformer than you know like a very um specialized equivariant architecture that it was uh in Alpha 4. So this is a bitter lesson a little bit. >> There is some aspect of a bitter lesson</p>
<p>but the interesting thing is that it's very far from you know being like a simple transformer. I think one of um this field is one of the uh I would argue very few fields in uh applied machine learning where we still have kind of architecture that are very specialized and you know there are many people that have tried to replace these</p>
<p><strong>[27:08]</strong></p>
<p>architectures with you know simple transformers and you know there's a lot of debate in the field but I think kind of the uh most of the consensus is that you know the performance that we get from the specialized architecture is faster ly superior than what we get through a single transformer. >> Yeah. >> Can can you talk a bit about that like specialized architecture? Um I assume you're referring to triangle layers probably as the core idea or >> there's something uh maybe it's probably quite fundamental about the fact that we sort of model this in like a you know</p>
<p>second order. So like instead of just the sequence we model every single pair and then to update every pair then we need to have these like sort of triangular type operations and um I think what's interesting about it is is is is a couple of things like one I think it relates a little bit to what the input is you know we talked about these multiple sequence alignments before and kind of this notion that like um you know we need to look at pairs of residues to try to understand you know</p>
<p>maybe this initial like distance matrix like Gabri was talking about um and that's something that is very natural right to to model um in 2D um and and I think also there's something about the output as well I think where I think supervising you know over these pairs I think is also quite powerful you know it's this idea of telling the model hey like these two things are close to one another these two things are not um And doing that I think in in 3D is is maybe</p>
<p>a bit more challenging. Um you know when I say 3D sorry I mean like so it's like 1D where we model the coordinates in three dimensions but doing that in like one dimension I think is probably more challenging for the model and and yeah I think to it's it's really survived the test of time. I mean you know this thing came out in 2021 and it's largely the same. I mean there's been this change to the the structure module that's been like largely simplified but where the a lot of the magic happens you know I think it's still it's still in the same place um with these like large like pair-wise interaction modeling</p>
<p>>> um that's maybe like the most differentiated portion the other part I think that's in off three uh is sort of this moving away from modeling just at the amino acid level to actually sort of having um the model sort of alternate between um you know sort of atomic resolution modeling and then more like it's called token level which is like at the amino acid level that's also something that was introduced um that I think you know was particularly helpful in like you know modeling these other modalities like small molecule etc and like this idea of like coarse grain like</p>
<p>um finer grain is I think that's actually quite popular I think in other areas as well so that's maybe like not too surprising but yeah I think this the fact that you for some reason you you know the models that have so much more inductive bias when you when you go you know into this 2D representation I think</p>
<p><strong>[30:08]</strong></p>
<p>is is is very interesting. >> So you you mentioned coarse and fine grain and that brings to mind the sort of ribbony diagrams of proteins that I've that everyone has probably seen. Can you actually pull up a like a molecule and kind of talk about what >> you know what the different components of that protein are we're looking at like with the spiral and the arrows and all those what components and those like what what level of uh granularity are we looking at h like how do we think about</p>
<p>that how does a model think about that >> yeah so um there's actually a little image of our of our own bull platform. Um I have a protein here. Um and you actually like sort of see both uh the coarse grain and the finer grain here. So we have the sort of ribbon like structure here that is um you know representing these these different amino acids in the protein. But then like when we zoom in over this like interaction with the small molecule um then you see like sort of this at the atomic level</p>
<p>like how these things like you know interact with one another. There's even like the actual like bond interactions uh here that are like shown. Um and yeah like you know we we go from like this very abstract representation of these things you know like the the sequence the graph of the molecule and and the goal is like every single atom should have a coordinate and um and you know ends up looking like something like this. It's actually pretty pretty elegant. I think this is like something that's nice really nice that this field has done is like it's made really beautiful visualizations of stuff which is like really nice to look at and yeah</p>
<p>I mean this is this is this is one example >> and so the there's like okay um the there's like ribbons there there's like the coily ribbons there's arrows there's like some sort of like not coily ribbons like what do those mean how does someone think about those >> yeah so um we can zoom into a few different areas of the protein this one's actually a good example because there's a few different secondary structures here. So, um here you have, you know, we call an alpha helix. Um there essentially like sort of three</p>
<p>categories. There's the alpha helyses. Um this is where it takes a little bit of like this like ribbon uh shape. Um there's here what we call a better sheet. Um which actually, you know, as the name says, uh sort of like ribbon going like this forming forming a bit of a of a sheet. And then you have um these more like loopy regions uh which look like more unstructured and those are you know the parts of the protein that are most flexible. they are super important. Uh you know maybe like one of the most</p>
<p>like canonical you know drug modalities are antibodies and antibodies have like you know six of these loops that are like largely flexible but when they interact you know kind of come into this like fixed structure when interacting with the um you know with with its target. Um so harder to model and really critical to interactions. Um, and yeah,</p>
<p><strong>[33:10]</strong></p>
<p>those are largely the three sort of big families. >> Okay. And and as a, you know, as a structural biologist or just a biologist, when you look at that, so you you're basically looking, okay, here's the the sheet part. Here's the and then you're you're kind of saying, okay, that so that'll be bendy and then I have like these coils those like what what do those mean to you when you look at them? >> Yeah, I mean, you know, I I should say I am not a structural biologist by any any way, shape or form. Um but you know there's certain types of interactions that are more canonically associated with these different types of</p>
<p>structures. Yeah. >> Um I think uh a more well-versed structural biologist you know could give you a more thorough answer than that. I don't know if you know anything more than I do but yeah um yeah and and and you know like we've seen for example this this maybe related to that point like we've seen um you know some of the early successes of protein design um being able to design a binder you know to to any any target um a lot of the early success was like these like very you know alpha helix centric type peptides which I think are um almost</p>
<p>like bricks you know um and the models had like a pretty good understanding of those like kind of interactions and so like there was like good success with that and then took a little bit of time to like go from that to like you know more exotic uh binders and and things like that and so um yeah there's certainly a lot of um yeah a lot of important um interaction behaviors associated with with these structures. Yeah, >> another interesting thing that I think on the staying on the modeling machine learning side which I think is somewhat</p>
<p>counterintuitive seeing some of the other kind of uh fields and applications is that scaling hasn't really worked kind of the same uh in this field. Um now you know models like alpha 2 and alpha 3 uh are you know still very large models but at the same time they in terms of parameters they're actually not very big. they are definitely below a billion parameters. You know, if you hear these days in LLM space, you know, a model with less than a billion</p>
<p>parameters, you would think can do anything. But on the other hand, when you look at the computational cost of running these models, they are actually a lot more expensive than uh it is to run a language models because as Jeremy was saying, we go from instead of having sort of like quadratic operations, we now a cubic operation and and so it's interesting how right now in the field and and this is maybe related to you know having kind of less data or you know needing more inactive biases but we</p>
<p>have um this ratio of you know amount of computation to parameters that is much much higher than in other in other places. >> Yeah, if I recall Alpha 2 was like what 70 million parameters something like that. >> Um yeah it's it's something like that. It's quite yeah it's quite small around 100 or so. Yeah. >> So like these these decisions of</p>
<p><strong>[36:11]</strong></p>
<p>triangle layers and like these for alpha 2 this like interesting equavarian architecture like really were priors that it baked in a lot of the physics of the system and also co-evolution data is I think people have argued that is kind of like almost like a database lookup of some sorts. It also sort of so that provides in some sense more parameters as well. Yeah, I mean it's uh it's more definitely the amount of like you know pure like compute flops, right? Is is very high and it's almost like more yeah more almost more like reasoning based</p>
<p>maybe than like more just like information extraction. You know, I think one of the things that the part of the reason the LMS are so large isn't just because of their reasoning capability, but also because of like like the sheer quantity of information that they store. And I think here there's a little bit less of that, you know, and I think it's more about like, you know, decoding this input rather than maybe like memorizing as much of it. >> So is there like a loop in the architecture that allows it to compute more for per parameter? Like how does that work? Part of it is just you know exclusively this fact that instead of you know having operations that operate</p>
<p>on the uh on the single chain they operate on the pair wise and so you instead of having like quadratic number of kind of interactions you have a cubic number of interactions and so that on its own you know leads you to have you know smaller kind of representation sizes but more representation that leads to more flops but fewer parameters. On the other hand, you know, there is actually also this idea of, you know, they somewhat similar to to reasoning where you recycle kind of this operation. from Alpha 4 2 but also kind</p>
<p>of Alpha 4 3. They have this interesting framework where you know you start we as we were discussing kind of the input to the model is sort of like this initial understanding of the interactions either from the evolution of the multiple sequence but also potentially from what we call templates that are basically database lookup of similar structures. And so how the model works is that you know it decodes these and tries to understand a good you know potential rough structure of the pair wise</p>
<p>interaction and then what you can do is basically do this recycling where you feed this uh kind of understanding back to the input of the model and then try to decode it again and people do this you know three or four times and you know in some cases you know I've even tried to do it uh tens of times and so you can see it as a very very early version of kind of reasoning uh or you know trying to uh to get. >> Yeah. So you you know uh Alpha 2 really cool, Alpha 3 really cool. Um but Alpha</p>
<p>3 came with a catch and I think this catch was important for the development of you know bolts and so on. So >> yeah the catch was that it was an amazing paper nature uh paper but unfortunately they uh decided not to release the model. uh you know Alpha Fall 2 uh was open source and since then was was used I think the the reported</p>
<p><strong>[39:12]</strong></p>
<p>numbers is you know more than a million scientists. Alpha for free for you know commercial reasons that you know um did mine has since spin-off as a morphic lab that is now trying to become sort of like a new pharmaceutical company uh and decided to keep this model internal and and only use internally and now uh both you know we were in the field and you know building on top of models like Alphafold and so now we no longer add you know kind of the base starting point uh to build on top but even more importantly everyone in uh both kind of</p>
<p>academic research and in industry no longer had access to these incredible models that you know was you know really useful to try to understand um kind of biologies but also try to develop new therapeutics. And so we um decided that you know to to take the the matter in our own hands and decided to kind of um try to obtain a model that was of similar accuracy. And so largely also you know using a lot of you know the uh</p>
<p>information that was in the alpha free manuscript we went ahead and built boltzswan which was um the first fully open source kind of model to approach the the level of accuracy of of our fault 3 and you know along the way and and you know uh we can talk about it more but you know we realized that you know it was probably too ambitions to have you know to see this as a an academic project and you know there are a lot of things that were kind of missing and so um we decided to also</p>
<p>start a public benefit company to push kind of this this mission of you know democratizing access to these models that we started with bolts one >> quick interjection I mean I remember this it was actually shocking how fast you got bolts one out like it was just like two or three months right >> I think we started in late May and it came in November if I remember correctly. So slightly longer but yeah. Yeah, it was relatively quick. I mean</p>
<p>for what it's worth like >> you know we were working on some of the some similar ideas at the time. I think like we you know for example this idea of like having a diffusion model on top of um this like more this pair wise strong was something that we were we were exploring independently. Um now when the paper came out it was like really clear like especially for example on the data pipelines there was like so much that we were like not really doing and so um there was a lot to like catch up on. Um but we were already in a place I think where we had you know some</p>
<p>experience working in you know with with the data and working with these type of models and I think that put us already in like a good place to you know to to produce it quickly and you know and I would I would even say like I think we could have done it quicker. The problem was like for a while we didn't really have the compute and so we couldn't really train the model and actually we only trained the big model once. Uh</p>
<p><strong>[42:13]</strong></p>
<p>that's how much compute we had. We could only train it once and so like while the model was training we were like finding bugs left and right. Uh a lot of them that I wrote and like I would I remember like us like sort of like you know doing like surgery in the middle like stopping the run, making the fix like relaunching and um yeah we never actually went back to the start. We just like kept training it with like the bug fixes along the way. Uh which was >> impossible to reproduce now. >> Yeah. Yeah. No, that model is like has gone through such a curriculum that you know it's learned some weird stuff. Uh</p>
<p>but uh yeah, somehow by miracle it worked out. >> The other uh funny thing is that the way that we were training most of that model was through uh a cluster from the department of energy, but that's sort of like a share cluster that many groups use. And so we were basically training the model for 2 days and then it would go back into the queue and stay a week in the queue. And so it was it was it was pretty painful. And so we actually kind of towards the end um I caught up with with Deon the CEO of of Genesis and</p>
<p>and basically I was telling him a bit a bit about the project and you know kind of telling him about this frustration with the computer. And so luckily, you know, uh he offered to kind of help and so we uh we got the help from Genesis to, you know, finish up the um the model otherwise it probably would have taken a couple of extra weeks of weeks. >> Yeah. >> Yeah. Bolt one. How did that compare to Alpha Fold 3? And then and then there's some progression from there.</p>
<p>>> Yeah. So I would say kind of the bolts one but also kind of these other kind of set of models that came um around the same time were kind of approaching were a big leap from you know kind of the previous kind of open source models uh and you know kind of uh really kind of approaching the level of alpha 3. But I would say still say that you know even to this day there are you know some specific instances where uh alpha 3 uh works better. I think one one common</p>
<p>examples is antibbody antigen uh prediction where you know alpha fold 3 still seems to have an edge uh in in many situations. Obviously these are somewhat different models. They are you know you run them you obtain different results. So it's it's not always the case that one model is better than the other but kind of in aggregate we still uh especially at the time so 3 is you know still having a bit of an edge we should talk about this more when we talk about volt but like how do you know one</p>
<p>is one model is better than the other like so you I make a prediction you make a prediction like how do you know >> yeah so the easily you know the the great thing about kind of structure prediction and you know once we're going to go into the design space of designing new small molecule new proteins this becomes It's a lot more complex. But a great thing about structure prediction is that a bit uh like you know CASP was doing basically the way that you can evaluate</p>
<p><strong>[45:14]</strong></p>
<p>them is that you know you train uh the model on a structure that was you know released across the field up until a certain time. And you know one of the things that we didn't talk about that was really critical in all this development is the uh PDB which is the protein data bank is this um common resources basically common database where every uh biologist and uh publishes their structures and so we can you know train on you know all the structures that were put in the PTB until a certain date and then we</p>
<p>basically look for recent structures. Okay, which structures look pretty different from anything that was published before? Because we really want to try to understand generalization and on this new structure we evaluate all these different models. >> So you just know when alpha 4 was three three was trained, you know when you're you intentionally train to the same date or something like that. >> Exactly. >> Right. Yeah. >> And so this is kind of the way that you can somewhat easily kind of compare these models. Obviously that assumes</p>
<p>that you know the the training set >> you've always been very passionate about validation. I remember like diff doc and then there was like diff do l and dogen like you you really thought you've thought very carefully about this in the past like um yeah I mean actually I think dogen is like a really funny story that I think um I don't know I don't know if you want to talk about that it's an interesting like uh >> yeah I think one of the amazing things about putting things open source is that you know it um we get a ton of feedback from from the</p>
<p>field and you know sometimes we get kind of great feedback of people really liking the model. But honestly, most of the times and you know to be honest that's also maybe the most useful feedback is you know people sharing about where it doesn't work. And so you know at the end of the day it's critical and this is you know also something you know across other fields of of machine learning it's always critical to set uh to do progress in machine learning set clear uh benchmarks and you know as you know you start you know doing progress</p>
<p>of certain benchmarks then you know you need to improve the benchmarks and make them harder and harder and this is kind of the progression of you know how the field operates and so you know the example of of uh doctrine was you know we um published this initial uh model called diff do um in my first year PhD which was sort of like you know one of the early um models to try to predict uh bio kind of interactions between proteins small molecules um that we</p>
<p>about a year after alpha 2 was published and now on the one end you know on these benchmarks that we were using at the I am uh diff was doing uh really well kind of you know uh outperforming kind of some of the traditional physics based methods but on the other hand you know when we started you know kind of giving these uh tools to kind of many</p>
<p><strong>[48:17]</strong></p>
<p>biologists and uh one example was uh that we collaborated with was the group of Nick Pitzy at Harvard. uh we not started noticing that there was this clear pattern where for proteins that were very different from the ones that we're trained on uh the models was was struggling. And so you know that seemed clear that you know this is probably kind of where we should you know put our focus on. And so we first developed you know with uh Nick and his group a new benchmark and then you know went after</p>
<p>and said okay what can we change and kind of about the current architecture to improve this uh pattern and generalization and this is the same that you know we uh we're still doing today you know uh kind of where does the model not work you know and then you know once we have that benchmark you know let's try to uh throw everything we uh any ideas that we have at the pro >> and there's a lot of like healthy skepticism in the field which I think you know is is is great and I think you know it's very clear that there's a ton of things the models don't really work</p>
<p>well on but I think one thing that's probably you know undeniable is just like the pace of pace of progress you know and how how much better we're getting you know every year and so I think if you you know if you assume you know any constant you know rate of progress moving forward I think you know um things are going to look pretty cool at some point in future was only 3 years ago >> yeah I mean it's wild like >> what? >> Yeah. Yeah. Yeah. It's one of those things like even being in the field, you don't see it coming, you know, and like I think Yeah. Um hopefully we'll, you know, we'll we'll continue to have as</p>
<p>much as we've had the past few years. >> So, this is maybe an an aside, but I I'm really curious. You get this great feedback from the from the community, right, by being open source. Um my question is partly like okay yeah if you open source then everyone can copy what you did but it's also maybe balancing priorities right where you like all my customers are saying I want this like there's all these problems with the model yeah yeah yeah but that like my customers don't care right so like how</p>
<p>do you how do you think about that >> yeah so I would say a couple of things one is you know part of uh our goal with bolts and you know this is also kind of established as kind of the mission of the public benefit company that we started is to democratize the access to these tools. But one of the reason why we realized that Boltz needed to be a company it couldn't just be an academic project is that putting a model on GitHub is definitely not enough to get you know chemists and biologists you</p>
<p>know across you know uh both academia, biotech and and pharma to use your model to uh in their therapeutic programs. And so a lot of what we think about you know at Bolts beyond kind of the just the models is thinking about all the layers that come on top of the models to get you know from you know those models to something that can really uh enable</p>
<p><strong>[51:19]</strong></p>
<p>scientists uh in the industry. And so that goes you know into building kind of the right kind of uh workflows that take in kind of for example the data and try to answer kind of directly that those problems that you know the chemists and the biologists are asking and then also kind of building the infrastructure. And so this to say that you know even with kind of you know models fully open you know we see kind of a ton of um potential for you know um you know products in the space and um the</p>
<p>critical part about a product is that even you know for example with an open source model you know running the model is not free you know as we were saying these are pretty expensive model and especially and maybe we'll get uh into this you know these is we're seeing kind of pretty dramatic inference time scaling uh of of these models where you know the more you run them the better the results are. Uh but there you know you start getting into a point that compute and compute cost becomes a critical factor and so putting a lot of</p>
<p>work into building the right kind of infrastructure building the optimizations and so on really allows us to provide you know a much better service potentially to the open source models. But that to say you know even though you know with a product we can provide a much better service. I do still think and we will continue to put a lot of our models open source because the um the critical kind of role I think of open source models is you know helping kind of the community progress on the research and you know from which</p>
<p>we we all benefit and so you know we'll continue to on the one end you know put some of our kind of base models open source so that the field can can build on top of it and you know as we discussed earlier we learn a ton from you know the way that the field uses and builds on top of our models but then you know try to build a product that gives the best experience possible to scientists um so that you know like a chemist or a biologist doesn't need to you know spin off a GPU and you know set</p>
<p>up you know our open source model in a particular way but can just you know uh a bit like you know I even though I am a computer scientist machine learning scientist I don't necessarily you know take a open-source LLM and try to kind of spin it off. But, you know, I just maybe open um the Chipy app or CL code and just use it as an amazing product. Uh we kind of want to give the same experience to scientists around the world. >> I heard a good analogy yesterday that a surgeon doesn't want the hospital to</p>
<p>design a scalpel, >> right? So, just buy the scalpel. You wouldn't believe like the number of people even like in my short time um you know between a full three coming out and and the end of the PhD like the um number of people that would like reach out just for like us to like run off a full three for them >> you know or things like that just because like um or bolts in our case</p>
<p><strong>[54:21]</strong></p>
<p>>> you know just because it's like not that easy you know to do that you know if you're not a computational person and I think like part of the goal here is also that you know We continue to obviously build an interface with competitional folks but that you know the models are also accessible to like a larger broader audience and and then that comes from like you know good interfaces and stuff like that. >> I think one like really interesting thing about Bolt is that with the release of it you didn't just release a model but you created a community. >> Yeah. >> Did that community it grew very quickly. Did that surprise you and like what is then the evolution of that community and</p>
<p>how is that fed into Bolts? >> If you look at its growth, it's it's like very much like when we release a new model, it's like there's a big uh big jump. Um but yeah, it's I mean it's been great. You know, we have a Slack community that has like thousands of people on it. Um and it's actually like self-sustaining now, which is like the really nice part because, you know, it's it's almost overwhelming, I think. you know, to be able to like answer everyone's questions and help. It's really difficult, you know, with the the few people that we were, but it ended up that like, you know, people would answer</p>
<p>each other's questions and like sort of like, you know, help one another. And so, the the the Slack, you know, has been like kind of yeah, self self sustaining and that's been that's been really cool to see. Um, and um, you know, that's that's for like the Slack bar, but then also obviously on GitHub as well. We've had like a nice nice community. Um you know I think we also aspire to be even more active on it you know than we've been in the past 6 months which been like a bit challenging you know for us but um yeah the the community has been has been really great and you know there's a lot of papers</p>
<p>also that have come out with like new evolutions on top of bolts and um it's surprised us to some degree because like there's a lot of models out there and I think like you know sort of people converging on that was was really cool and you know I think it speaks also I think to the importance of like you know when when you put code out like to try to put a lot of emphasis in like making it like as easy to use as possible and something we thought a lot about when we released the the codebase um you know it's far from perfect but you know >> do you think that that was one of the factors that caused your community to</p>
<p>grow is just the focus on easy to use make it accessible >> I think so yeah and we we've we've heard it from a few people over over the over the years now and um you know and some people still think it should be a lot nicer and and they're right >> uh and they're right but um Yeah, I think it was, you know, at the time maybe a little bit easier than than other things. The other I think part that I think led to to the community and to some extent I think you know like the somewhat the trust in the community and kind of what we what we put out is the fact that you know it's not really been</p>
<p>kind of you know one model but and maybe we'll talk about it you know after bolts one you know there were maybe another couple of models kind of released you know uh or open source kind of soon after we kind of continued kind of that open source journey release bolts too where we are not only improving kind the structure prediction but also starting to do affinity prediction. So understanding kind of the strength of the interactions between these different</p>
<p><strong>[57:22]</strong></p>
<p>models which is this critical component critical property that you often want to optimize uh in discovery programs and then you know more recently also kind of protein design model. And so we've sort of been building this suite of of models that come together interact with one another where you know kind of there is almost an expectation that you know we we take very at heart of you know always having kind of you know across kind of the entire suite of different task the best or across the best model uh out there. So that it's sort of like uh our</p>
<p>open source tool can be kind of the go-to uh model for everybody in the in the industry. >> I really want to talk about bold gen. But before that one last question in this direction. Was there anything about the community which surprised you? Were there any like someone was doing something and you're like why would you do that? That's crazy. Or that's actually genius and I never would have thought about that. >> I mean we've had you know many contributions. I think like some of the interesting ones like I mean we had you</p>
<p>know this one individual who like wrote like a complex GPU kernel you know for part of the architecture um on on a piece of the funny thing is like that piece of the architecture had been there since Alpha 2 and I don't know why it took bolts for this you know office person to you to decide to do it but that was like a really great contribution we've had a bunch of others There's like you know people figuring out like ways to you know hack the model to do cyclic peptides like you know</p>
<p>there's I don't know if there's any other interesting one cool one and and this was you know something that initially was uh proposed as you know as a message in the slack channel by by Tim O'Donnell was basically he was you know there are some cases especially for example we discussed you know antibody antigen interactions where the models don't necessarily kind of uh get the right answer what he noticed is that you the models were somewhat stuck into predicting kind of the the antibbody to interact with a part of the antigen that</p>
<p>was incorrect. And so he basically um run the experiments. In this model, you can condition uh basically you can give hints. And so he basically gave uh you know random hints uh to the model basically. Okay, you should bind to this residue uh you should bind to the first residue or you should bind to the 11th residue or you should bind to the 21st residue. you know basically every 10 residue scanning the entire antigen and >> residues are the the >> the amino acids amino acid so the first amino acids the 11 amino acids and so</p>
<p>on. So it's sort of like doing a scan and then you know conditioning the model to predict all of them and then looking at the confidence of the model in each of those cases and taking the top and so it's sort of like a very somewhat crude way of doing kind of inference time search but surprisingly you know for for antibbody antigen prediction actually kind of helped quite a bit and so there's some you know interesting ideas</p>
<p><strong>[60:23]</strong></p>
<p>that you know as a um obviously as kind of developing the model you say kind of you know wow this is why would the model you know, be so dumb. But, you know, it's it's very interesting and and that, you know, leads you to also kind of, you know, start thinking about, okay, how do I can I do this, you know, not, you know, with this brute force, but, you know, in a in a smarter way. And so, we've also done a lot of work on that direction. >> And that speaks to like the, you know, the power of scoring. Uh we're seeing that a lot. I'm sure we'll talk about it more when we talk about bulls gen. But um you know, our ability to like take a</p>
<p>structure and determine that that structure is like good, >> you know, like somewhat accurate. uh whether that's a single chain or like an interaction is a really powerful you know way of improving you know the the models like sort of like you know if you can sample a ton and you assume that like you know if you sample enough you're likely to have like you know the good structure then it really just becomes a ranking problem. Um, and you know, now we're, you know, part of the inference time scaling that Gabri was talking about is is very much that it's</p>
<p>like, you know, the more we sample, the more we like, you know, the ranking model ends up finding something it really likes. Um, and so I think our ability to get better at ranking, I think is also what's going to enable sort of the next, you know, next big big breakthroughs. >> Interesting. But I guess there's a my understanding there's a diffusion model and you generate some stuff and then you I guess it's just what you said, right? Then you rank it using a score and then you finally um and so like can you talk about those different parts?</p>
<p>>> Yeah. So first of all like the one of the critical kind of you know beliefs that we had you know also when we started working on pulse one was sort of like the structure prediction models are somewhat you know our field version of some foundation models you know learning about kind of how proteins and other molecules interact and and then we can leverage that learning to do also to other things and so with Bolu we leverage that learning to do things uh affinity prediction So understanding</p>
<p>kind of you know if I give you this protein these small molecules how tightly is the interaction uh for bolshen what we did was taking kind of that kind of foundation models and then fine-tune it to predict kind of entire new proteins and so the way basically that that works is sort of like instead of uh for the protein that you're designing instead of feeding in an actual sequence you feed in a set of blank tokens and you train the models to you know predict both the structure of</p>
<p>kind of that protein and with the structure also what the different amino acids uh of um that proteins are. And so basically the way that uh bolt chain operates is that you feed a this a target a protein that you may want to kind of bind to or you know another DNA RNA and then you feed um the highle kind</p>
<p><strong>[63:26]</strong></p>
<p>of design specification of you know what you want your new protein uh to be for example it could be like an antibbody with a particular framework could be a peptide could be many other things >> and that's with natural lang language or >> and that's you know basically you know prompting and we have kind of this sort of like spec that you you specify >> and you know you feed kind of this this spec to the model and then the model translate this into you know a set of you know uh tokens a set of conditioning to the model a set of you know blank</p>
<p>tokens and and then you know basically decodes as part of the um diffusion models decodes a new structure and a new sequence for your your protein and you know basically and then we take that and as Jeremy was saying you know trying to score it and you know how good of you know a binder it is to that original target that you you're using both basically bolts to to predict the folding and the affinity to that molecule. So and then</p>
<p>that is your that kind of gives you a score. Is that >> exactly? So you you use this model to predict the structure and then you do two things. One is that you predict the structure >> and with something like bolts do and then you basically compare that structure with what the model uh predicted what bolshion predicted um and this is sort of like in the field calls consistency. It's basically you want to make sure that you know the structure that you're predicting is actually what you're trying to design and that gives</p>
<p>you a much better confidence that you know that's a good design. And so that's the the first filtering and the second filtering that we did as part of kind of the the Bolshion pipeline um that was released is uh that we look at the confidence that the model has in the structure. Now unfortunately kind of going to your your question of you know predicting affinity unfortunately confidence is not a very good predictor of affinity um and so one of the things</p>
<p>that we've actually done a ton of progress you know since uh we released Bolchen and kind of we have some uh new results that we are going to kind of announce soon is kind of you know the ability to get much better rates when instead of you know trying to rely on confidence of the model we are actually directly trying to predict the affinity uh of that interaction. >> Okay, just backing up a minute. So your diffusion model actually predicts not</p>
<p>only the protein sequence but also the folding of it. >> Exactly. And actually kind of the way one of the big um kind of different things that we did compared to uh other models in the space and you know there were some uh papers had already kind of done this before but we uh really scaled it up was you know basically</p>
<p><strong>[66:27]</strong></p>
<p>somewhat merging kind of the structure prediction and the sequence prediction into almost the same task. And so the way that Bolsten works uh is that you are basically the only thing that you're doing is predicting the structure. So the only sort of like supervision is we give you a supervision on the structure but because the structure is atomic and you know the different amino acids have a different atomic composition basically from the way that you place the atoms. We also understand not only kind of the</p>
<p>structure that you wanted but also the identity of the amino acid that you know the models believed was there. And so we've basically instead of you know having these two supervision signals you know one discrete one continuous that somewhat you know don't interact well together we sort of like build kind of like an encoding of you know sequences in structures that allows us to basically use exactly the same supervision signal that we using to bolts 2 that you know uh you know largely similar to what Alpha 3 uh</p>
<p>proposed which is very scalable and we we can use that to design new proteins. >> Oh, interesting. >> Maybe a quick shout out to Hannes uh Stark on our team who like did all this work. Um yeah. >> Yeah, it was a really cool idea. I mean like looking at the paper and there's just this like encoding where you just add a bunch of I guess kind of atoms which can be anything and then they get sort of rearranged and then basically plopped on top of each other so that and then that encodes what the amino acid is</p>
<p>and there's sort of like a unique way of doing this. It was that was like such a really such a cool fun idea. Yeah, >> I think that idea was had existed before. Yeah, there were a couple of papers that had proposed this and and analysts really took it to um to the large scale. >> In the paper, a lot of the paper for both gen is dedicated to actually the validation of the model. In my opinion, we talk a all the people we basically talk about feel that this sort of like in the wet lab or whatever the appropriate you know sort of val like in</p>
<p>real world validation is the whole problem or not the whole problem a big giant part of the problem. So can you talk a little bit about the highlights from there that really because to me uh the results are uh impressive both from a the perspective of the you know the model and also just the the effort that went into the validation by a large team. First of all, I think I should tr start saying is that both when we were at MIT and Thomas Yakolas and Regina</p>
<p>Barcel's lab as well as at Bolts, you know, we are not a we're not a biolab and you know, we are not a therapeutic company. And so to some extent, you know, we were first forced to, you know, look outside of, you know, our group, our um team to do the experimental validation. And so one of the things</p>
<p><strong>[69:29]</strong></p>
<p>that um really honest uh in the team pioneer was the idea okay can we go not only to you know maybe a specific group and you know trying to find a specific system and you know maybe overfitit a bit to that system and and trying to validate but how can we test these models across a very wide variety of different settings so that you know anyone in uh in the field and you know printing design is you know such a kind of wide um task with all sorts of different applications from therapeutic to you</p>
<p>know bio sensors and uh many others that you know so can we get a validation that is kind of goes across uh many different tasks and so he basically put together you know I think it was something like you know 25 different you know academic and industry labs that you know sort of like committed to you know testing uh some of designs from the model and some of this testing is is still ongoing. uh and you know giving uh results kind of uh back to us in exchange for you know</p>
<p>hopefully getting some you know new se new great sequences for uh their task and and he was able to you know coordinate this you know very wide uh set of you know uh scientists and uh already in the paper I think we uh shared uh results from I think uh 8 to 10 different uh labs uh kind of showing results from you know designing peptides uh designing uh to target you know ordered proteins, peptides targeting</p>
<p>disordered proteins. We show results you know of uh designing proteins that bind to small molecules. Uh we showed results of you know designing nanobodies and across a wide variety of different targets. And so that sort of like gave to the to the paper a lot of you know validation and to the model a lot of validation that was kind of uh wide uh >> just uh again um for our nonbiologist uh audience peptides nanoparticles what are these things that are being designed</p>
<p>why like what is interesting about these particular things why are is there focus in them >> yeah so largely you know they're all proteins it's just different shape the proteins they peptides uh is is a small protein uh and is you know a relatively common type of of therapeutic the very common examples these days are the you know GLP1 ompic right >> ompic and so on they're all peptides formed by both canonical and</p>
<p>non-cononical amino acids um the when we think about kind of larger proteins there also can take different shapes There is you know maybe one of we have this term called mini proteins which is like a very vague term to say kind of any sort of shape. Uh but then there are</p>
<p><strong>[72:31]</strong></p>
<p>some specific shapes that you know proteins can take and um so one very common one is antibodies and antibodies are uh particular type of protein in our body that is involved uh in our immune system and it's formed by um basically a set of you know four different um protein chains you know two uh two heavy call heavy which are longer and to light that come together and form kind of this interesting structure and</p>
<p>those are you know very common type also of therapeutic because of you know the function that they have in our immune system and finally there are kind of what are called nanobodies that I mentioned that are sort of like the equivalent of antibodies but on specific in specific animals so there are some animals and I think some examples are >> llamas camels and sharks that instead of having kind of this more complex set of you know four proteins coming together, it's a single protein</p>
<p>and and so recent in recent years has been also a relatively common type of therapeutic uh that people are trying to design. And so these are sort of like have a similar function to antibodies but are simpler in terms of uh structure. >> And so those would be therapeutics for those animals or they relevant to humans as well? they're relevant to humans as well. Obviously, you need to do some work into uh quote unquote humanizing them, making sure that you know they have the right characteristic to so</p>
<p>they're not uh toxic to humans and so on. Uh but there are um some approved medicine in the uh in the market there are no >> there's a general pattern I think in like in trying to design things that are smaller, you know, like it's easier to manufacture. Um at the same time like that comes with like potentially other challenges like maybe a little bit less selectivity than like if you have something that has like more hands you know um but the yeah there's this big desire to you know try to yeah design</p>
<p>many proteins nanobodies small peptides you know that just are just great drug modalities >> and that that's because they're more selective. >> No generally I think it's largely a manufacturing thing. >> Oh okay I see. So it's you know the bigger the bigger the protein the more complex essentially. Yeah, >> I put a pin in. I want to understand like how do you actually build a protein? Like I know you guys are not wet lab technicians, but >> I want to like hear more about the validations that you've done. >> Um I can try</p>
<p>>> essentially like so we work with uh organizations to do the the lab evaluation. um we we don't do any of it ourselves and typically what we send those people is we send them you know the sequence of the target and the sequence of the binder in this case for example an antibbody it would be like a single single chain uh we have to order the DNA that's yet another company that produces that DNA sends it over and then</p>
<p><strong>[75:33]</strong></p>
<p>you have to like express the proteins so you have to like express the target you have to express the binders and by expressing what I mean is like you put you know the the DNA uh typically in like you know either uh some capsid or you put it in like a and then you express it in like a yeast for example or you can like do it in like sulfury systems now but um essentially you know you you use like typical biological mechanism um to >> you're kind of like hijacking the yeast to create >> yeah you give it the extra DNA and you're like okay like create this thing</p>
<p>>> so so you want to this is like um you're amplifying the DNA in some way is that basically >> yeah yeah so there yeah so they're jumping some steps there's like a yeah amplifying the DNA there's all this stuff and then um but at the end of the day the you starts to produce a lot of this protein then you need to purify it because there's a lot of other stuff in there so typically you have like a tag on the protein and then like based on that tag you can sort of purify um once you have like your pure target your pure binder you can then like run your binding assay and then that's where my</p>
<p>knowledge stops but there's various methods to do that uh you know you put things in a well and then you can measure um the you know the the binding strength um and then we get the results back um and we also get to know like you know whether it was a binder but also like how strong of a binder it was um and that's generally the the process >> right so so that you kind of you specify the molecule they create some DNA that can create the RNA they create some RNA</p>
<p>from that that creates the >> uh the protein You take the protein and you use lab voodoo to measure the the binding strength of that or the two proteins or two molecules. >> Yeah. Okay. That's right. >> Okay. So, we were I think we were left off we were talking about >> validation in the lab and I was very excited about seeing like all the diverse validations that you've done. Can you</p>
<p>>> Yeah. >> go into some more detail about them >> specific ones? Yeah, the nanobbody one I think we did what was it 15 targets is that correct 14 >> 14 targets um testing um so we typically the way this works is like we uh make a lot of designs all right on the order of like tens of thousands and then we like rank them and we pick like the top n uh in this case n was 15 right um for each target and then we like measure sort of like the success rates both on like how</p>
<p>many targets we were ble to um to get a binder for and then also like more generally like out of all of the you know binders that we designed how many actually proved to be uh good binders. Some of the other ones I think involved like yeah like we had a a cool one where there was a small molecule or design a protein that uh you know binds to it. Um that has a lot of like interesting</p>
<p><strong>[78:34]</strong></p>
<p>applications you know for example like Gabby mentioned like biosensing and things like that uh which is pretty cool. Um we had we had a disordered protein I think you mentioned also. Um and yeah I think some of maybe some those were some of the the highlights. >> Yeah. So I would say that the way that we structure kind of some of those validations was on the one end we have validations across a whole set of different problems uh that you know the biologists that we're working uh with came to us with. So we are trying to uh</p>
<p>for example in some of the experiments design peptides that would uh target rack C which is uh a target that is involved in metabolism. Um we had you know number of other uh applications where we were trying to design you know peptides or other modalities against some other therapeutic relevant targets. Um we designed some um proteins to bind small molecules and then some of the other uh testing that we did was really</p>
<p>trying to get like a more broader sense of how does the model work especially when tested you know on somewhat generalization. So one of the things that you know we we found uh with the field was that a lot of the validation especially outside of the validation that was done on specific problems was done on targets that have a lot of you know known interactions in in the training data. And so it's a bit always a bit hard to understand, you know, how much are these models really just</p>
<p>regurgitating kind of what they've seen or trying to imitate what they've seen in training data versus, you know, really be able to design uh new proteins. And so one of the experiments that we did was to uh take nine targets from um the PDB filter into things where there is no known interaction. uh in in the PDB. So basically the model has never seen kind of this particular</p>
<p>protein bound or a similar protein bound to another protein. So there is no way that the model uh you know from its training set can sort of like say okay I'm just going to uh kind of >> tweak something >> tweak something and and just imitate this particular kind of interaction and and so we we took those nine proteins we worked with adaptive CRO and basically tested you know 15 mini proteins and 15 nanobodies against each one of them and the very cool thing that we saw was that</p>
<p>on twothirds of those targets we were able to from these 15 designs uh get uh nanomolar uh binders. Nanomolar roughly speaking is just a measure of you know how strongly kind of the interaction is and roughly speaking kind of like a nanomolar binder is approximately the kind of binding strength of binding that you need for a therapeutic.</p>
<p><strong>[81:36]</strong></p>
<p>>> Okay. >> Yeah. So maybe switching uh directions a bit. Um so I I Boltz Lab was just announced um this week or was it last week? Yeah. >> Um uh this is like your first I guess product if if that's the if you want to call it that. Um can you talk about what Bolts Lab is and um yeah you know what you hope that people take away from this. Yeah, you know, as we mentioned like I think at the very beginning is the goal with the product has been to,</p>
<p>you know, address what the models don't on their own. Um, and there's largely sort of two categories there. Um, you know, or let's say let's say I'll split it in three. Um, the first one is that you know it's one thing to predict you know a single interaction for example like a single structure. Um it's another to like you know very effectively uh search a space a design space you know to to produce something of value and you</p>
<p>know what we've what we found like sort of building up this product is that there's a lot of steps involved you know in that that we sort of need to like you know accompany the user through. Um you know one of those steps for example is like you know the the creation of the target itself. you know, how do we make sure the model has like a good enough understanding of the target so we can like design something and there's all sorts of tricks, you know, that you can do uh to improve like a particular, you know, structure prediction. And so that's sort of like you know the first stage and then there's like this stage of like you know designing and searching</p>
<p>the space efficiently you know for something like BSG for example like you you know you design many things and then you rank them for example for small molecule the process is a little bit more complicated. we actually need to also make sure that uh the molecules are synthesizable. And so the way we do that is that you know we have a generative model that um learns to use like appropriate building blocks such that you know it can design within a space that we know is like synthesizable. And so there's like you know this whole pipeline really of different models involved you know in being able to um to</p>
<p>design a molecule. And so that's been sort of like the first thing we call them agents. We have a protein agent and we have a small molecule design agents. And that's really like at the core of like what powers, you know, the post platform. >> So these agents are are they like a language model wrapper or they're just like your models and you're just calling them agents because they they they sort of perform a function on behalf of >> they're more of like a you know a recipe if you wish and I think uh we use that term sort of because of you know sort of</p>
<p>the complex pipelining and automation you know that goes into like all this plumbing. Um so so that's the first part of the product. Um the second part is the infrastructure. You know we need to be able to do this at very large scale for any one you know group that's doing a design campaign. Um you know let's say you're designing you know let's say a 100,000 possible candidates right to find the good one. Um that is you know a</p>
<p><strong>[84:38]</strong></p>
<p>very large amount of compute. Uh you know for small molecules it's on the order of like a few seconds per uh per design. for proteins can be a bit longer and so you know ideally you want to do that in parallel otherwise it's going to take you weeks um and so you know we've put a lot of effort into like you know our ability to have a GPU fleet that allows any one user you know to be able to do this kind of like large parallel search >> so you're amvertising the cost over your your users basically >> exactly and you know to some degree like it's whether you do uh you use 10,000 GPUs for like you know uh a minute is the same cost as using you know uh one</p>
<p>GPUs for god knows how long, right? So, you might as well try to paralyze if you can. So, you know, a lot of work has gone has gone into that making it very robust, you know, so that we can have like a lot of people on the platform doing that at the same time. Um, and and the third one is is the interface and the interface comes in in two shapes. one is um in form of an API and that's you know um really suited for companies that want to integrate you know these pipelines these agents directly in existing you know workflows that they</p>
<p>have or like existing user interfaces that they have and we're already like partnering with you know a few distributors you know that are going to integrate our API and then the second part is the the act the user interface and you know we we've put a lot of thoughts also into that and this is when I I mentioned earlier you know this idea of like broadening the audience that's kind of what the the user interface is about and we've built a lot of interesting features in it. You know, for example, for collaboration, um you know, when you have like potentially multiple medicine chemists are going through the results and trying to pick out, okay, like what are the molecules</p>
<p>that we're going to go and test in the lab, it's powerful for them to be able to, you know, for example, each provide their own ranking and then do consensus building and um so there's a lot of features around, you know, launching these large job, but also around like collaborating on analyzing the results um that we try to solve, you know, with with that part of the platform. So, Boltz Lab is sort of combination of these three objectives into like one, you know, sort of cohesive platform. >> Who is this accessible to? >> Everyone. Uh, you do need to request access today. We're still like, you know, sort of ramping up the usage. Um, but anyone can request access. Um, if</p>
<p>you are an academic in particular, uh, we uh, you know, we provide um, you know, a fair amount of free credit. So, you can like play with the platform. If you are a startup uh or a biotech, you may also, you know, reach out and we'll typically like actually hop on a call just to like understand what you're trying to do and um also provide a lot of free credit to to get started. Um and uh of course also with larger companies uh you know we uh we can deploy this this platform in a more like secure environment and so that's like more like custom you know uh deals that we make</p>
<p>you know with with the partners. Um, so you know that and that's sort of that the ethos of Bolt. I think this idea of like uh servicing everyone and not necessarily like going after just you know the um the really large enterprises. Um and that starts from the open source but it's also you know main a key key design principle of of the</p>
<p><strong>[87:38]</strong></p>
<p>product itself. >> Yeah. >> One thing I was thinking about with regards to infrastructure like in the LLM space you know the cost of a token has gone down by I think a factor of a thousand or so over the last three years right. Yeah. >> And is it possible that like essentially you can exploit economies of scale and infrastructure that you can make it cheaper to run these things yourself than for any person to roll their own >> 100%. Yeah. I mean we're already there you know like running bolts on our platform especially on in a large screen is like considerably cheaper um than it would probably take anyone to put the open source model out there and run it.</p>
<p>And you know on top of the infrastructure like one of the thing that we've been working on is is accelerating the models. So you know our our small molecule screening pipeline is 10x faster on bolts lab than it is in the open source. Um you know and that's that's also part of like you know building building a a product you know of something that that scales really well. Um and yeah uh we really wanted to get to a point where like you know we could keep prices uh very low um you know in in a way that it would be a no</p>
<p>no-brainer you know to to use bolts through through our platform. How do you think about validation of your like agentic systems? Because you know as you saying earlier like we're alphafold style models are really good at let's say monomeic you know proteins where you have you know co-evolution data but now suddenly the whole point of this is to design something which doesn't have >> you know co-evolution data something which is really novel. So now you're basically leaving the the domain that you thought was you know that you you</p>
<p>know you were good at. So like how do you validate that? >> Yeah. Um I mean I I like Gabri complete but there's um there's obviously you know a ton of computational metrics that we rely on but those are only take you so far. Um you really got to go to the lab you know and and test you know okay with this method A and this method B uh how much better are we you know how much better is my uh my hit rate? How stronger are my binders? Also it's not just about hit rate it's also about how how good the binders are. And there's really like no way no way around that.</p>
<p>And I think, you know, we've really ramped up the amount of experimental validation um that we do so that we like really track progress, you know, as scientifically sound, you know, as as possible. um out of this I think. >> Yeah. Know I think you know one thing that is unique about us and maybe companies like us is that because we're not working on like maybe a couple of therapeutic uh pipelines where you know our validation would be focused on those. We when we do an experimental validation, we try to test it across</p>
<p>tens of targets. And so that on the one end we can get a much more statistically significant um results and and really allows us to make progress from the methodological side without being you know steered by you know overfitting on any one particular system. And of course we choose you know we always try to choose uh targets and problems are sort</p>
<p><strong>[90:41]</strong></p>
<p>of like at the frontier of what's possible today. So you know you don't want something too easy you don't want something too hard otherwise you're not going to see progress. And so you know this is a somewhat evolving set of targets. We talked earlier about the targets that we looked at with with bolt and now we are even trying kind of you know even harder targets both for molecule and proteins. And so we try to keep oursel on the on the boundary uh of what's possible. >> So do you have like infrastructure or this is like you just have a lot of different partnerships with academic labs and you're just going to keep</p>
<p>pushing on these and driving these? We do partially this through academic labs but more and more we do this through uh CRO just because of you know to some extent is also we need kind of replicability often kind of you know going after the same targets you know multiple times and you know to see the the progress from you know one month to the next >> and speed of execution. Yeah. >> And so what happens if you start getting a bunch of like really strong biters against therapeutic targets? What do you do? >> Um I miss them. Yeah.</p>
<p>>> In open sort like >> Yeah. I mean, you know, I mean, when we say we have no interest in making dress, we're serious like you know, uh I mean when it when it was with the academic labs basically, you know, it was they keep it, they do whatever they want with it. And with the with the CRO so far, yeah, we've been we've been very Yeah. releasing releasing them. You know, I I will also say and and I think this is a bit been a bit of the issue that I I have with with some of kind of the things I've been said in the field is like when we say that we design new</p>
<p>proteins or we say that we design new molecules, you know, uh that you know go and bind these particular targets, >> we should be very clear, you know, these are not drugs, you know, these are not things are ready to be put into into a human And there is still a lot of development that that goes with it. And so this is this is kind of to to us you know we see oursel as you know building tools for scientists. You know at the end of the day you know it really relies on the scientist having a great</p>
<p>therapeutic appease and then pushing through kind of all the stages of development and you know we try to build tools that can accompany them uh in that journey. We It's not like a a magic box where, you know, uh you can just turn it and get >> get FDA approved drugs, >> FDA approved drugs, FDA approved drugs. Um, yeah. But but actually that brings up an interesting question that I have I've been wondering about is do you guys see yourself like staying in this like this for lack of a better way of saying</p>
<p>it layer or do you think that you'll start to like either on the physical sense looking at different layers of the virtual cell so to speak or um also you know so there's the like the development process that goes you know sort of like design preclinical clinical approval and thinking about</p>
<p><strong>[93:43]</strong></p>
<p>like improving the performance throughout that process based on the designs. Is that a direction that you guys are pushing? >> Yeah. So one of the things as Jeremy said you know we are not a therapeutic company and we want to kind of stay not to be a therapeutic company always be at the service of you know all the different you know companies including therapeutic companies that we serve and you know that to some extent does mean you know that we need to try to you know go deeper and deeper in getting these models better and better. One of the</p>
<p>things that we are doing across you know uh many other uh in the field is you know now that we already they're start to be good both for small molecule and for proteins to design kind of binders designed relatively tight binders is starting to look at all these other properties you know called developabilities or atme that you know we care about when developing a drug and trying can we uh design them from uh from get and the thing about those properties in some of them, you know, um</p>
<p>you need to, you know, start having an understanding of of the cell and and so that's on the one hand kind of why we need that understanding, but also, you know, the way that we also think about kind of, you know, um all different and complex diseases is that these models and these tools that we're building have a good understanding of kind of, you know, biomolelecular interactions and kind of their interactions. Now at the same time every disease is often kind of unique and every therapeutic hypothesis is unique and so you maybe want to have</p>
<p>something that um needs to uh hit the particular you know um let's say target uh in in a virus in a particular way but you don't maybe know exactly what uh way you want to do. And so maybe in the first set of designs, you're going to try to target different epitopes in different ways and then you're going to test them in the lab, maybe directly in vivo, and you're going to see which ones work and which one don't. And so then you need to bring those results back</p>
<p>into the models. And then the models can start to have a more uh wider understanding you know not just of the biohysical of the uh antibodies interacting with that target but also how that is shaped within the entire uh the entire cell. And so first of all you know that means on the one end that we need you know kind of these loops and this is also partially how we we design the platform to be but that also means that we also need to start understanding more and more kind of higher level things and you know I wouldn't say that</p>
<p>we're working in any way on like a virtual cell like others are but we're definitely thinking kind of very deeply about kind of you know how does you know kind of the way that we target um certain proteins interfere here interact with you know maybe pathways that are existing in the cell. >> One question that has come up is you talk a lot about user interface and so</p>
<p><strong>[96:43]</strong></p>
<p>on and I think this is really important but like my experience with dealing with medicinal chemists when you give them machine learning models is they are the most superstitious skeptical like pseudo religious people I've ever talked to when it comes to doing science. So sorry for the medicinal chemist. Yeah, they're they're amazing. Like they're absolutely I've worked with some spectacular medicinal chemists who just pull magic out of their hat again and again and I have no idea how they do it. But when you bring them a machine learning model, it is sometimes quite tricky to get them to deal with it. How how has your</p>
<p>interaction been with this and how have you thought about like building Boltz Lab to work with the skeptics? One of the great value unlocks for us and for our product has been when we brought to the team um m chemist his name is Jeffrey. So I think kind of like on the one end you know day one you know he obviously had a lot of opinions on kind of a lot of the ways that we should uh change you know both kind of the way that the agents worked the way that the platform worked. Uh but it's been really amazing kind of you know once also we</p>
<p>started kind of shaping kind of the platform uh in a better way with with this feedback how we went from you know some extent you know fair skepticism to him you know actually using a lot more compute than any of our computational uh folks in the team you know um at times that you know he's you know running you know he has all these sort of tur uh hyp hypothesis. Okay, maybe I can hit this protein this particular way. I can hit</p>
<p>in that way. Actually, let me look at for this particular molecular space. Let me try to optimize for this particular interactions. So, he ends up, you know, running several screens in parallel, you know, using hundreds of GPUs, you know, on his own. And you know, so this has been, you know, pretty incredible to see kind of how, you know, maybe the way that I was more thinking about a problem, which is okay, you just trying to design a binder, a small molecule to a particular protein. The way that he thinks about it is, you know, much more deeply and, you know, trying all these</p>
<p>different things, these different hypothesis. And then you know once he gets the results from the um from the model he doesn't just you know take the top 15 uh but he it really kind of looks over and you know kind of tries to understand you know the different things and then when we uh select you know maybe some designs to uh to bring forth you know he has you know something where you know both the models understand that something is good but all himself as well and that's why we also built kind of the platform to be uh an interface for you know this kind uh this kind of</p>
<p>chemist and you know also like collaborative experience. I >> I think at the end of the day like you know for people to be convinced you have to show them something that they didn't think was possible and until you have that aha moment you know I think the skepticism will remain but then when you know every once in a while I think there's like a result that like really surprises people and then it's like oh</p>
<p><strong>[99:44]</strong></p>
<p>wow okay this actually I can do something with this. >> So you just get in their hands have them try it out and they'll be convinced. >> Yeah. or like at maybe once the lab results come back >> or their their friend Yeah. or maybe one of their colleagues is convinced and >> I think it it it takes going to the lab I think at some point there's no avoiding that you know as beautiful as the platform can be as nice as the molecules might look you know that the model predicted I think what really convinces people is like you know hits >> you see the results</p>
<p>>> exactly yeah >> cool thank you for you know taking the time to chat with us >> yeah you know is there anything that you would like your audience to know >> I mean first of all you know uh we're just getting started, you know, uh continuing to to build a team. And so, uh definitely always looking for, uh great folks both on, you know, kind of, you know, software side, you know, machine learning side, but also scientists, uh to join the team and help us, you know, uh shape >> on the infrastructure side too.</p>
<p>>> Indeed. >> If you if you think that if you want a new challenge because this is not just next token prediction, this is really a new engineering challenge. >> Exactly. If you if no matter you know how much experience you have with you know biologist and chemistry if you want to come you know help us you know shape what you know biology and chemistry hopefully will look like in 5 10 years um we'd love to hear from you and so um go to bold bio and you know come join the team. >> Cool. Thank you.</p>
<p>>> Awesome. Thank you so much. Thank you.</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=nP0N1kYLegc</guid>
      <pubDate>Thu, 12 Feb 2026 02:05:23 +0000</pubDate>
    </item>
    <item>
      <title>The AI Frontier: from Gemini 3 Deep Think distilling to Flash — Jeff Dean</title>
      <link>https://www.youtube.com/watch?v=F_1oDPWxpFQ</link>
      <description>From rewriting Google’s search stack in the early 2000s to reviving sparse trillion-parameter models and co-designing TPUs with frontier ML research, Jeff Dean has quietly shaped nearly every layer of the modern AI stack. As Chief AI Scientist at Google and a driving force behind Gemini, Jeff has lived through multiple scaling revolutions from CPUs and sharded indices to multimodal models that reason across text, video, and code.

Jeff joins us to unpack what it really means to “own the Pareto frontier,” why distillation is the engine behind every Flash model breakthrough, how energy (in picojoules) not FLOPs is becoming the true bottleneck, what it was like leading the charge to unify all of Google’s AI teams, and why the next leap won’t come from bigger context windows alone, but from systems that give the illusion of attending to trillions of tokens.

We discuss:
• Jeff’s early neural net thesis in 1990: parallel training before it was cool, why he believed scaling would win decades early, and the “bigger model, more data, better results” mantra that held for 15 years
• The evolution of Google Search: sharding, moving the entire index into memory in 2001, softening query semantics pre-LLMs, and why retrieval pipelines already resemble modern LLM systems
• Pareto frontier strategy: why you need both frontier “Pro” models and low-latency “Flash” models, and how distillation lets smaller models surpass prior generations
• Distillation deep dive: ensembles → compression → logits as soft supervision, and why you need the biggest model to make the smallest one good
• Latency as a first-class objective: why 10–50x lower latency changes UX entirely, and how future reasoning workloads will demand 10,000 tokens/sec
• Energy-based thinking: picojoules per bit, why moving data costs 1000x more than a multiply, batching through the lens of energy, and speculative decoding as amortization
• TPU co-design: predicting ML workloads 2–6 years out, speculative hardware features, precision reduction, sparsity, and the constant feedback loop between model architecture and silicon
• Sparse models and “outrageously large” networks: trillions of parameters with 1–5% activation, and why sparsity was always the right abstraction
• Unified vs. specialized models: abandoning symbolic systems, why general multimodal models tend to dominate vertical silos, and when vertical fine-tuning still makes sense
• Long context and the illusion of scale: beyond needle-in-a-haystack benchmarks toward systems that narrow trillions of tokens to 117 relevant documents
• Personalized AI: attending to your emails, photos, and documents (with permission), and why retrieval + reasoning will unlock deeply personal assistants
• Coding agents: 50 AI interns, crisp specifications as a new core skill, and how ultra-low latency will reshape human–agent collaboration
• Why ideas still matter: transformers, sparsity, RL, hardware, systems — scaling wasn’t blind; the pieces had to multiply together

Substack Article w/Show Notes: https://www.latent.space/p/jeffdean

—

Jeff Dean
• LinkedIn: https://www.linkedin.com/in/jeff-dean-8b212555
• X: https://x.com/jeffdean

Google
• https://google.com
• https://deepmind.google

00:00:00 Intro
00:01:31 Frontier vs Flash &amp; Distillation Strategy  
00:05:09 Distillation, RL &amp; Flash Economic Advantage  
00:07:35 Flash in Products + Importance of Latency  
00:11:11 Benchmarks, Long Context &amp; Real Use Cases  
00:15:01 Attending to Trillions of Tokens &amp; Multimodality  
00:20:11 LLM Search &amp; Google Search Evolution  
00:24:09 Systems Design Principles + Latency Numbers  
00:32:09 Energy, Batching &amp; TPU Co-Design  
00:42:21 Research Frontiers: Reliability &amp; RL Challenges  
00:46:27 Unified Models vs Symbolic Systems (IMO)  
00:50:38 Knowledge vs Reasoning + Vertical/Modular Models  
00:55:58 Multilingual + Low-Resource Language Insights  
00:57:58 Vision-Language Representations Example  
01:07:15 Gemini Origin Story + Organizational Memo  
01:09:27 Coding with AI &amp; Agent Interaction Style  
01:14:26 Prompting Skills &amp; Spec Design  
01:19:54 Latency Predictions &amp; Tokens/sec Vision  
01:21:29 Future Predictions: Personal Models &amp; Hardware  
01:23:11 Closing</description>
      <content:encoded><![CDATA[<p><strong>[00:00]</strong></p>
<p>[music] Hey everyone, welcome to the L in space podcast. This is Allesio, founder of Colonel Labs, and I'm joined by Swix, editor of L in Space. >> Hello. Hello. We're here in the studio with Jeff Dean, chief AI scientist at Google. Welcome. >> Thanks for having me. [laughter] >> It's a bit surreal to have you in the studio. I've I've watched so many of your talks uh and obviously uh you your career has been super legendary. So, uh I mean, congrats. I I think the the first thing must be said congrats on owning the Purto Frontier. [laughter] >> Thank you. Thank you. Parto Frontiers are good and it's good to be out there.</p>
<p>>> Yeah. I mean I I think it's a combination of both uh your you have to own the Parto Frontier you have to have like frontier capability but also efficiency and then offer that range of models [snorts] that people like to use. uh and you know some part of this was started because of your hardware work some part of that is your model work and uh you know I'm sure there's lots of secret sauce that you guys uh have worked on uh accumulatively but like it's it's really impressive to see it all come together in like this steadily</p>
<p>advancing frontier. >> Yeah. Yeah. I mean I think as you say it's not just one thing it's like a whole bunch of things up and down the stack >> and uh you know all of those really combined to help make you an OS able to make highly capable large models as well as you know software techniques to get those large model capabilities into much smaller lighter weight models that are you know much more cost-effective and lower latency but still you know quite capable for their size. So >> yeah, >> how how much pressure do you have on</p>
<p>like having the lower bound of the prior frontier too? I think like the new labs are always trying to push the top performance frontier because they need to raise more money and all of that. And you guys have billions of users and I think initially when you work on the CPU you were thinking about you know if everybody that used Google we used the voice model for like 3 minutes a day they were like you need to double your CPU number like what's that discussion today at Google like how do you prioritize frontier versus like we actually need to deploy it if we build it. Yeah, I mean I think we always want</p>
<p>to have models that are at the frontier or pushing the frontier because I think that's where you see what capabilities now exist that didn't exist at the sort of slightly less capable last year's version or last [clears throat] six months ago version. >> Um at the same time, you know, we know there those are going to be really useful for a bunch of use cases, but they're going to be uh a bit slower and a bit more expensive than people might like for a bunch of other broader use cases. So I think what we want to do is always have um kind of a highly capable</p>
<p>uh sort of uh affordable model that enables a whole bunch of you know lower latency use cases. People can use them for agentic coding much more readily. Um and then have the the high-end you know frontier model that is really useful for um you know deep reasoning you know solving really complicated math problems those kinds of things. And and it's not</p>
<p><strong>[03:01]</strong></p>
<p>that one or the other is useful. They're both useful. So I think we like to do both. And also, you know, through distillation, which is a key technique for making the smaller models more capable, you know, you have to have the frontier model in order to then distill it into your your smaller model. So it's not like an either or choice. You sort of need that in order to actually get a highly capable more modest size model. >> Yeah. And I mean you and Jeffrey In came out with this solution in 2014. >> Don't forget L'Oreal Vine as well. a long time ago. Like >> I'm curious how you [snorts] think about</p>
<p>the cycle of these ideas even like you know sparse models and uh you know how do you re-evaluate them? How do you think about in the next generational model what is worth revisiting like a yeah they're just kind of like a you know you worked on so many ideas that end up being influential but like in the moment they might not feel that way necessarily. Yeah, I mean I I think distillation was originally motivated because we were seeing that we had a very large image data set at the time, you know, 300 million images that we could train on with, you know, I forget like 20,000 categories or something, so</p>
<p>much bigger than ImageNet. And we were seeing that if you create specialists for different subsets of those image categories, you know, this one's going to be really good at sort of mammals and this one's going to be really good at sort of indoor room scenes or whatever. and you can cluster those categories and train on an enriched stream of data after you do pre-training on on a much broader set of images. You get much better performance if you then treat that whole set of maybe 50 models you've trained as a large ensemble. Um but</p>
<p>that's not a very practical thing to serve, >> right? So distillation really came about from the idea of okay what if we want to actually serve that and train all these independent sort of expert models um and then squish it into something that actually fits in a form factor that you can actually serve. And that's you know not that different from what we're doing today. You know often today we're instead of having an ensemble of 50 models we're having a much larger scale model that we then distill into a much smaller scale model.</p>
<p>Yeah, a part of me also wonders if distillation also has a story with the RL um revolution. So what let me let me maybe try to articulate what I mean by that. uh which is you can uh RL basically spikes models in a certain uh part of the distribution and then you have to sort of well you can spike models but usually sometimes it might be lossy in other areas and it's kind of like an uneven technique but you can probably distill it back uh and you can</p>
<p>uh I think that the sort of general um dream is to be able to advance capabilities without regressing on anything else >> and I think like that that whole capability merging without loss. Uh uh I feel like it's like you know some part of that should be a distillation process but I can't quite articulate it. I haven't seen much papers about it.</p>
<p><strong>[06:02]</strong></p>
<p>>> Yeah. I mean I I tend to think of one of the key advantages of distillation is that you can have a much smaller model and you can have a very large uh you know training data set and you can get utility out of making many passes over that data set because you're now getting the logits from the much larger model in order to sort of >> sort of coax the right behavior out of the smaller model uh that you don't wouldn't otherwise get with just the hard labels and and so um you know I think that's what we've observed is you can get, you know, clo very close to</p>
<p>your largest model performance with distillation approaches. And that that seems to be, you know, a nice sweet spot for a lot of people because it enables us to kind of for multiple Gemini generations now, we've been able to make >> the sort of flash version of the next generation >> as good or even substantially better than the previous generations pro. And I think we're going to keep trying to do that because that seems like a good uh trend to follow. >> Um dare I ask uh so it was it was the original map was Flash Pro and Ultra.</p>
<p>>> Uh is ultra are you just sitting on ultra and distilling from that? Is that like the mother load? [laughter] >> Uh I mean we have a lot of different kinds of models. Some are internal ones that are not necessarily meant to be released or served. Some are you know our pros scale model and we can distill from that as well into our flash scale model. So I think you know uh it's u it's an important set of capabilities to have and also inference time scaling can also be a useful thing to improve the capabilities of a model and >> yeah cool yeah and obviously I think the</p>
<p>economy of flash is what led to the total dominance I think the the latest number is like 50 trillion uh tokens I I don't know I mean obviously it's changing every day >> but uh you know by market share >> hopefully hopefully up [laughter] >> no I mean there's no I mean Just the economics wise like uh because flash is so economical like you can use it for everything like it's in Gmail now it's in YouTube like it's it's in everything >> we're using it more in our search products of various AI mode overviews. >> Oh my god flash parts AI mode. Oh my god. Yeah that's yeah I didn't even</p>
<p>think about that. >> Um [laughter] I mean I think one of the things that is uh quite nice about the flash model is not only is it more affordable it's also a lower latency. And I think latency is actually a pretty important characteristic for these models because we're going to want models to do much more complicated things that are going to involve, you know, generating many more tokens from when you ask the model to do something until it actually finishes what you ask it to do because you're going to ask now not just write me a for loop, but like write me a a whole software package to</p>
<p>do X or Y or Z. And so having low latency systems that can do that uh seems really important. and flash is one direction, one one way of doing that. >> Yeah. >> You know, obviously our hardware platforms enable a bunch of interesting aspects of our, you know, serving stack as well like TPUs. Uh the interconnect between chips on the TPUs, uh is</p>
<p><strong>[09:03]</strong></p>
<p>actually quite quite high performance and quite amendable to for example long context kind of attention operations. You know, having sparse models with lots of experts. These kinds of things really really matter a lot in terms of how do you make them servable at scale. >> Yeah. Does it feel like there's some breaking point for like the protoflash distillation kind of like one generation delayed? I I almost think about almost like the capability asmtote in certain tasks like the pro model today is as saturated some sort of task. Mhm.</p>
<p>>> So next generation that same task will be saturated at the flash price point and I think for most of the things that people use models for at some point the flash model in two generation will be able to do basically everything and how do you make it economical to like keep pushing the pro frontier when a lot of the population will be okay with the flash model? I'm curious how you think about that. >> I mean I think that's true if your distribution of what people are asking people the models to do is stationary, right? But I think what often happens is</p>
<p>as the models become more capable, people ask them to do more, right? So I mean I think this happens in my own usage like I used to try our models a year ago for some sort of coding task and it was okay at some simpler things but wouldn't do work very well for more complicated things. And since then we've improved dramatically on the more on the more complicated coding tasks and now I'll ask it to do much more complicated things. And I think that's true not just of coding but of you know now you know can you analyze all the you know</p>
<p>renewable energy uh deployments in the world and give me a report on solar panel deployment or whatever. That's a very complicated you know more complicated task than people would have asked a year ago. >> And so you are going to want more capable models to push the frontier in some sense of what people ask the models to do. And that also then gives us insight into okay where does the where do things break down? How can we improve the model in these these particular areas uh in order to sort of um make the next generation even better?</p>
<p>>> Yeah. Are there any benchmarks or like test sets that you use internally? Because it's almost like the same benchmarks get reported every time and it's like all right it's like 99 instead of 97. Like how do you have to keep pushing the team internally too to like this is what we're building towards? >> Yeah. I mean, I think benchmarks, particularly external ones that are publicly available, have their utility, but they often kind of have a lifespan of utility where they're introduced and maybe they're quite hard for current models. You know, I I like to think of</p>
<p>the best kinds of benchmarks are ones where the initial scores are like 10 to 20 or 30% maybe, but not higher. And then you can sort of work on improving that capability for uh whatever it is the benchmark is trying to assess and get it up to like 80 90% whatever. I I think once it hits kind of 95% or</p>
<p><strong>[12:03]</strong></p>
<p>something you get very diminishing returns from really focusing on that benchmark because it's sort of it's either the case that you've now achieved that capability [snorts] or there's also the issue of leakage in public data or very related kind of data being being in your training data. Um, so we have a bunch of held out internal benchmarks that we really look at where we know that wasn't represented in the training data at all. There are capabilities that we want the model to have um that it doesn't have now and then we can work on, you know, assessing, you know, how do we make the model better at these</p>
<p>kinds of things? Is it we need different kind of data to train on that's more specialized for this particular kind of task? Do we need um you know a bunch of uh you know architectural improvements or some sort of uh model capability improvements? You know what would help make that better? >> Is there is there such an example that you uh a benchmark inspired an architectural improvement? like uh I'm just kind of jumping on that because you just >> uh I mean I think some of the long context capabilities of the of the Gemini models that came I guess first in</p>
<p>1.5 >> really were about looking at okay we want to have um >> you know [clears throat] >> immediately everyone jumped to like completely green charts of like everyone had I was like how did everyone crack this at the same time like [laughter] >> right yeah I mean I think um and once you're set I mean as you say that needle single needle in a haststack benchmark is really saturated for at least context lengths up to 128k or something. I think most people >> don't actually have you know much larger than 128k these days or 256 or</p>
<p>something. Um you know we're trying to push the frontier of 1 million or 2 million context language. >> I think Google's still the leader 2 million. >> Yep. which is good because I think there are a lot of use cases where you know putting a thousand pages of text or putting you know multiple you know hourlong videos in the context and then actually being able to make use of that is useful but the the single needle in a haststack benchmark is sort of saturated. Um so you really want more complicated uh sort of multi- needle or</p>
<p>you know more realistic take all this content and produce this kind of answer from uh uh a long context that sort of better assesses what it is people really want to do with long context which is not just you know can you tell me the product number for this particular thing. >> Yeah it's retrieval it's it's retrieval within machine learning. Uh yeah, it's it's interesting because like I think that the more meta lesson level I'm trying to operate at here is uh you have a benchmark you're like okay I see the</p>
<p>architectural thing I need to do in order to go fix that but like should you do it because sometimes you know that's an inductive bias basically that you're Jason we used to work at Google would say like exactly the kind of thing like yeah you're going to win short term longer term I don't know if that's going to scale you might have to undo that [laughter] >> I mean I I I like to sort of not focus</p>
<p><strong>[15:05]</strong></p>
<p>on exactly what solution one should drive but what capability would you want and I think we're very convinced that you know long context is useful but it's way too short today >> right like I think what you would really want is can I attend to the internet while I answer my question right [laughter] >> but that's not going to be solved by purely scaling the existing solutions which are quadratic so a million tokens kind of pushes >> uh what you can do you're not going to do that to a trillion tokens, let alone, you know, a billion tokens, let alone a</p>
<p>trillion. Um, but I think if you could give the illusion that you can attend to trillions of tokens, that would be amazing. You'd be find all kinds of uses for that. You would have um attend to the internet. you could attend to the pixels of YouTube and the sort of deeper representations that we can form for a single video, but across many videos, you know, uh on a personal Gemini level, you could attend to all of your personal state with your permission. So like your emails, your photos, your</p>
<p>>> yeah, >> your docs, your plane tickets you have. Um I I think that would be really really useful. And the question is, how do you get algorithmic improvements and system level improvements that get you to something where you actually can attend to trillions of tokens in some meaningful way? >> Yeah. But by the way, I think I I did some math and if like if you spoke all day every day for eight hours a day, um you only generate a maximum of like 100k tokens, which like very comfortably fits, [laughter]</p>
<p>>> right? But if you then say okay I want to be able to um understand everything people are putting on video. >> Exactly. Exactly. Well also I think that the classic example is um you start going beyond language into like proteins and whatever else is extremely information dense. >> Yeah. >> Yeah. I mean, I think one of the things about Gemini's multimodal aspects is we've always wanted it to be multimodal from the start. And so, you know, that sometimes to people means</p>
<p>text and images and video sort of humanlike and audio audio humanike modalities. But I think it's also really useful to have Gemini know about nonhuman modalities. like LAR sensor data from say Whimo vehicles or like robots or you know various kinds of health modalities, X-rays and MRIs and imaging and genomics information. Um and I think there's probably hundreds of modalities of data where you'd like the model to be able to at least be exposed</p>
<p>to the fact that this is an interesting modality and has certain meaning in the world. uh where even if you haven't trained on all the LAR data or MRI data you could have because maybe that's not you know doesn't make sense in terms of trade-offs of you know what you include in your main pre-training data mix at least including a little bit of it is actually quite useful because it sort of >> uh tempts the model that this is a thing. >> Yeah. Yeah. Do do you believe I mean</p>
<p><strong>[18:06]</strong></p>
<p>since we're on this topic and something I just get to ask you all the questions I always wanted to ask which is fantastic. uh like there are there some king modalities like modalities that supersede all the other modalities. So the a simple example was vision um can on a pixel level encode text and deepc had this deepr paper that did that. Uh vision has also been shown to maybe incorporate audio because you can do audio spectrograms and that's that's also like a vision uh capable thing like so so maybe vision is just the king modality and like >> yeah I mean [laughter] vision and motion are quite important things right</p>
<p>>> motion uh >> video as opposed to static images >> because I mean there's a reason evolution has evolved eyes like 23 independent ways because it's such a useful capability for sensing the world around you which is really what we want these models to be able to do is interpret the things we're seeing or the things we're we're paying attention to and then help us in uh using that information to to do things. >> Yeah, I I think motion uh you know I still want to shout out I think Gemini uh still the only native video</p>
<p>understanding model that is out there. Uh so I use it for YouTube all the time. >> Yeah. Yeah. I mean, it's actually I think people kind of are not necessarily aware of what the Gemini models can actually do with video. Like, uh, I have an example I've used in one of my talks. >> It had like, uh, it was like a YouTube highlight video of 18 memorable sports moments across the last 20 years or something. So, it has like Michael Jordan hitting some jump shot at the end of the finals and, you know, some soccer uh, goals and things like that. And you</p>
<p>can literally just give it the video and say, "Can you please make me a table of what all these different events are, what when the date is, when they happened, and a short description of the event." And so you get like now an 18 row table of that information extracted from the video, which is, you know, not something most people think of as like a turn video into SQL like table. >> Yeah. Has there been any discussion inside of Google of like you mentioned</p>
<p>tending to the whole internet? Right. Google it's almost built because the a human cannot tend to the whole internet and you need some sort of ranking to find what you need. >> Yep. >> That ranking is like much different for an LLM because you you can expect a person to look at maybe the first five six links in a Google search >> versus for an LLM should you expect to have 20 links that are highly relevant? like how do you internally figure out you know how do we build the AI mode that is like maybe like much broader >> search [clears throat] and span versus</p>
<p>like the more human one. >> Yeah. I mean I think even pre- language model based work you know our ranking systems would be built to start with a giant number of web pages in our index. Many of them are not relevant. So you identify a subset of them that are relevant with very lightweight kinds of methods. Now you're down to like 30,000</p>
<p><strong>[21:07]</strong></p>
<p>documents or something. And then you have gradually refine that to apply more and more sophisticated algorithms and more and more sophisticated sort of signals of various kinds in order to get down to ultimately what you show which is you know the final 10 results or you know 10 results plus other kinds of information. And I think an LLM based system is not going to be that dissimilar, right? you're going to tend to trillions of tokens, but you're going to want to identify, you know, what are the 30,000ish documents that with the, you know, uh, maybe</p>
<p>30 million interesting tokens and then how do you go from that into what are the 117 documents I really should be paying attention to in order to carry out the task that the user has asked me to do. Um and I think you know you can imag you can imagine systems where you have you know a lot of uh highly parallel processing to identify those initial 30,000 candidates maybe with very lightweight kinds of models. Um then you have some system that sort of helps you</p>
<p>narrow down from 30,000 to the 117 uh with maybe a little bit more sophisticated um model uh or set of models. And then maybe the final model is the thing that looks at 117 things. That might be your most capable model. So I think it has to it's going to be some system like that that is really enables you to give the illusion of attending to trillions of tokens. Um sort of the way Google search gives you you know not the illusion but you are searching the internet. Yeah.</p>
<p>>> But you're finding you know a very small subset of things that are that are relevant. >> Yeah. I I often tell a lot of people uh that are not steeped in like Google search history that uh well you know like BERT was like used like basically immediately inside of Google search uh and that improves results a lot right like I I don't I don't have any numbers off the top of my head but like I'm sure you that's obviously the most important numbers to to Google. Yeah, I mean I I think going to an LLMbased representation of text and words and so</p>
<p>on enables you to get out of the explicit hard notion of of particular words having to be on the page, but really getting at the notion of this topic of this page or this paragraph is highly relevant to this query. >> Yeah. Yeah. I I don't think people understand how much LMS have taken over all these very high traffic system. very high traffic. Yeah, like [laughter] >> it's Google. Uh it's YouTube. Uh YouTube has this like semantics uh ID thing where there's like every token or every uh item in the vocab is a YouTube video</p>
<p>or something that predicts the video using a code book which is absurd to me for YouTube size. And then most recently Grock also for for XAI which is like >> I mean I'll call out even before LLMs were used extensively in search we put a lot of emphasis on softening the notion of what the user actually entered into the query so that >> do you have like a history of like</p>
<p><strong>[24:08]</strong></p>
<p>what's the >> yeah I mean I actually gave a talk in uh I guess uh web search and data mining conference in 2009. >> Okay. uh where we never actually published any papers about the origins of Google search uh sort of but we went through sort of four or five or six generations four or five or six generations of uh redesigning of the search and retrieval system uh from about 1999 through 2004 or five and that talk is really about that evolution and one of the things that really happened in 2001 was we were</p>
<p>sort of working to scale the system in multiple dimensions. So one is we wanted to make our index bigger so we could retrieve from a larger index which always helps your quality in general uh because if you don't have the page in your index you're going to not do well. Um and then we also needed to scale our capacity because we were our traffic was growing quite extensively. Um and so we had you know a sharded system where you have more and more shards as the index grows. you have like 30 shards and then</p>
<p>if you want to double the index size you make 60 shards so that you can bound the latency by which you respond for any particular user query. Um and then as traffic grows you add more and more replicas of each of those. And so we eventually did the math that realized that in a data center where we had say 60 shards and um you know 20 copies of each shard we now had 1,200 machines uh with discs. and we did the math and we're like, hey, one copy of that index would actually fit in memory across,200</p>
<p>machines. >> Mhm. >> So in 2001 we introduced uh we put our entire index in memory. >> And what that enabled from a quality perspective was amazing because before you had to be really careful about, you know, how many different terms you looked at for a query because every one of them would involve a disk seek on every one of the 60 shards. And so you as you make your index bigger, that becomes even more inefficient. But once you have the whole index in memory, it's totally fine to have 50</p>
<p>terms you throw into the query from the user's original three or four word query because now you can add synonyms like restaurant and restaurants and cafe and uh beastro and all these things. And you can suddenly start uh sort of really uh getting at the meaning of the word as opposed to the exact semantic form. the user typed in. And that was, you know, 2001, very much preLLM, but really it was about softening the the strict definition of what the user typed in</p>
<p>order to get at the meaning. >> What are like principles that you use to like design the systems, especially when you have I mean in 2001 the internet is like doubling tripling every year in size. It's not like a you know, and I think today you kind of see that with LLMs too where like every year the jumps in size and like capabilities are just so big. Are there just any you know principles that you use to like think about this? >> Yeah, I mean I think uh you know first</p>
<p><strong>[27:11]</strong></p>
<p>whenever you're designing a system you want to understand what are the sort of design parameters that are going to be most important in deciding that you know so you know how many queries per second do you need to handle? How big is the index you need to handle? How much data do you need to keep for every document in the index? How are you going to look at it when you retrieve things? um what happens if traffic were to double or triple you know will that system work well and I think a good design principle is you're want to design a system so that the most important characteristics could scale by</p>
<p>like factors of five or 10 but probably not beyond that because >> often what happens is if you design a system for X and something suddenly becomes 100X that would enable a very different point in the design space that would not make sense at X but all of a sudden 100x makes total sense. So like going from a disk spaced index to a in-memory index makes a lot of sense once you have enough traffic because now you have enough replicas of the sort of state on disk that those machines now</p>
<p>actually can hold uh you know a full copy of the me uh index in memory. >> Yeah. >> And that all of a sudden enables a completely different design that wouldn't have been practical before. >> Yeah. Um, so I'm I'm a big fan of thinking through designs in your head, just kind of playing with the design space a little before you actually do a lot of writing of code. But you know, as you said, in the early days of Google, we were you growing the index uh quite extensively. We were growing the update</p>
<p>rate of the index. So the update rate actually is the parameter that changed the most surprisingly. So it used to be once a month. >> Yeah. And then we went to a system that could update any particular page in like sub one minute. >> Okay. Yeah. Because this is a competitive advantage, right? >> Because all of a sudden news related queries, you know, if you're if you've got last month's news index, it's not actually that useful for >> a special beast. Was there any like you could have split it onto a separate system? >> Well, we did we launched a Google News</p>
<p>product, but you also want news related queries that people type into the main index to also be >> sort of updated. So, >> yeah. Yeah. It's interesting. And then you have to like classify whether the page is you have to decide which pages should be updated at what frequency. >> Oh yeah, there's a whole like uh system behind the scenes that's trying to decide update rates and importance of the pages. So even if the update rate seems low, you might still want to rec crawl important pages quite often because >> uh the likelihood they change might be low but the value of having them updated</p>
<p>is high. >> Yeah. Yeah. Yeah. Yeah. uh what you know this uh you know mention of latency and and saving things to this reminds me of one of your classics which I have to bring up which is latency numbers every programmer should know. >> Uh >> was there was there just a just general story behind that did you just write it down? >> I mean this has like sort of eight or 10 different kinds of metrics that are like how long does a cache miss take, how</p>
<p><strong>[30:12]</strong></p>
<p>long does branch miss predict take, how long does a reference domain memory take, how long does a distance take these >> how long does it take to send you know a packet from the US to the Netherlands or something. Um, >> why Netherlands by the way or is it is that because of Chrome? >> Uh, we had a data center in [laughter] >> um so I mean I think this gets to the point of being able to do these back at the envelope calculations. So these are sort of the raw ingredients of those and you can use them to say okay well if I need to design a system to do image search and thumbnailing or something of</p>
<p>the result page you know how might I do that? I could premp compute the image thumbnails. I could like try to thumbnail them on the fly from the larger images. What would that do? How much dis bandwidth I need? How many disc seeks would I do? Um and you can sort of actually do thought experiments in you know 30 seconds or a minute with the sort of uh basic uh basic numbers at your fingertips. Uh and then as you sort of build software using higher level libraries, you kind of want to develop</p>
<p>the same intuitions for how long does it take to you know look up something in this particular kind of hash table I use or you know how long will it take me to sort a million numbers or something. >> Yeah. The the reason I bring it up actually is actually for I think like two years now I've been trying to make numbers every AI programmer should know. >> Okay. Yeah. >> Uh I don't have a great one. uh because it's not as it's not physical constants like you have physical constants in here you know it's and >> uh but I do think like uh so a simple one would be number of parameters to um</p>
<p>uh disk size if you if you need to convert that uh which is a simple bite conversion that's not that's nothing interesting I wonder if you have any if you want if you if you were to update your >> I mean I think uh it's really good to think about uh calculations you're doing in a model either for training or inference. Um, often a good way to view that is how much uh state will you need to bring in from memory either like onchip SRAMM or</p>
<p>HPM from the accelerator attached uh memory or DRAM or over the network. Um and then how expensive is that data motion relative to uh the cost of say an actual multiply in the matrix multiply unit >> and that cost is actually really really low right because it's you know order you know uh depending on your precision I think it's like sub pico one picole >> oh okay you measure it by energy</p>
<p>>> yeah yeah I mean it's all going to be about energy and how do you make the most energy efficient Um, and then moving data from the SRAMM on the other side of the chip, not not even off the off chip, but on the other side of the same chip can be, you know, a thousand pajles. >> Oh. >> Or Yeah. And so all of a sudden this is</p>
<p><strong>[33:14]</strong></p>
<p>why your accelerators uh require batching because if you move like say the parameter of a model from SRAMM on the on the chip into the multiplier unit that's going to cost you a thousand pico tools. So you better make use of that that thing that you moved many many times with. So that's where the batch dimension comes in because all of a sudden, you know, if you have a batch of 256 or something, that's not so bad. But if you have a batch of one, that's really not good. >> Yeah. Yeah. >> Right. Because then you paid a thousand podles in order to do your one pico multiply.</p>
<p>>> I have never heard a energy based analysis of batching. [laughter] >> Yeah. I mean, that's why people batch, right? Yeah, ideally you'd like to use batch size one because the latency would be great >> but the energy cost and the the compute cost inefficiency that you get um is is quite large. So >> yeah is there a similar trick like uh like like you did with uh you know putting everything in memory like you know I think uh obviously Nvidia has caused a lot of waves with uh betting very hard on on SRAMM with grock. Uh I I I wonder if like that's something that</p>
<p>you already saw with with the TPUs, right? Like that that you had to uh to serve at your scale. Uh you probably sort of saw that coming like what what what hardware uh innovations or insights were formed because of what you're seeing there. >> Yeah. I mean, I think you know, TPUs have this nice uh sort of regular structure of 2D or 3D meshes with a bunch of chips connected and each one of those has HPM attached. Um I think for serving some kinds of models,</p>
<p>>> uh you know, you you pay a lot higher cost and time latency um bringing things in from HBM than you do bringing them in from uh SRAMM on the chip. So if you have a small enough model, you can actually do model parallelism, spread it out over lots of chips, and you actually get quite good throughput improvements and latency improvements from doing that. And so you're now sort of striping your smalish scale model over say 16 or 64 chips. Uh</p>
<p>but if if you do that and it all fits in SRAMM, uh that can be a big win. So yeah, that's not a surprise, but it is a good technique. >> Yeah. What about the TPU? design like how much do you decide where the improvements have to go? So like this is like a good example of like is there a way to bring the thousand pig jewel down jewels [clears throat] down to 50 and like is it worth designing a new chip to do that? The extreme is like when people say oh you should burn the model on the ASIC and that's kind of like the most</p>
<p>extreme thing. >> How much of it is it worth doing in hardware when things change so quickly? Like what what's the internal discussion? Yeah, I mean we we have a lot of interaction between say the TPU chip design architecture team and the sort of higher level modeling uh experts because we really want to take advantage of being able to co-design what should future TPUs look like based on where we think the sort of ML research puck is</p>
<p><strong>[36:16]</strong></p>
<p>going uh in some sense because uh you know as a hardware designer for ML in particular you're trying to design a chip starting today and that design might take two years before it even lands in a data center and then it has to sort of be a reasonable lifetime of the chip to take you three, four or five years. So you're trying to predict two to six years out where what ML computations will people want to run two to six years out in a very fast changing field. And so having people with</p>
<p>interesting ML research ideas of things we think will start to work in that time frame or will be more important in that time frame. Uh really enables us to then get you know interesting hardware features put into you know TPU N plus2 where TPUn is what we have today. >> Oh the cycle time is plus two >> roughly. I mean >> because uh >> I mean sometimes you can squeeze some changes into N plus1 but you know bigger</p>
<p>changes are going to require the chip design be earlier in its lifetime design process. Um, so whenever we can do that, it's generally good. And sometimes you can put in speculative features that maybe won't cost you much chip area, but if it works out, it would make something, you know, 10 times as fast. And if it doesn't work out, well, you burned a little bit of tiny amount of your chip area on that thing, but it's not that big a deal. Uh, sometimes it's a very big change and we want to be pretty sure this is going to work out.</p>
<p>So we'll do like lots of careful ML experimentation to show us uh this is actually the the way we want to go. >> Yeah. >> Is there a reverse of like we already committed to this chip design so we cannot take the model architecture that way because it doesn't quite fit? >> Yeah. Yeah, I mean you you definitely have things where you're going to adapt what the model architecture looks like so that they're efficient on the chips that you're going to have for both training and inference of that of that uh generation of model. So I think it</p>
<p>kind of goes both ways. Um you know sometimes you can take advantage of you know lower precision things that are coming in a future generation. So you might train it at that lower precision even if the current generation doesn't quite uh do that. >> Mhm. Yeah. How low can we go in precision? >> People are saying like turner is like [laughter] >> Yeah. I mean I'm a big fan of very low precision because I think that gets that saves you a tremendous amount of energy, right? Because it's poujles per bit that</p>
<p>you're transferring and reducing the number of bits is a really good way to >> to reduce that. Um, you know, I think people have gotten a lot of luck, uh, mileage out of having very low bit precision things, but then having scaling vectors that apply to a whole bunch of, uh, those those weights >> scaling. Okay. Interesting. You so low</p>
<p><strong>[39:16]</strong></p>
<p>precision but scaled up weights. >> Yeah. >> Huh. Yeah. Never considered that. Interesting. Uh while we're on this topic, you know, I think there's a lot of um uh just the concept of precision at all is weird when we're sampling, you know, uh we just at the end of this we're going to have all these like chips that all do like very good math and then we're just going to throw a random number generator at the start and [laughter] >> so I mean I there's a movement towards energy based uh models and pro processors. I'm just curious if you've obviously you've thought about it but</p>
<p>like what's your commentary? Yeah, I mean I think there's a bunch of interesting trends. So energy based models is one. You know, diffusion based models which don't sort of sequentially decode tokens is another. >> Yes. Um, you know, speculative decoding is a way that you can get sort of an equivalent very small >> draft >> batch factor uh for like you predict eight tokens out and that enables you to sort of increase the effective batch size of what you're doing by a factor of eight even and then you maybe accept five or six of those tokens. So you get</p>
<p>five a 5x improvement in the amortization of moving weights uh into the multipliers to do the prediction for the the tokens. So these are all really good techniques and I think it's really good to look at them from the lens of uh energy real energy not energy based models um and and also latency and throughput right if you look at things from that lens that sort of guides you to solutions that are going to be uh you</p>
<p>know better from uh you know being able to serve larger models or you know equivalent size models more cheaply and with lower latency. >> Yeah. Well, I think I think I um it's appealing intellectually. Uh haven't seen it like really hit the mainstream, but um I do think that uh there's some poetry in the sense that uh you know, we don't have to do uh a lot of shenanigans if like we fundamentally design it into the hardware.</p>
<p>>> Yeah. Yeah. I mean, I think there's still a there's also sort of the more exotic things like analog based uh uh computing substrates as opposed to digital ones. Uh I'm, you know, I think those are super interesting because they can be potentially low power. >> Uh but I think you often end up wanting to interface that with digital systems and you end up losing a lot of the power advantages in the digital to analog and analog to digital conversions you end up doing >> uh at the sort of boundaries and periphery of that system. M >> um I still think there's a tremendous</p>
<p>distance we can go from where we are today in terms of energy efficiency with sort of uh much better and specialized hardware for the models we care about. >> Yeah. >> Um any other interesting research ideas that you've seen or like maybe things that you cannot pursue at Google that you would be interested in seeing researchers take a stab at? I guess you</p>
<p><strong>[42:17]</strong></p>
<p>have a lot of researchers. Yeah, we have a lot of our our research portfolio is [laughter] pretty broad. I would say um I mean I think [snorts] uh in terms of research directions, there's a whole bunch of uh you know open problems and how do you make these models reliable and able to do much longer kind of uh more complex tasks that have lots of subtasks? How do you orchestrate you know maybe one model that's using other models as tools in order to sort of build uh things that can accomplish uh you know much more significant pieces of</p>
<p>work uh collectively than you would ask a single model to do. Um so that's super interesting. How do you get more verifiable uh you know how do you get RL to work for non-verifiable domains? I think it's a pretty interesting open problem because I think that would broaden out the capabilities of the models, the improvements that you're seeing in both math and coding. Uh if we could apply those to other less verifiable domains because we've come up with RL techniques that actually enable us to do that uh effectively that would</p>
<p>that would really make the models improve quite a lot. I think >> I'm curious like when we had no brown on the podcast, he said um they already proved you can do it with deep research. Mhm. >> Um, you kind of have it with AI mode in a way. It's not verifiable. >> I'm curious if there's any thread that you think is interesting there. Like what is it? Both are like information retrieval of JSON. So I wonder if it's like the retrieval is like the verifiable part that you can score or what are like yeah how how would you</p>
<p>model that that problem? Yeah, I mean I think there are ways of having other models that can evaluate the results of what a first model did. Maybe in retrieving, can you have another model that says, is this things are these things you retrieved relevant or can you rate these 2,00 things you retrieved to assess which ones are the 50 most relevant or something. Um, I think those kinds of techniques are actually quite effective. Sometimes that can even be the same model just prompted differently to be a you know critic as</p>
<p>opposed to a uh actual retrieval system. >> Yeah. Um, I do think like there there is that that weird cliff where like it feels like we've done the easy stuff and then now it's but it always feels like that like every year [clears throat] it's like oh like we know you know and the next part is super hard and nobody's figured it out and >> uh like exactly with this RLVR thing where like everyone's talking about well okay how do we do the next stage of the non-verifiable stuff and everyone's like I don't know you know judge [laughter]</p>
<p>>> I mean I feel like The nice thing about this field is there's lots and lots of smart people thinking about creative solutions to some of the, you know, problems that we all see. Uh because I think everyone sort of sees that the models, you know, are great at some things and they fall down around the edges of those things and and are not as capable as we'd like in those areas. And then coming up with good techniques and</p>
<p><strong>[45:20]</strong></p>
<p>trying those and seeing which ones actually make a difference is sort of what the whole research aspect of this field is is pushing forward. And I think that's why it's super interesting. You know, if you think back two years ago, we were struggling with GSM8K problems, right? Like, you know, Fred has two rabbits, he gets three more rabbits. How many rabbits does he have? >> That's a pretty far cry from the kinds of mathematics that the models can. >> And now you're doing Yeah. And erosure language. Yeah. >> Yeah. Pure language. So that is a really really amazing jump in</p>
<p>capabilities in you know a year and a half or something. And I think um for other areas it'd be great if we could make that kind of leap. Uh and you know we don't exactly see how to do it for some some areas but we do see it for some other areas and we're going to work hard on making that better. >> Yeah. >> Yeah. Like YouTube thumbnail generation that would be very helpful. >> We need that. That would be AGI. we need for as far as content creators go. >> I guess I'm not a YouTube creator, so I don't care that much about that problem,</p>
<p>but I guess uh many people do. >> It Yeah, it doesn't it doesn't matter. People do judge books by their covers as it turns out. >> Um just to draw a bit on the IMO gold. Um I'm still not over the fact that a year ago we had Alpha Proof and Alpha Geometry and all those things and then this year we were like screw that, we'll just chuck it into Gemini. What's your reflection? Like I think this this question about like the merger of like symbolic systems and like and and LLMs uh was a very much core belief and then</p>
<p>somewhere along the line people just said nope we'll just all do it in LLM. >> Yeah. I mean, I think it makes a lot of sense to me because, you know, humans manipulate symbols, but we probably don't have like a symbolic representation in our heads, >> right? We have some distributed representation that is neural netlike in some way of lots of different neurons and activation patterns firing when we see certain things and that enables us to reason and plan and, you know, do</p>
<p>chains of thought and, you know, roll them back. you know that that approach for solving the problem doesn't seem like it's going to work. I'm going to try this one. And you know, in a lot of ways, we're emulating what we intuitively think uh is happening inside real brains in neural netbased models. So it never made sense to me to have like completely separate discrete uh symbolic things and then a completely different way of of uh you know thinking about those things.</p>
<p>>> Interesting. Yeah. Uh I mean it's maybe seems obvious to you but it wasn't obvious to me a year ago. [laughter] >> Yeah. I mean I do think like that >> IMO with you know translating to lean and using lean and then the next year and and also a specialized geometry model and then this year switching to a single unified model that is roughly the</p>
<p><strong>[48:22]</strong></p>
<p>production model with a little bit more inference budget uh is actually you know quite good because it shows you that the capabilities of that general model yeah have improved dramatically and and now you don't need these specialized models. This is actually sort of very similar to the 2013 to6 era of machine learning, right? Like it used to be people would train separate models for lots of different each different problem, right? I have I want to recognize street signs in something. So I train a street sign recogn recognition model or I want to you know decode speech recognition. I</p>
<p>have a speech model. Right? I think now the era of unified models that do everything is really upon us and the question is how well do those models generalize to new things they've never been asked to do and they're getting better and better >> and you don't need domain experts like one of my uh so I interviewed Eay who was on who's on that team >> uh and he was like yeah I I don't know how they work I don't know where the IMO competition was held I don't know the rules of it I just train the models I'm good at training [laughter] models >> and it's kind of interesting thing that</p>
<p>like people with these this like universal skill set of just like machine learning you just give them data and give them enough compute and they can kind of tackle any task which is >> yeah right [laughter] and >> a bitter lesson I guess I don't know >> yeah yeah I mean I think uh general models uh will win out over specialized ones in most cases >> so I want to push there a bit I think there's one hole here which is like uh there's this concept of like uh maybe capacity of a model like abstractly a model can only contain [clears throat] the number of bits that it has and uh</p>
<p>and so you know god knows like Gemini Pro is like one to 10 trillion parameters we don't know but uh the Gemma models for example right like a lot of people want like the open source local models that are like that that that and and uh they have some knowledge which is not necessary right like they can't know everything like like you have the luxury of you have the big model and big model should be able to capable of everything but like when when you're distilling and you're going down to the small models,</p>
<p>you know, you're actually memorizing things that are not useful >> and so like how do we I guess do we want to extract that? Can we can we divorce knowledge from reasoning, you know? >> Yeah. I mean, I think you do want the model to be most effective at reasoning if it can retrieve things, right? having the model devote precious parameter space to remember obscure facts that could be looked up >> is actually not the best use of that parameter space right like you might prefer something that is more generally</p>
<p>useful in more settings than this obscure fact that it has um so I think that's always a tension at the same time you also don't want your model to be kind of completely detached from you know knowing stuff about the world right like it's probably useful to know how long the Golden Gate Bridge is just as a general sense of like how long are</p>
<p><strong>[51:22]</strong></p>
<p>bridges, right? And uh it should have that kind of knowledge. It maybe doesn't need to know how long some teeny little bridge in some other more obscure part of the world is, but uh it does help it to have a fair bit of world knowledge. And the bigger your model is, the more you can have. Uh but I do think combining retrieval with sort of reasoning and making the model really good at doing multiple stages of retrieval and reasoning through the intermediate retrieval results is going to be a a pretty effective way of making</p>
<p>the models seem much more capable because if you think about say a personal Gemini >> Yeah. Right? Like we're not going to train Gemini on my email. Probably we'd rather have a single model that uh we can then use and use being able to retrieve from my email as a tool and have the model reason about it and retrieve from my photos or whatever. Uh and then make use of that and have multiple u you know stages of interaction. >> That makes sense. Do you think the</p>
<p>vertical models are like [clears throat] an interesting pursuit? Like when people are like, "Oh, we're building the best healthcare LLM. We're building the best law LLM." Are those kind of like short-term stop caps or >> No, I mean I think I think vertical models are interesting like you want them to start from a pretty good base model, but then you can sort of I sort of viewing them view them as enriching the data distribution for that particular vertical domain for healthcare. say um we're probably not going to train or for say robotics,</p>
<p>we're probably not going to train Gemini on all possible robotics data. We you could train it on because we wanted to have a balanced set of capabilities. Um, so we'll expose it to some robotics data, but if you're trying to build a really, really good robotics model, you're going to want to start with that and then train it on more robotics data and then maybe that would hurt its multilingual translation capability but improve its robotics capabilities. And we're always making these kind of uh, you know, tradeoffs in the data mix that</p>
<p>we train the base Gemini models on. You know, we'd love to include data from 200 more languages and as much data as we have for those languages. >> Yeah. >> But that's going to displace some other capabilities of the model. >> It won't be as good at um you know, Pearl programming. you know, it'll still be good at Python programming because we'll include enough of that, but there's other longtail computer languages or coding capabilities that it may suffer on or multi- uh multimodal reasoning capabilities may suffer</p>
<p>because we didn't get to expose it to as much data there, but it's really good at multilingual things. So, I I think some combination of specialized models, maybe more modular models. So it'd be nice to have the capability to have those 200 languages plus this awesome robotics model plus this awesome healthcare uh module that all can be knitted together</p>
<p><strong>[54:22]</strong></p>
<p>to work in concert and called upon in different circumstances, right? Like if I have a health related thing, then it should enable using this health module in conjunction with the main base model to be even better at those kinds of things. >> Yeah. Installable knowledge. Yeah. Right. just [clears throat] download as a as a >> and some of that installable stuff can come from retrieval, >> but some of it probably should come from training on you know uh 100 billion tokens or a trillion tokens of health data. >> Yeah. And for listeners I think uh I will highlight the GEMA 3 end paper</p>
<p>where they there was a little bit of that I think. >> Yeah. >> Yeah. I guess the question is like how many billions of tokens do you need to outpace the frontier model >> improvements? You know, it's like if I have to make this model better at healthcare and the main Gemini model is still improving, >> do I need 50 billion tokens? Can I do it with 100? If I need a trillion healthcare tokens, it's like they're probably not out there that you don't have, you know, I think that's really like the challenge. >> Oh, I mean, I think healthcare is a</p>
<p>particularly challenging domain. So there's a lot of healthcare data that you know we don't have access to appropriately but there's a lot of you know uh healthcare organizations that want to train models on their own data that is not public healthcare data uh not public health but public healthare data. Um, so I think there are opportunities there to say partner with a large healthcare organization and train models for their use that are going to be, you know, more bespoke but probably uh might be better than a</p>
<p>general model trained on say public data. >> Yeah. >> Yeah. I I believe uh by the way al this is like somewhat related to the language conversation. Uh I think one of your your favorite examples was you can put a low resource language in the context and it just learns [laughter] in context. >> Oh yeah. I think the example we used was Calamong which is truly low resource because it's only spoken by I think 120 people in the world and there's no written text. >> So >> so you can just do it that way just to get in the context. >> Yeah. [laughter]</p>
<p>>> Yeah. But I put your whole data set in context, right? >> If you if you take a language like uh you know Somali or something there is a fair bit of Somali text in the world that uh or Ethiopian Amharic or something um you know we probably are not putting all the data from those languages into the Gemini based training. We put some of it but if you put more of it you'll improve the capabilities of those models. >> Yeah. >> So or of those languages. >> Uh yeah cool. Uh it's uh the I I have a side interest in linguistics. I I I I</p>
<p>did a few classes in back in college and like uh part of me like if I was a linguist and I could have access to all these models I would just be asking really fundamental questions about language itself like uh one is there's one very obvious one which is superior warf like how much does like the language that you speak affect your thinking but then also there are some languages where there's just concepts that are not represented in other languages but some others many others</p>
<p><strong>[57:22]</strong></p>
<p>that are just duplicates right where uh there's also another paper that people love called the platonic representation where you know like the the an image of a cup is uh if you say learn a model on that and you you you have a lot of text with the word cup it eventually maps to like roughly the same place in laten space and so like that should apply to languages except where it doesn't and that's actually like very interesting differences in what humanity has discovered as concepts that maybe English doesn't have. [laughter]</p>
<p>>> I I don't know. That's just like my my rant on languages. >> Yeah, I I did some work on a early model that fused together a languagebased model with you have, you know, nice word-based representations and then an image model where you have trained it on imageet like things. Yes. >> And then you fuse together the top layers of >> uh no this is device >> device. >> Uh the you do a little bit more training to fuse together those representations. And what you found was that if you give a novel image that is not in any of the</p>
<p>categories in the image model it was trained on the model can often assign kind of the right c the right label to that image. Um so for example um I think uh telescope and uh binoculars were both in the training uh categories for the image model but um microscope was not. M. >> And so if you give it an image of a microscope, it actually can come up with something that's got the word microscope as the label that are designed even though it's never actually seen an image</p>
<p>labeled that. >> Oh, that's nice. >> Yeah. >> Um, so yeah, >> useful. Uh, cool. I think there there's more general like broad questions, but like I guess what what do you uh wish you were asked more in in in general? Like you know you have such a broad scope. We've covered the hardware. covered the the models research. >> Yeah, I mean I think uh one thing that's kind of interesting is you know I I did a undergrad thesis on neural network uh training uh parallel</p>
<p>neural network training uh back in 1990 when I got exposed to to neural nets and I always felt kind of they were the right abstraction but we just needed way more compute than we had then. So like the 32 processors in the department parallel computer you know could get you a a little bit more interesting uh model but not not enough to solve real problems. And so starting in 2008 or nine, you know, the world started to have enough computing power through Moors law and, you know, larger interesting data sets to train on to</p>
<p>actually, you know, start training neural nets that could tackle real problems that people cared about, speech recognition, vision, and eventually language. Um and so um when I started working on neural nets at Google in in late 2011 um you know I really just felt like we should scale up the size of neural</p>
<p><strong>[60:24]</strong></p>
<p>networks we can train using you know large amounts of parallel computation and so I actually revived some ideas for my undergrad thesis where I'd done both model parallel and data parallel uh training and I compared them. >> I called them something different. It was like pattern partitioned and you know model partitioned or something. >> We'll have to Is it is it public? Can we go dig? >> Yeah, it's on it's on the web. >> Okay. >> Um but uh you know I think combining a lot of those techniques and really just trying to push on scaling things up over the last you know 15 years has been you</p>
<p>know really important and that means you know improvements in the hardware. So you know pushing on building specialized hardware like TPUs. Uh it also means you know pushing on software abstraction layers to let people express ML ideas uh effectively. Um and then also working on things like uh say sparse models. I've felt for a long time that sort of sparsely activated models are a really important thing because you want the models to have a lot of capacity to our</p>
<p>earlier discussion about remembering a lot of stuff. >> Yeah. >> But you also want to be super efficient in how you activate your models. So you'd like you know trillions of parameters but activate only you know one 1% or 5% or 10% of that and um that you know we did a early u paper on this where we really scaled up uh you know outrageously large neural networks that the title I think that's Nome's uh Gnome's wording in the title which is a</p>
<p>good catchy title. >> I mean in 2017 he was out there talking about one trillion parameter models. >> Yeah. So I mean that that that is really good because that gave you like a 10x improvement in you know time to quality or compute cost to qual a given quality level relative to non-sparse models. Um transformers similarly gave you a 10x to 100x improvement in you know uh compute cost to a given quality level uh versus say LSTMs at the time and all of those things multiply together. Um so I think</p>
<p>all those things really are important to work on you know the hardware the systems infrastructure the you know algorithmic aspects of model architecture improving the data you know improving the RL recipes all these things uh are what are stacking together and multiplying together >> to give us models of 2026 are much more better than models of 25 and are awesomely better than 24 and 23 and</p>
<p>and and a huge uh honestly like organizational challenge like there's like a thousand people or maybe more like I know I know when the first Gemini paper came out it was like a thousand of co-authors. >> Yeah. Yeah. We have uh 10 pages of co-authors in the in the tech [laughter] report >> but it was nice. I mean you know people want to be acknowledged on probably a historical paper. >> Yeah. I mean, I think it's perfectly good to have actually a lot of co-authors and I do think >> organizing that number of people so that</p>
<p><strong>[63:25]</strong></p>
<p>they're effectively pushing in common directions that all all their work actually sort of multiplies together in the ultimate output which is you know the next generation of model is actually pretty tricky and we have awesome people uh throughout the Gemini team to help orchestrate this. So you know myself, Noom and Oral are sort of helping steer this and then we have people thinking about you know what is the pre-training uh setup look like what does the infrastructure look like what does the post-raining recipe look like and what</p>
<p>does the data preparation and eval multimodal capabilities and IN capabilities >> um you know there's a lot of different kinds of areas coding capabilities all these areas are are super important and it's really good to have people uh paying close attention to those things and then also paying close attention to all the other things. >> Yeah. I'm told Sergey is like very actively back and like very much involved in coding stuff. >> Yep. Yeah. Yeah. Yeah. We all use the same micro kitchen.</p>
<p>>> Yeah. Uh oh. Okay. Like there's so many jumping off point. Uh so by the way I found out from the recent uh I mean you've probably told this story a few times but apparently Google brain was also started in a micro kitchen. >> Yeah. Yeah. [laughter] >> Just like your micro kitchens are very important. >> Yeah. I don't know if people like understand. >> Yeah. Uh yeah, I actually bumped into Andrew Ing who's a Stanford faculty member and uh I knew him from I'd given talks at Stanford a couple years before so I sort of knew him and I'm like, "Oh, what are you doing here?" He's like, "Oh, I'm not sure yet. I just started,</p>
<p>you know, a couple weeks ago. I'm going to spend one day a week here consulting. Um I'm not sure what I'm working on, but my students at Stanford are starting to get good results um on using u neural nets for speech uh recognition. I'm like, "Oh, neural nets. I like neural nets." Like I remembered back to my 1990 thesis. I'm like, "Oh, that sounds interesting. We should train really really big neural nets." >> So [snorts] that was the >> which you say that and that's a very interesting first instinct, which is that we should scale this up a lot. >> Yeah. Well, I mean, I felt like</p>
<p>Google is is has lots of computational uh capability and so if they were seeing good results on, you know, what were effectively single GPU or uh models, >> you know, if we were uh we actually didn't have GPUs in our data centers then we didn't have any accelerators. We had lots of CPUs, but you know, we could build a software system that would enable you to distribute with both model parallelism and data parallelism across lots of computers. And we ended up</p>
<p>training a pretty big model was 50x bigger than any previous neural net as far as we could tell. Um, so it's two billion parameters uh vision model uh trained on 16,000 CPU cores for like multiple weeks. Uh and that's what gave us really good it would gave us a 70% relative error improvement in imageet 22k which is the 22,000 category thing and that's how we really saw okay</p>
<p><strong>[66:27]</strong></p>
<p>scaling this up actually matters. We didn't write a, you know, a sophisticated scaling analysis, but we had a a saying, bigger model, more data, better results. >> And that was our our mantra for like six or seven years of scaling. And we every time we did that, we saw better results in speech, in language, in in vision. >> Uh, speaking of um bets, and this might and this, you know, I'll preface with like this might be a little bit more sensitive topic, but you have obviously a lot of opinions about this. We had a previous guest, David Juan, who used to</p>
<p>work for you, and uh he he kind of like blames almost the brain marketplace as like the reason that Google didn't invest enough in language models. And I wonder if that's uh something you would you would agree with at the time or uh is there like a different sort of postmortm >> the brain marketplace for computers >> compute quotas where basically he was like okay the like >> David worked at OpenAI as VP engine then he worked at Google he was like fundamentally open was willing to go all in like bet the farm on one thing</p>
<p>whereas Google was more democratic like everyone had a >> had a quota and I was like okay like like if if you believe in scaling as an important thing that's a that's an important organizationalwide decision to do. >> Yeah. Uh yeah, I mean I think uh I would somewhat agree with that. I mean I think I actually wrote a one-page memo saying we were being stupid by uh fragmenting our resources. >> Um so in particular at the time we had uh you know uh efforts within Google</p>
<p>research on uh and and in the brain team in particular on large language models. We also had efforts on multimodal models um in uh other parts of brain and and Google research and then legacy deep mind had uh efforts like um chinchilla models and uh flamingo models. Uh and so really we were fragmenting not only our compute uh across those separate efforts but also our best people and our best</p>
<p>ideas, right? And so I said this is just stupid. Why don't we combine things and have one effort to uh train >> and this is the merge. Yeah. >> To train an awesome single unified model that is multimodal from the start that's good at everything and that was the origin of the Gemini effort and my one page memo worked which is good. >> Did you have the name because also for those who don't know you named Gemini. >> I did. Yeah. [laughter] Yeah. There was there was another name proposed and I I said, you know, it's sort of like these</p>
<p>two organizations really are like uh twins >> in some sense coming together. Um so I kind of like that. And then there's also the NASA interpretation of you know the early Gemini project >> uh being an important thing on your way to um you know the Apollo project. So it seemed like a good name. Twins coming together,</p>
<p><strong>[69:27]</strong></p>
<p>>> right? Yeah. Nice. Um, I know we're already running out of time, but I'm curious how you use AI today to code. So, I mean, you're probably one of the most prolific engineers in the history of computer science. Um, I was reading on through the article about you and Sanji's friendship and how you work together [clears throat] and >> you have one quote about you need to find someone that you're going to pair program with who's compatible with your way of thinking so that the two of you together are a complimentary force. Mhm. >> And I was thinking about how you think about coding agents in this like how do you shape</p>
<p>>> a coding agents to be compatible with your way of thinking like h how would you rate the tools today? Like where should things go? >> Yeah. I mean first I think the coding tools are you know getting vastly better compared to where they were a year or two two years ago. So now you can actually rely on them to do more complex things that you as a as a software engineer want to accomplish and you can sort of delegate you know pretty complex things to these tools. And I think one really nice aspect about the uh interaction between a a human uh</p>
<p>software engineer and a a coding model that they're working with is your way of talking to that uh coding model actually sort of uh dictates how it interacts with you, right? Like you could ask it please write a bunch of good tests for this. You could ask it, please help me brainstorm performance ideas. And your way of doing that is going to shape how the model responds, what kinds of problems it tackles. You know, how much</p>
<p>do you want the model to go off and do things that are larger and more independent versus interact with it more to make sure that you're shaping the right kinds of of things? And I think it's not the case that any one style is the right thing for everything, right? like some kinds of problems you actually want uh maybe a more frequent interaction style with the model and other ones you're just like, "Yeah, please just go write this cuz I I know I need this thing. I can specify it well enough." Um and go off and do it and come back when you're done. And so I do</p>
<p>think there's going to be more of a style of having lots of independent uh software agents off doing things on your behalf and figuring out the right sort of human computer interaction model and UI and so on for when should it interrupt you and say hey I need a little more guidance here or I've done this thing now what now what should I do? Um I think we we're not at the end all answer to that question and as the models get better that uh set of decisions you put into how the interaction should happen may may change</p>
<p>right like if you if you have a team of 50 interns how would you manage that if they were people and I think it's not >> do you want 50 interns [laughter] >> you might if they're really good right >> it's a lot of management >> but but it's a lot of Uh yeah, I mean I think that is probably</p>
<p><strong>[72:29]</strong></p>
<p>within the realm of possibilities that lots of people could have 50 interns >> and so how would you actually deal with that as a person, right? Like you would probably want them to form small sub teams so you don't have to interact with 50 of them. You could interact with five of five of those teams and they're off doing things on your behalf. But I don't know exactly what the how this is going to unfold. >> Yeah. How do you think about bringing people like the pair programming is always helpful to like get net new ideas in the distribution so to speak? It</p>
<p>feels as we have more of these coding agents write the code. It's hard to bring other people into the problem. Say you go to like you know you have 50 interns [clears throat] right and then you want to go to nom shazir be like hey nom I want to like pair on this thing >> but now there's like this huge amount of work that has been done in parallel that you need to catch him up on >> right >> and I'm curious like if people are going to be in a way more isolated in their teams where it's like okay there's so much context in these 50 interns that it's just hard for me to like relay everything back to you</p>
<p>>> maybe I mean on the other hand like imagine a classical software or organization without any AI assisted tools, right? You would have, you know, 50 people doing stuff and their interaction style is going to be naturally very hierarchical because, you know, these 50 people are going to be working on this part of the system and not interact that much with these other people over here. But if you have, you know, five people each managing 50 virtual agents, you know, they might be</p>
<p>able to actually have much higher bandwidth communication among the five people uh than you would have among five people who are also trying to coordinate, you know, a 50 person software team each. Yeah. So, >> how do you I'm curious how you change your just working rhythm, you know, like do you spend more time ahead with people going through specs and design goals like >> um I mean I do think it's interesting that you know whenever people were taught how to write software they were taught that it's really important to</p>
<p>write specifications super clearly. But no one really believed that. Like it was like yeah whatever I don't need to do that I'm going to [laughter] >> really >> I don't know. I mean, writing the English the English language specification was never kind of an artifact that was really paid a lot of attention to. I mean, it was important, but it wasn't sort of the thing that drove the actual creative process quite as much as if you specify what software you want the agent to write for you, you'd better be pretty darn careful in how you specify that</p>
<p>because that's going to dictate the quality of the output, right? like if you if you don't cover that it needs to handle this kind of thing or that this is a super important corner case or that you know you really care about the performance of this part of it you know it may uh not do what you want and the better you get at interacting with these models and and I think one of the ways</p>
<p><strong>[75:30]</strong></p>
<p>people will get better is they will get really good at crisply specifying things rather than leaving things to ambiguity >> and that is actually probably not a bad It's not a bad skill to have regardless of whether you're a software engineer or a you know trying to do some other kind of uh task. You know, being able to crisply specify what it is you want. It's going to be really important. >> Yeah. My joke is um you know, good prompting [clears throat] is in uh indistinguishable from sufficially advanced executive communication. Like it's like writing an internal memo. Like</p>
<p>>> Yeah. Yeah. >> Weigh your words very carefully. And also I think very important to be multimodal, right? I think one thing that anti-gravity from from Google also did was like just come out the gate very very strong multimodal including videos and that's the highest bandwidth communication prompt that you can give the >> the model which is fantastic. >> Yeah. >> How do you collect things that you often you would have in your mind. So you have this amazing like performance hints thing that you wrote about how to look for performance improvements and is there a lot more value in like people writing these like generic things down</p>
<p>so that they can then put them back as like potential retrieval artifacts for the model like or do I have like the edge cases is like a good example right it's like [snorts] if you're building systems you already have in your mind specific edge cases depending on it but now you have to like every time repeat it >> like are you having people spend a lot more I'm writing out more generic things to bring back or >> um I mean [snorts] I do think [clears throat] well-written guides of of how to do good software engineering are going to be useful because they can</p>
<p>be used as input to models or you know read by other developers so that their prompts are you know more clear about what the the underlying software system should should be doing. Um, you know, I think it may not be that you need to create a custom one for every situation. If you have general guides and put those into, you know, the context of a coding agent that that can be helpful like in you can imagine one for distributed</p>
<p>systems. You could say, okay, think about failures of these kinds of things and these are some techniques you can deal with failures. you know, you can have uh, you know, Paxos like replication or, you know, you can, uh, send the request to two places and tolerate failure because you only need one of them to come back. You know, a little description of 20 techniques like that in building distributed systems probably would go a long way to having a coding agent be able to sort of cobble up more reliable and robust distributed systems.</p>
<p>>> Yeah. Yeah. [clears throat] Wonder when Gemini will be able to build spanner, >> right? Probably already has the code inside, you know. [laughter] >> Yeah, that I mean that's a good example, right? When you have like you know the cap theorem and it's like well this is like truth and you cannot break that and then you build something that broke it. Like I'm curious like models in a way are like [clears throat] what did he say he broke it? Would you say you broke cat</p>
<p><strong>[78:31]</strong></p>
<p>theorem? >> Really? Yeah. Okay. All right. >> I mean [laughter] >> under local assumptions. Yeah. And some and they're like, you know, good clocks. >> Yeah. [laughter] It's like some sometimes you don't have to like always follow what is known to be true. And I I think models in a way like if you tell them something, they like really buy into that, you know. Um >> so yeah, just more thinking than any answer on how to fix that. Yeah, my my uh you know just on this like like big prompting and and uh iteration you know I think that coming back to your latency</p>
<p>point um I always I always trying to one one AB test or experiment or benchmark or research I would like is what is the uh performance difference between let's say three dumb fast model calls with human alignment because the human will correct >> human alignment means the human looks at the first one and produces a new prompt for the second one as opposed to like you [clears throat] spec it out, you know, you spend a long time writing a pro a big big fat prompt and then you have a very smart model do it, right? You know, because uh really is is our</p>
<p>lacks in performance uh an issue of like, well, you just haven't specified well enough. There's no universe in which I can produce what you want because you just haven't told me, >> right? It's underspecified. So, I could produce 10 different things and only one of them is the thing you wanted. >> Yeah. And the multi-turn taking with a flash model is enough. >> Yeah. [laughter] >> Yeah. I'm I'm a big believer in pushing on latency because I think being able to have really low latency interactions with a system you're using is just much more delightful than something that is,</p>
<p>you know, 10 times as slow or 20 times as slow. And I think, you know, in the future, we'll see models that are and and underlying software and hardware systems that are 20x lower latency than what we have today, 50x lower latency. And that's going to be really really important for systems that need to do a lot of stuff uh between your interactions. >> Yeah. Yeah. There's two extremes, right? And then meanwhile [clears throat] you also have deep think which is all the way on the other side, >> right? [laughter] But you would use deep think all the time if it weren't for</p>
<p>cost and latency, right? If if you could have that capability in a model because the latency improvement was 20x uh in the underlying hardware and system and costs, you know, there's no reason you wouldn't want that. >> Yeah. But at the same time, then you'd probably have a model that is even better that would take you 20 times longer even on that new hardware. >> Yeah. Uh you know that there's the Fredo curve keeps climbing. Um >> yeah,</p>
<p>>> onward and outward. on way. [laughter] >> Yeah. Should we ask him for predictions to to go? I don't know if you have any >> predictions that you that you like to keep, you know, like uh one one way to do this is you have your tests whenever a a new model comes out that you run. Uh what's something that you're not quite happy with yet that you think will get done soon? >> Um let me make two predictions that are not</p>
<p><strong>[81:33]</strong></p>
<p>quite in that vein. >> Yeah. So I think a personalized model that knows you and knows all your state and is able to retrieve over all state you have access to that you opt into is going to be incredibly useful compared to a more generic model that doesn't have access to that. So like can something attend to everything I've ever seen, every email, every photo, every video I've watched. That's going to be really useful. uh I think uh more and more specialized hardware is going to enable much lower latency models and</p>
<p>much more capable models for affordable prices uh than say the current current status quo. Uh that's going to be also quite important. >> Yeah. When you say much lower latency, uh people usually talk in tokens per second. Is that a term that is okay? Okay. Uh you know [clears throat] we're at let's say 100 now. Yeah, we can go to the thousands. Is it meaningful to go 10 thousands? >> Yes. >> Really? Okay. >> Absolutely. Right. >> Yeah. Because of chain of thought and all</p>
<p>>> chain of thought reasoning. I mean you could think you know uh many more tokens. You could do many more parallel rollouts. You could generate way more code uh and check that the code is correct with uh chain of thought reasoning. So I think you know being able to do that at 10,000 tokens per second would be awesome. >> Yeah. At 10,000 tokens per second you are no longer reading code. Yeah. like you'll just generate it. You'll not remember it may not it may not >> end up with 10,000 tokens of code a thousand tokens of code that with 9,000 tokens of reasoning behind it.</p>
<p>>> Yeah. Yeah. >> Which would actually be probably much better code to read. >> Yeah. Yeah. >> Yeah. If I had more time, I would have written a shorter letter. >> Yeah. Yeah. >> Um awesome, Jeff. This was amazing. Thanks for making the time. >> Thank you. It's been it's been fun. Thanks for having me. >> [music]</p>]]></content:encoded>
      <guid isPermaLink="false">https://www.youtube.com/watch?v=F_1oDPWxpFQ</guid>
      <pubDate>Thu, 12 Feb 2026 22:03:01 +0000</pubDate>
    </item>
  </channel>
</rss>
